{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regression Deep Learning Model for [PROJECT NAME] Using Keras Version 5\n",
    "### David Lowe\n",
    "### January 14, 2020\n",
    "\n",
    "Template Credit: Adapted from a template made available by Dr. Jason Brownlee of Machine Learning Mastery. [https://machinelearningmastery.com/]\n",
    "\n",
    "SUMMARY: The purpose of this project is to construct a predictive model using various machine learning algorithms and to document the end-to-end steps using a template. The [PROJECT NAME] dataset is a regression situation where we are trying to predict the value of a continuous variable.\n",
    "\n",
    "INTRODUCTION: [Sample Paragraph - The purpose of the analysis is to predict the housing values in the suburbs of Boston by using the home sale transaction history.]\n",
    "\n",
    "ANALYSIS: [Sample Paragraph - The baseline performance of the model achieved a root mean squared error (RMSE) of 5.06. After tuning the hyperparameters, the best model processed the training dataset with an RMSE of 4.72. Furthermore, the final model processed the test dataset with an RMSE of 3.03, which was even better than the baseline result from the training dataset.]\n",
    "\n",
    "CONCLUSION: For this dataset, the model built using Keras and TensorFlow achieved a satisfactory result and should be considered for future modeling activities.\n",
    "\n",
    "Dataset Used: [PROJECT NAME] Dataset\n",
    "\n",
    "Dataset ML Model: Regression with numerical attributes\n",
    "\n",
    "Dataset Reference: [https://raw.githubusercontent.com/jbrownlee/Datasets/master/housing.data]\n",
    "\n",
    "One potential source of performance benchmarks: [https://machinelearningmastery.com/regression-tutorial-keras-deep-learning-library-python/]\n",
    "\n",
    "Any deep-learning modeling project genrally can be broken down into about seven major tasks:\n",
    "\n",
    "0. Prepare Environment\n",
    "1. Load Data\n",
    "2. Define Model\n",
    "3. Fit and Evaluate Model\n",
    "4. Optimize Model\n",
    "5. Finalize Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Section 0. Prepare Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the warning message filter\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the random seed number for reproducible results\n",
    "seedNum = 888"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load libraries and packages\n",
    "import random\n",
    "random.seed(seedNum)\n",
    "import numpy as np\n",
    "np.random.seed(seedNum)\n",
    "import tensorflow as tf\n",
    "tf.random.set_seed(seedNum)\n",
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.layers import Flatten\n",
    "from tensorflow.keras.layers import Dropout\n",
    "from tensorflow.keras.layers import Conv2D\n",
    "from tensorflow.keras.layers import MaxPooling2D\n",
    "from tensorflow.keras.layers import BatchNormalization\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.wrappers.scikit_learn import KerasRegressor\n",
    "import pandas as pd\n",
    "import math\n",
    "import os\n",
    "import sys\n",
    "import shutil\n",
    "import urllib.request\n",
    "import zipfile\n",
    "import smtplib\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import datetime\n",
    "from email.message import EmailMessage\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn import preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num GPUs Available:  0\n"
     ]
    }
   ],
   "source": [
    "# Begin the timer for the script processing\n",
    "startTimeScript = datetime.now()\n",
    "\n",
    "# Set up the verbose flag to print detailed messages for debugging (setting to True will activate)\n",
    "# verbose = True\n",
    "# tf.debugging.set_log_device_placement(verbose)\n",
    "\n",
    "# Set up the number of CPU cores available for multi-thread processing\n",
    "n_jobs = -1\n",
    "print(\"Num GPUs Available: \", len(tf.config.experimental.list_physical_devices('GPU')))\n",
    "\n",
    "# Set up the flag to stop sending progress emails (setting to True will send status emails!)\n",
    "notifyStatus = False\n",
    "\n",
    "# Set the number of folds for cross validation\n",
    "n_folds = 5\n",
    "\n",
    "# Set the flag for splitting the dataset\n",
    "splitDataset = True\n",
    "splitPercentage = 0.25\n",
    "\n",
    "# Set various default Keras modeling parameters\n",
    "default_loss = 'mean_squared_error'\n",
    "default_optimizer = tf.optimizers.Adam(learning_rate=0.001)\n",
    "default_kernel_init = tf.initializers.RandomNormal(seed=seedNum)\n",
    "default_epoch = 100\n",
    "default_batch = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up the email notification function\n",
    "def email_notify(msg_text):\n",
    "    sender = os.environ.get('MAIL_SENDER')\n",
    "    receiver = os.environ.get('MAIL_RECEIVER')\n",
    "    gateway = os.environ.get('SMTP_GATEWAY')\n",
    "    smtpuser = os.environ.get('SMTP_USERNAME')\n",
    "    password = os.environ.get('SMTP_PASSWORD')\n",
    "    if sender==None or receiver==None or gateway==None or smtpuser==None or password==None:\n",
    "        sys.exit(\"Incomplete email setup info. Script Processing Aborted!!!\")\n",
    "    msg = EmailMessage()\n",
    "    msg.set_content(msg_text)\n",
    "    msg['Subject'] = 'Notification from Keras Regression Script'\n",
    "    msg['From'] = sender\n",
    "    msg['To'] = receiver\n",
    "    server = smtplib.SMTP(gateway, 587)\n",
    "    server.starttls()\n",
    "    server.login(smtpuser, password)\n",
    "    server.send_message(msg)\n",
    "    server.quit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reset the random number generators\n",
    "def reset_random(x):\n",
    "    random.seed(x)\n",
    "    np.random.seed(x)\n",
    "    tf.random.set_seed(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "if (notifyStatus): email_notify(\"Phase 0 Prepare Environment completed! \"+datetime.now().strftime('%a %B %d, %Y %I:%M:%S %p'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Section 1. Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "if (notifyStatus): email_notify(\"Phase 1 Load Data has begun! \"+datetime.now().strftime('%a %B %d, %Y %I:%M:%S %p'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.a) Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>CRIM</th>\n",
       "      <th>ZN</th>\n",
       "      <th>INDUS</th>\n",
       "      <th>CHAS</th>\n",
       "      <th>NOX</th>\n",
       "      <th>RM</th>\n",
       "      <th>AGE</th>\n",
       "      <th>DIS</th>\n",
       "      <th>RAD</th>\n",
       "      <th>TAX</th>\n",
       "      <th>PTRATIO</th>\n",
       "      <th>B</th>\n",
       "      <th>LSTAT</th>\n",
       "      <th>MEDV</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.00632</td>\n",
       "      <td>18.0</td>\n",
       "      <td>2.31</td>\n",
       "      <td>0</td>\n",
       "      <td>0.538</td>\n",
       "      <td>6.575</td>\n",
       "      <td>65.2</td>\n",
       "      <td>4.0900</td>\n",
       "      <td>1</td>\n",
       "      <td>296.0</td>\n",
       "      <td>15.3</td>\n",
       "      <td>396.90</td>\n",
       "      <td>4.98</td>\n",
       "      <td>24.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.02731</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.07</td>\n",
       "      <td>0</td>\n",
       "      <td>0.469</td>\n",
       "      <td>6.421</td>\n",
       "      <td>78.9</td>\n",
       "      <td>4.9671</td>\n",
       "      <td>2</td>\n",
       "      <td>242.0</td>\n",
       "      <td>17.8</td>\n",
       "      <td>396.90</td>\n",
       "      <td>9.14</td>\n",
       "      <td>21.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.02729</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.07</td>\n",
       "      <td>0</td>\n",
       "      <td>0.469</td>\n",
       "      <td>7.185</td>\n",
       "      <td>61.1</td>\n",
       "      <td>4.9671</td>\n",
       "      <td>2</td>\n",
       "      <td>242.0</td>\n",
       "      <td>17.8</td>\n",
       "      <td>392.83</td>\n",
       "      <td>4.03</td>\n",
       "      <td>34.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.03237</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.18</td>\n",
       "      <td>0</td>\n",
       "      <td>0.458</td>\n",
       "      <td>6.998</td>\n",
       "      <td>45.8</td>\n",
       "      <td>6.0622</td>\n",
       "      <td>3</td>\n",
       "      <td>222.0</td>\n",
       "      <td>18.7</td>\n",
       "      <td>394.63</td>\n",
       "      <td>2.94</td>\n",
       "      <td>33.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.06905</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.18</td>\n",
       "      <td>0</td>\n",
       "      <td>0.458</td>\n",
       "      <td>7.147</td>\n",
       "      <td>54.2</td>\n",
       "      <td>6.0622</td>\n",
       "      <td>3</td>\n",
       "      <td>222.0</td>\n",
       "      <td>18.7</td>\n",
       "      <td>396.90</td>\n",
       "      <td>5.33</td>\n",
       "      <td>36.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.02985</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.18</td>\n",
       "      <td>0</td>\n",
       "      <td>0.458</td>\n",
       "      <td>6.430</td>\n",
       "      <td>58.7</td>\n",
       "      <td>6.0622</td>\n",
       "      <td>3</td>\n",
       "      <td>222.0</td>\n",
       "      <td>18.7</td>\n",
       "      <td>394.12</td>\n",
       "      <td>5.21</td>\n",
       "      <td>28.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.08829</td>\n",
       "      <td>12.5</td>\n",
       "      <td>7.87</td>\n",
       "      <td>0</td>\n",
       "      <td>0.524</td>\n",
       "      <td>6.012</td>\n",
       "      <td>66.6</td>\n",
       "      <td>5.5605</td>\n",
       "      <td>5</td>\n",
       "      <td>311.0</td>\n",
       "      <td>15.2</td>\n",
       "      <td>395.60</td>\n",
       "      <td>12.43</td>\n",
       "      <td>22.9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.14455</td>\n",
       "      <td>12.5</td>\n",
       "      <td>7.87</td>\n",
       "      <td>0</td>\n",
       "      <td>0.524</td>\n",
       "      <td>6.172</td>\n",
       "      <td>96.1</td>\n",
       "      <td>5.9505</td>\n",
       "      <td>5</td>\n",
       "      <td>311.0</td>\n",
       "      <td>15.2</td>\n",
       "      <td>396.90</td>\n",
       "      <td>19.15</td>\n",
       "      <td>27.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.21124</td>\n",
       "      <td>12.5</td>\n",
       "      <td>7.87</td>\n",
       "      <td>0</td>\n",
       "      <td>0.524</td>\n",
       "      <td>5.631</td>\n",
       "      <td>100.0</td>\n",
       "      <td>6.0821</td>\n",
       "      <td>5</td>\n",
       "      <td>311.0</td>\n",
       "      <td>15.2</td>\n",
       "      <td>386.63</td>\n",
       "      <td>29.93</td>\n",
       "      <td>16.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.17004</td>\n",
       "      <td>12.5</td>\n",
       "      <td>7.87</td>\n",
       "      <td>0</td>\n",
       "      <td>0.524</td>\n",
       "      <td>6.004</td>\n",
       "      <td>85.9</td>\n",
       "      <td>6.5921</td>\n",
       "      <td>5</td>\n",
       "      <td>311.0</td>\n",
       "      <td>15.2</td>\n",
       "      <td>386.71</td>\n",
       "      <td>17.10</td>\n",
       "      <td>18.9</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      CRIM    ZN  INDUS  CHAS    NOX     RM    AGE     DIS  RAD    TAX  \\\n",
       "0  0.00632  18.0   2.31     0  0.538  6.575   65.2  4.0900    1  296.0   \n",
       "1  0.02731   0.0   7.07     0  0.469  6.421   78.9  4.9671    2  242.0   \n",
       "2  0.02729   0.0   7.07     0  0.469  7.185   61.1  4.9671    2  242.0   \n",
       "3  0.03237   0.0   2.18     0  0.458  6.998   45.8  6.0622    3  222.0   \n",
       "4  0.06905   0.0   2.18     0  0.458  7.147   54.2  6.0622    3  222.0   \n",
       "5  0.02985   0.0   2.18     0  0.458  6.430   58.7  6.0622    3  222.0   \n",
       "6  0.08829  12.5   7.87     0  0.524  6.012   66.6  5.5605    5  311.0   \n",
       "7  0.14455  12.5   7.87     0  0.524  6.172   96.1  5.9505    5  311.0   \n",
       "8  0.21124  12.5   7.87     0  0.524  5.631  100.0  6.0821    5  311.0   \n",
       "9  0.17004  12.5   7.87     0  0.524  6.004   85.9  6.5921    5  311.0   \n",
       "\n",
       "   PTRATIO       B  LSTAT  MEDV  \n",
       "0     15.3  396.90   4.98  24.0  \n",
       "1     17.8  396.90   9.14  21.6  \n",
       "2     17.8  392.83   4.03  34.7  \n",
       "3     18.7  394.63   2.94  33.4  \n",
       "4     18.7  396.90   5.33  36.2  \n",
       "5     18.7  394.12   5.21  28.7  \n",
       "6     15.2  395.60  12.43  22.9  \n",
       "7     15.2  396.90  19.15  27.1  \n",
       "8     15.2  386.63  29.93  16.5  \n",
       "9     15.2  386.71  17.10  18.9  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_path = 'https://archive.ics.uci.edu/ml/machine-learning-databases/housing/housing.data'\n",
    "# dest_file = os.path.basename(dataset_path)\n",
    "# if (os.path.isfile(dest_file) == False) :\n",
    "#     print('Downloading ' + dataset_path + ' as ' + dest_file)\n",
    "#     with urllib.request.urlopen(dataset_path) as in_resp, open(dest_file, 'wb') as out_file:\n",
    "#         shutil.copyfileobj(in_resp, out_file)\n",
    "#     print(dest_file + 'downloaded!')\n",
    "#     print('Unpacking ' + dest_file)\n",
    "#     with zipfile.ZipFile(dest_file, 'r') as zip_ref:\n",
    "#         zip_ref.extractall('.')\n",
    "#     print(dest_file + 'unpacked!')\n",
    "\n",
    "# inputFile = dest_file\n",
    "# attrNames = ['attr' + str(i).zfill(2) for i in range(1,10)]\n",
    "# colNames = ['id'] + attrNames + ['target']\n",
    "colNames = ['CRIM','ZN','INDUS','CHAS','NOX','RM','AGE','DIS','RAD','TAX','PTRATIO','B','LSTAT','MEDV']\n",
    "Xy_original = pd.read_csv(dataset_path, names=colNames, delim_whitespace=True, header=None)\n",
    "\n",
    "# Take a peek at the dataframe after the import\n",
    "Xy_original.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 506 entries, 0 to 505\n",
      "Data columns (total 14 columns):\n",
      "CRIM       506 non-null float64\n",
      "ZN         506 non-null float64\n",
      "INDUS      506 non-null float64\n",
      "CHAS       506 non-null int64\n",
      "NOX        506 non-null float64\n",
      "RM         506 non-null float64\n",
      "AGE        506 non-null float64\n",
      "DIS        506 non-null float64\n",
      "RAD        506 non-null int64\n",
      "TAX        506 non-null float64\n",
      "PTRATIO    506 non-null float64\n",
      "B          506 non-null float64\n",
      "LSTAT      506 non-null float64\n",
      "MEDV       506 non-null float64\n",
      "dtypes: float64(12), int64(2)\n",
      "memory usage: 55.5 KB\n"
     ]
    }
   ],
   "source": [
    "Xy_original.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>CRIM</th>\n",
       "      <th>ZN</th>\n",
       "      <th>INDUS</th>\n",
       "      <th>CHAS</th>\n",
       "      <th>NOX</th>\n",
       "      <th>RM</th>\n",
       "      <th>AGE</th>\n",
       "      <th>DIS</th>\n",
       "      <th>RAD</th>\n",
       "      <th>TAX</th>\n",
       "      <th>PTRATIO</th>\n",
       "      <th>B</th>\n",
       "      <th>LSTAT</th>\n",
       "      <th>MEDV</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>506.000000</td>\n",
       "      <td>506.000000</td>\n",
       "      <td>506.000000</td>\n",
       "      <td>506.000000</td>\n",
       "      <td>506.000000</td>\n",
       "      <td>506.000000</td>\n",
       "      <td>506.000000</td>\n",
       "      <td>506.000000</td>\n",
       "      <td>506.000000</td>\n",
       "      <td>506.000000</td>\n",
       "      <td>506.000000</td>\n",
       "      <td>506.000000</td>\n",
       "      <td>506.000000</td>\n",
       "      <td>506.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>3.613524</td>\n",
       "      <td>11.363636</td>\n",
       "      <td>11.136779</td>\n",
       "      <td>0.069170</td>\n",
       "      <td>0.554695</td>\n",
       "      <td>6.284634</td>\n",
       "      <td>68.574901</td>\n",
       "      <td>3.795043</td>\n",
       "      <td>9.549407</td>\n",
       "      <td>408.237154</td>\n",
       "      <td>18.455534</td>\n",
       "      <td>356.674032</td>\n",
       "      <td>12.653063</td>\n",
       "      <td>22.532806</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>8.601545</td>\n",
       "      <td>23.322453</td>\n",
       "      <td>6.860353</td>\n",
       "      <td>0.253994</td>\n",
       "      <td>0.115878</td>\n",
       "      <td>0.702617</td>\n",
       "      <td>28.148861</td>\n",
       "      <td>2.105710</td>\n",
       "      <td>8.707259</td>\n",
       "      <td>168.537116</td>\n",
       "      <td>2.164946</td>\n",
       "      <td>91.294864</td>\n",
       "      <td>7.141062</td>\n",
       "      <td>9.197104</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.006320</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.460000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.385000</td>\n",
       "      <td>3.561000</td>\n",
       "      <td>2.900000</td>\n",
       "      <td>1.129600</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>187.000000</td>\n",
       "      <td>12.600000</td>\n",
       "      <td>0.320000</td>\n",
       "      <td>1.730000</td>\n",
       "      <td>5.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.082045</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>5.190000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.449000</td>\n",
       "      <td>5.885500</td>\n",
       "      <td>45.025000</td>\n",
       "      <td>2.100175</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>279.000000</td>\n",
       "      <td>17.400000</td>\n",
       "      <td>375.377500</td>\n",
       "      <td>6.950000</td>\n",
       "      <td>17.025000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.256510</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>9.690000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.538000</td>\n",
       "      <td>6.208500</td>\n",
       "      <td>77.500000</td>\n",
       "      <td>3.207450</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>330.000000</td>\n",
       "      <td>19.050000</td>\n",
       "      <td>391.440000</td>\n",
       "      <td>11.360000</td>\n",
       "      <td>21.200000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>3.677082</td>\n",
       "      <td>12.500000</td>\n",
       "      <td>18.100000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.624000</td>\n",
       "      <td>6.623500</td>\n",
       "      <td>94.075000</td>\n",
       "      <td>5.188425</td>\n",
       "      <td>24.000000</td>\n",
       "      <td>666.000000</td>\n",
       "      <td>20.200000</td>\n",
       "      <td>396.225000</td>\n",
       "      <td>16.955000</td>\n",
       "      <td>25.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>88.976200</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>27.740000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.871000</td>\n",
       "      <td>8.780000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>12.126500</td>\n",
       "      <td>24.000000</td>\n",
       "      <td>711.000000</td>\n",
       "      <td>22.000000</td>\n",
       "      <td>396.900000</td>\n",
       "      <td>37.970000</td>\n",
       "      <td>50.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             CRIM          ZN       INDUS        CHAS         NOX          RM  \\\n",
       "count  506.000000  506.000000  506.000000  506.000000  506.000000  506.000000   \n",
       "mean     3.613524   11.363636   11.136779    0.069170    0.554695    6.284634   \n",
       "std      8.601545   23.322453    6.860353    0.253994    0.115878    0.702617   \n",
       "min      0.006320    0.000000    0.460000    0.000000    0.385000    3.561000   \n",
       "25%      0.082045    0.000000    5.190000    0.000000    0.449000    5.885500   \n",
       "50%      0.256510    0.000000    9.690000    0.000000    0.538000    6.208500   \n",
       "75%      3.677082   12.500000   18.100000    0.000000    0.624000    6.623500   \n",
       "max     88.976200  100.000000   27.740000    1.000000    0.871000    8.780000   \n",
       "\n",
       "              AGE         DIS         RAD         TAX     PTRATIO           B  \\\n",
       "count  506.000000  506.000000  506.000000  506.000000  506.000000  506.000000   \n",
       "mean    68.574901    3.795043    9.549407  408.237154   18.455534  356.674032   \n",
       "std     28.148861    2.105710    8.707259  168.537116    2.164946   91.294864   \n",
       "min      2.900000    1.129600    1.000000  187.000000   12.600000    0.320000   \n",
       "25%     45.025000    2.100175    4.000000  279.000000   17.400000  375.377500   \n",
       "50%     77.500000    3.207450    5.000000  330.000000   19.050000  391.440000   \n",
       "75%     94.075000    5.188425   24.000000  666.000000   20.200000  396.225000   \n",
       "max    100.000000   12.126500   24.000000  711.000000   22.000000  396.900000   \n",
       "\n",
       "            LSTAT        MEDV  \n",
       "count  506.000000  506.000000  \n",
       "mean    12.653063   22.532806  \n",
       "std      7.141062    9.197104  \n",
       "min      1.730000    5.000000  \n",
       "25%      6.950000   17.025000  \n",
       "50%     11.360000   21.200000  \n",
       "75%     16.955000   25.000000  \n",
       "max     37.970000   50.000000  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Xy_original.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CRIM       0\n",
      "ZN         0\n",
      "INDUS      0\n",
      "CHAS       0\n",
      "NOX        0\n",
      "RM         0\n",
      "AGE        0\n",
      "DIS        0\n",
      "RAD        0\n",
      "TAX        0\n",
      "PTRATIO    0\n",
      "B          0\n",
      "LSTAT      0\n",
      "MEDV       0\n",
      "dtype: int64\n",
      "Total number of NaN in the dataframe:  0\n"
     ]
    }
   ],
   "source": [
    "print(Xy_original.isnull().sum())\n",
    "print('Total number of NaN in the dataframe: ', Xy_original.isnull().sum().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.b) Data Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>CRIM</th>\n",
       "      <th>ZN</th>\n",
       "      <th>INDUS</th>\n",
       "      <th>CHAS</th>\n",
       "      <th>NOX</th>\n",
       "      <th>RM</th>\n",
       "      <th>AGE</th>\n",
       "      <th>DIS</th>\n",
       "      <th>RAD</th>\n",
       "      <th>TAX</th>\n",
       "      <th>PTRATIO</th>\n",
       "      <th>B</th>\n",
       "      <th>LSTAT</th>\n",
       "      <th>MEDV</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.00632</td>\n",
       "      <td>18.0</td>\n",
       "      <td>2.31</td>\n",
       "      <td>0</td>\n",
       "      <td>0.538</td>\n",
       "      <td>6.575</td>\n",
       "      <td>65.2</td>\n",
       "      <td>4.0900</td>\n",
       "      <td>1</td>\n",
       "      <td>296.0</td>\n",
       "      <td>15.3</td>\n",
       "      <td>396.90</td>\n",
       "      <td>4.98</td>\n",
       "      <td>24.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.02731</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.07</td>\n",
       "      <td>0</td>\n",
       "      <td>0.469</td>\n",
       "      <td>6.421</td>\n",
       "      <td>78.9</td>\n",
       "      <td>4.9671</td>\n",
       "      <td>2</td>\n",
       "      <td>242.0</td>\n",
       "      <td>17.8</td>\n",
       "      <td>396.90</td>\n",
       "      <td>9.14</td>\n",
       "      <td>21.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.02729</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.07</td>\n",
       "      <td>0</td>\n",
       "      <td>0.469</td>\n",
       "      <td>7.185</td>\n",
       "      <td>61.1</td>\n",
       "      <td>4.9671</td>\n",
       "      <td>2</td>\n",
       "      <td>242.0</td>\n",
       "      <td>17.8</td>\n",
       "      <td>392.83</td>\n",
       "      <td>4.03</td>\n",
       "      <td>34.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.03237</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.18</td>\n",
       "      <td>0</td>\n",
       "      <td>0.458</td>\n",
       "      <td>6.998</td>\n",
       "      <td>45.8</td>\n",
       "      <td>6.0622</td>\n",
       "      <td>3</td>\n",
       "      <td>222.0</td>\n",
       "      <td>18.7</td>\n",
       "      <td>394.63</td>\n",
       "      <td>2.94</td>\n",
       "      <td>33.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.06905</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.18</td>\n",
       "      <td>0</td>\n",
       "      <td>0.458</td>\n",
       "      <td>7.147</td>\n",
       "      <td>54.2</td>\n",
       "      <td>6.0622</td>\n",
       "      <td>3</td>\n",
       "      <td>222.0</td>\n",
       "      <td>18.7</td>\n",
       "      <td>396.90</td>\n",
       "      <td>5.33</td>\n",
       "      <td>36.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.02985</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.18</td>\n",
       "      <td>0</td>\n",
       "      <td>0.458</td>\n",
       "      <td>6.430</td>\n",
       "      <td>58.7</td>\n",
       "      <td>6.0622</td>\n",
       "      <td>3</td>\n",
       "      <td>222.0</td>\n",
       "      <td>18.7</td>\n",
       "      <td>394.12</td>\n",
       "      <td>5.21</td>\n",
       "      <td>28.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.08829</td>\n",
       "      <td>12.5</td>\n",
       "      <td>7.87</td>\n",
       "      <td>0</td>\n",
       "      <td>0.524</td>\n",
       "      <td>6.012</td>\n",
       "      <td>66.6</td>\n",
       "      <td>5.5605</td>\n",
       "      <td>5</td>\n",
       "      <td>311.0</td>\n",
       "      <td>15.2</td>\n",
       "      <td>395.60</td>\n",
       "      <td>12.43</td>\n",
       "      <td>22.9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.14455</td>\n",
       "      <td>12.5</td>\n",
       "      <td>7.87</td>\n",
       "      <td>0</td>\n",
       "      <td>0.524</td>\n",
       "      <td>6.172</td>\n",
       "      <td>96.1</td>\n",
       "      <td>5.9505</td>\n",
       "      <td>5</td>\n",
       "      <td>311.0</td>\n",
       "      <td>15.2</td>\n",
       "      <td>396.90</td>\n",
       "      <td>19.15</td>\n",
       "      <td>27.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.21124</td>\n",
       "      <td>12.5</td>\n",
       "      <td>7.87</td>\n",
       "      <td>0</td>\n",
       "      <td>0.524</td>\n",
       "      <td>5.631</td>\n",
       "      <td>100.0</td>\n",
       "      <td>6.0821</td>\n",
       "      <td>5</td>\n",
       "      <td>311.0</td>\n",
       "      <td>15.2</td>\n",
       "      <td>386.63</td>\n",
       "      <td>29.93</td>\n",
       "      <td>16.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.17004</td>\n",
       "      <td>12.5</td>\n",
       "      <td>7.87</td>\n",
       "      <td>0</td>\n",
       "      <td>0.524</td>\n",
       "      <td>6.004</td>\n",
       "      <td>85.9</td>\n",
       "      <td>6.5921</td>\n",
       "      <td>5</td>\n",
       "      <td>311.0</td>\n",
       "      <td>15.2</td>\n",
       "      <td>386.71</td>\n",
       "      <td>17.10</td>\n",
       "      <td>18.9</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      CRIM    ZN  INDUS  CHAS    NOX     RM    AGE     DIS  RAD    TAX  \\\n",
       "0  0.00632  18.0   2.31     0  0.538  6.575   65.2  4.0900    1  296.0   \n",
       "1  0.02731   0.0   7.07     0  0.469  6.421   78.9  4.9671    2  242.0   \n",
       "2  0.02729   0.0   7.07     0  0.469  7.185   61.1  4.9671    2  242.0   \n",
       "3  0.03237   0.0   2.18     0  0.458  6.998   45.8  6.0622    3  222.0   \n",
       "4  0.06905   0.0   2.18     0  0.458  7.147   54.2  6.0622    3  222.0   \n",
       "5  0.02985   0.0   2.18     0  0.458  6.430   58.7  6.0622    3  222.0   \n",
       "6  0.08829  12.5   7.87     0  0.524  6.012   66.6  5.5605    5  311.0   \n",
       "7  0.14455  12.5   7.87     0  0.524  6.172   96.1  5.9505    5  311.0   \n",
       "8  0.21124  12.5   7.87     0  0.524  5.631  100.0  6.0821    5  311.0   \n",
       "9  0.17004  12.5   7.87     0  0.524  6.004   85.9  6.5921    5  311.0   \n",
       "\n",
       "   PTRATIO       B  LSTAT  MEDV  \n",
       "0     15.3  396.90   4.98  24.0  \n",
       "1     17.8  396.90   9.14  21.6  \n",
       "2     17.8  392.83   4.03  34.7  \n",
       "3     18.7  394.63   2.94  33.4  \n",
       "4     18.7  396.90   5.33  36.2  \n",
       "5     18.7  394.12   5.21  28.7  \n",
       "6     15.2  395.60  12.43  22.9  \n",
       "7     15.2  396.90  19.15  27.1  \n",
       "8     15.2  386.63  29.93  16.5  \n",
       "9     15.2  386.71  17.10  18.9  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Standardize the class column to the name of targetVar if required\n",
    "# Xy_original = Xy_original.rename(columns={'old_name': 'targetVar'})\n",
    "\n",
    "# Dropping features\n",
    "# Xy_original.drop(columns=['attribute_name'], inplace=True)\n",
    "\n",
    "# Impute missing values\n",
    "# Xy_original['col_name'].fillna('someValue', inplace=True)\n",
    "# Xy_original['attribute_name'].fillna(value=Xy_original['attribute_name'].median(), inplace=True)\n",
    "\n",
    "# Convert columns from one data type to another\n",
    "# Xy_original.column_name = Xy_original.column_name.astype('int')\n",
    "# Xy_original.column_name = Xy_original.column_name.astype('category')\n",
    "\n",
    "# Convert features with Y/N levels into categorical feature of 1/0\n",
    "# def reClassSomecol(target):\n",
    "#     if (target == 'Y'): return 1\n",
    "#     else: return 0\n",
    "# Xy_original['targetVar'] = Xy_original['target'].apply(reClassSomecol)\n",
    "# Xy_original.drop(columns=['target'], inplace=True)\n",
    "\n",
    "# Take a peek at the dataframe after the cleaning\n",
    "Xy_original.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 506 entries, 0 to 505\n",
      "Data columns (total 14 columns):\n",
      "CRIM       506 non-null float64\n",
      "ZN         506 non-null float64\n",
      "INDUS      506 non-null float64\n",
      "CHAS       506 non-null int64\n",
      "NOX        506 non-null float64\n",
      "RM         506 non-null float64\n",
      "AGE        506 non-null float64\n",
      "DIS        506 non-null float64\n",
      "RAD        506 non-null int64\n",
      "TAX        506 non-null float64\n",
      "PTRATIO    506 non-null float64\n",
      "B          506 non-null float64\n",
      "LSTAT      506 non-null float64\n",
      "MEDV       506 non-null float64\n",
      "dtypes: float64(12), int64(2)\n",
      "memory usage: 55.5 KB\n"
     ]
    }
   ],
   "source": [
    "Xy_original.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>CRIM</th>\n",
       "      <th>ZN</th>\n",
       "      <th>INDUS</th>\n",
       "      <th>CHAS</th>\n",
       "      <th>NOX</th>\n",
       "      <th>RM</th>\n",
       "      <th>AGE</th>\n",
       "      <th>DIS</th>\n",
       "      <th>RAD</th>\n",
       "      <th>TAX</th>\n",
       "      <th>PTRATIO</th>\n",
       "      <th>B</th>\n",
       "      <th>LSTAT</th>\n",
       "      <th>MEDV</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>506.000000</td>\n",
       "      <td>506.000000</td>\n",
       "      <td>506.000000</td>\n",
       "      <td>506.000000</td>\n",
       "      <td>506.000000</td>\n",
       "      <td>506.000000</td>\n",
       "      <td>506.000000</td>\n",
       "      <td>506.000000</td>\n",
       "      <td>506.000000</td>\n",
       "      <td>506.000000</td>\n",
       "      <td>506.000000</td>\n",
       "      <td>506.000000</td>\n",
       "      <td>506.000000</td>\n",
       "      <td>506.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>3.613524</td>\n",
       "      <td>11.363636</td>\n",
       "      <td>11.136779</td>\n",
       "      <td>0.069170</td>\n",
       "      <td>0.554695</td>\n",
       "      <td>6.284634</td>\n",
       "      <td>68.574901</td>\n",
       "      <td>3.795043</td>\n",
       "      <td>9.549407</td>\n",
       "      <td>408.237154</td>\n",
       "      <td>18.455534</td>\n",
       "      <td>356.674032</td>\n",
       "      <td>12.653063</td>\n",
       "      <td>22.532806</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>8.601545</td>\n",
       "      <td>23.322453</td>\n",
       "      <td>6.860353</td>\n",
       "      <td>0.253994</td>\n",
       "      <td>0.115878</td>\n",
       "      <td>0.702617</td>\n",
       "      <td>28.148861</td>\n",
       "      <td>2.105710</td>\n",
       "      <td>8.707259</td>\n",
       "      <td>168.537116</td>\n",
       "      <td>2.164946</td>\n",
       "      <td>91.294864</td>\n",
       "      <td>7.141062</td>\n",
       "      <td>9.197104</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.006320</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.460000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.385000</td>\n",
       "      <td>3.561000</td>\n",
       "      <td>2.900000</td>\n",
       "      <td>1.129600</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>187.000000</td>\n",
       "      <td>12.600000</td>\n",
       "      <td>0.320000</td>\n",
       "      <td>1.730000</td>\n",
       "      <td>5.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.082045</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>5.190000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.449000</td>\n",
       "      <td>5.885500</td>\n",
       "      <td>45.025000</td>\n",
       "      <td>2.100175</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>279.000000</td>\n",
       "      <td>17.400000</td>\n",
       "      <td>375.377500</td>\n",
       "      <td>6.950000</td>\n",
       "      <td>17.025000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.256510</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>9.690000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.538000</td>\n",
       "      <td>6.208500</td>\n",
       "      <td>77.500000</td>\n",
       "      <td>3.207450</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>330.000000</td>\n",
       "      <td>19.050000</td>\n",
       "      <td>391.440000</td>\n",
       "      <td>11.360000</td>\n",
       "      <td>21.200000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>3.677082</td>\n",
       "      <td>12.500000</td>\n",
       "      <td>18.100000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.624000</td>\n",
       "      <td>6.623500</td>\n",
       "      <td>94.075000</td>\n",
       "      <td>5.188425</td>\n",
       "      <td>24.000000</td>\n",
       "      <td>666.000000</td>\n",
       "      <td>20.200000</td>\n",
       "      <td>396.225000</td>\n",
       "      <td>16.955000</td>\n",
       "      <td>25.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>88.976200</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>27.740000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.871000</td>\n",
       "      <td>8.780000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>12.126500</td>\n",
       "      <td>24.000000</td>\n",
       "      <td>711.000000</td>\n",
       "      <td>22.000000</td>\n",
       "      <td>396.900000</td>\n",
       "      <td>37.970000</td>\n",
       "      <td>50.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             CRIM          ZN       INDUS        CHAS         NOX          RM  \\\n",
       "count  506.000000  506.000000  506.000000  506.000000  506.000000  506.000000   \n",
       "mean     3.613524   11.363636   11.136779    0.069170    0.554695    6.284634   \n",
       "std      8.601545   23.322453    6.860353    0.253994    0.115878    0.702617   \n",
       "min      0.006320    0.000000    0.460000    0.000000    0.385000    3.561000   \n",
       "25%      0.082045    0.000000    5.190000    0.000000    0.449000    5.885500   \n",
       "50%      0.256510    0.000000    9.690000    0.000000    0.538000    6.208500   \n",
       "75%      3.677082   12.500000   18.100000    0.000000    0.624000    6.623500   \n",
       "max     88.976200  100.000000   27.740000    1.000000    0.871000    8.780000   \n",
       "\n",
       "              AGE         DIS         RAD         TAX     PTRATIO           B  \\\n",
       "count  506.000000  506.000000  506.000000  506.000000  506.000000  506.000000   \n",
       "mean    68.574901    3.795043    9.549407  408.237154   18.455534  356.674032   \n",
       "std     28.148861    2.105710    8.707259  168.537116    2.164946   91.294864   \n",
       "min      2.900000    1.129600    1.000000  187.000000   12.600000    0.320000   \n",
       "25%     45.025000    2.100175    4.000000  279.000000   17.400000  375.377500   \n",
       "50%     77.500000    3.207450    5.000000  330.000000   19.050000  391.440000   \n",
       "75%     94.075000    5.188425   24.000000  666.000000   20.200000  396.225000   \n",
       "max    100.000000   12.126500   24.000000  711.000000   22.000000  396.900000   \n",
       "\n",
       "            LSTAT        MEDV  \n",
       "count  506.000000  506.000000  \n",
       "mean    12.653063   22.532806  \n",
       "std      7.141062    9.197104  \n",
       "min      1.730000    5.000000  \n",
       "25%      6.950000   17.025000  \n",
       "50%     11.360000   21.200000  \n",
       "75%     16.955000   25.000000  \n",
       "max     37.970000   50.000000  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Xy_original.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CRIM       0\n",
      "ZN         0\n",
      "INDUS      0\n",
      "CHAS       0\n",
      "NOX        0\n",
      "RM         0\n",
      "AGE        0\n",
      "DIS        0\n",
      "RAD        0\n",
      "TAX        0\n",
      "PTRATIO    0\n",
      "B          0\n",
      "LSTAT      0\n",
      "MEDV       0\n",
      "dtype: int64\n",
      "Total number of NaN in the dataframe:  0\n"
     ]
    }
   ],
   "source": [
    "print(Xy_original.isnull().sum())\n",
    "print('Total number of NaN in the dataframe: ', Xy_original.isnull().sum().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.c) Feature Scaling and Pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use variable totCol to hold the number of columns in the dataframe\n",
    "totCol = len(Xy_original.columns)\n",
    "\n",
    "# Set up variable totAttr for the total number of attribute columns\n",
    "totAttr = totCol-1\n",
    "\n",
    "# targetCol variable indicates the column location of the target/class variable\n",
    "# If the first column, set targetCol to 1. If the last column, set targetCol to totCol\n",
    "# If (targetCol <> 1) and (targetCol <> totCol), be aware when slicing up the dataframes for visualization\n",
    "targetCol = totCol"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Xy_original.shape: (506, 14) X_original.shape: (506, 13) y_original.shape: (506,)\n"
     ]
    }
   ],
   "source": [
    "# We create attribute-only and target-only datasets (X_original and y_original)\n",
    "# for various visualization and cleaning/transformation operations\n",
    "\n",
    "if targetCol == totCol:\n",
    "    X_original = Xy_original.iloc[:,0:totAttr]\n",
    "    y_original = Xy_original.iloc[:,totAttr]\n",
    "else:\n",
    "    X_original = Xy_original.iloc[:,1:totCol]\n",
    "    y_original = Xy_original.iloc[:,0]\n",
    "\n",
    "print(\"Xy_original.shape: {} X_original.shape: {} y_original.shape: {}\".format(Xy_original.shape, X_original.shape, y_original.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up the number of row and columns for visualization display. dispRow * dispCol should be >= totAttr\n",
    "dispCol = 4\n",
    "if totAttr % dispCol == 0 :\n",
    "    dispRow = totAttr // dispCol\n",
    "else :\n",
    "    dispRow = (totAttr // dispCol) + 1\n",
    "    \n",
    "# Set figure width to display the data visualization plots\n",
    "fig_size = plt.rcParams[\"figure.figsize\"]\n",
    "fig_size[0] = dispCol*4\n",
    "fig_size[1] = dispRow*4\n",
    "plt.rcParams[\"figure.figsize\"] = fig_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1152x1152 with 16 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Histograms for each attribute before pre-processing\n",
    "X_original.hist(layout=(dispRow,dispCol))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['CRIM', 'ZN', 'INDUS', 'CHAS', 'NOX', 'RM', 'AGE', 'DIS', 'RAD', 'TAX', 'PTRATIO', 'B', 'LSTAT']\n"
     ]
    }
   ],
   "source": [
    "tobe_transformed_cols = X_original.columns.tolist()\n",
    "# tobe_transformed_cols.remove('some_column_label')\n",
    "print(tobe_transformed_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>CRIM</th>\n",
       "      <th>ZN</th>\n",
       "      <th>INDUS</th>\n",
       "      <th>CHAS</th>\n",
       "      <th>NOX</th>\n",
       "      <th>RM</th>\n",
       "      <th>AGE</th>\n",
       "      <th>DIS</th>\n",
       "      <th>RAD</th>\n",
       "      <th>TAX</th>\n",
       "      <th>PTRATIO</th>\n",
       "      <th>B</th>\n",
       "      <th>LSTAT</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.419782</td>\n",
       "      <td>0.284830</td>\n",
       "      <td>-1.287909</td>\n",
       "      <td>-0.272599</td>\n",
       "      <td>-0.144217</td>\n",
       "      <td>0.413672</td>\n",
       "      <td>-0.120013</td>\n",
       "      <td>0.140214</td>\n",
       "      <td>-0.982843</td>\n",
       "      <td>-0.666608</td>\n",
       "      <td>-1.459000</td>\n",
       "      <td>0.441052</td>\n",
       "      <td>-1.075562</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.417339</td>\n",
       "      <td>-0.487722</td>\n",
       "      <td>-0.593381</td>\n",
       "      <td>-0.272599</td>\n",
       "      <td>-0.740262</td>\n",
       "      <td>0.194274</td>\n",
       "      <td>0.367166</td>\n",
       "      <td>0.557160</td>\n",
       "      <td>-0.867883</td>\n",
       "      <td>-0.987329</td>\n",
       "      <td>-0.303094</td>\n",
       "      <td>0.441052</td>\n",
       "      <td>-0.492439</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.417342</td>\n",
       "      <td>-0.487722</td>\n",
       "      <td>-0.593381</td>\n",
       "      <td>-0.272599</td>\n",
       "      <td>-0.740262</td>\n",
       "      <td>1.282714</td>\n",
       "      <td>-0.265812</td>\n",
       "      <td>0.557160</td>\n",
       "      <td>-0.867883</td>\n",
       "      <td>-0.987329</td>\n",
       "      <td>-0.303094</td>\n",
       "      <td>0.396427</td>\n",
       "      <td>-1.208727</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.416750</td>\n",
       "      <td>-0.487722</td>\n",
       "      <td>-1.306878</td>\n",
       "      <td>-0.272599</td>\n",
       "      <td>-0.835284</td>\n",
       "      <td>1.016303</td>\n",
       "      <td>-0.809889</td>\n",
       "      <td>1.077737</td>\n",
       "      <td>-0.752922</td>\n",
       "      <td>-1.106115</td>\n",
       "      <td>0.113032</td>\n",
       "      <td>0.416163</td>\n",
       "      <td>-1.361517</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.412482</td>\n",
       "      <td>-0.487722</td>\n",
       "      <td>-1.306878</td>\n",
       "      <td>-0.272599</td>\n",
       "      <td>-0.835284</td>\n",
       "      <td>1.228577</td>\n",
       "      <td>-0.511180</td>\n",
       "      <td>1.077737</td>\n",
       "      <td>-0.752922</td>\n",
       "      <td>-1.106115</td>\n",
       "      <td>0.113032</td>\n",
       "      <td>0.441052</td>\n",
       "      <td>-1.026501</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>491</th>\n",
       "      <td>-0.408212</td>\n",
       "      <td>-0.487722</td>\n",
       "      <td>2.422565</td>\n",
       "      <td>-0.272599</td>\n",
       "      <td>0.469104</td>\n",
       "      <td>-0.429726</td>\n",
       "      <td>1.074822</td>\n",
       "      <td>-0.916009</td>\n",
       "      <td>-0.637962</td>\n",
       "      <td>1.798194</td>\n",
       "      <td>0.760340</td>\n",
       "      <td>0.366604</td>\n",
       "      <td>0.759313</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>492</th>\n",
       "      <td>-0.407563</td>\n",
       "      <td>-0.487722</td>\n",
       "      <td>2.422565</td>\n",
       "      <td>-0.272599</td>\n",
       "      <td>0.469104</td>\n",
       "      <td>-0.429726</td>\n",
       "      <td>0.530745</td>\n",
       "      <td>-0.801065</td>\n",
       "      <td>-0.637962</td>\n",
       "      <td>1.798194</td>\n",
       "      <td>0.760340</td>\n",
       "      <td>0.441052</td>\n",
       "      <td>0.097692</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>493</th>\n",
       "      <td>-0.400349</td>\n",
       "      <td>-0.487722</td>\n",
       "      <td>-0.211099</td>\n",
       "      <td>-0.272599</td>\n",
       "      <td>0.261784</td>\n",
       "      <td>-0.822932</td>\n",
       "      <td>-0.518292</td>\n",
       "      <td>-0.671859</td>\n",
       "      <td>-0.408041</td>\n",
       "      <td>-0.102376</td>\n",
       "      <td>0.344213</td>\n",
       "      <td>0.441052</td>\n",
       "      <td>-0.090141</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>494</th>\n",
       "      <td>-0.387983</td>\n",
       "      <td>-0.487722</td>\n",
       "      <td>-0.211099</td>\n",
       "      <td>-0.272599</td>\n",
       "      <td>0.261784</td>\n",
       "      <td>-0.510932</td>\n",
       "      <td>-0.923682</td>\n",
       "      <td>-0.671859</td>\n",
       "      <td>-0.408041</td>\n",
       "      <td>-0.102376</td>\n",
       "      <td>0.344213</td>\n",
       "      <td>0.441052</td>\n",
       "      <td>0.131334</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>495</th>\n",
       "      <td>-0.399688</td>\n",
       "      <td>-0.487722</td>\n",
       "      <td>-0.211099</td>\n",
       "      <td>-0.272599</td>\n",
       "      <td>0.261784</td>\n",
       "      <td>-0.875644</td>\n",
       "      <td>-1.414418</td>\n",
       "      <td>-0.473678</td>\n",
       "      <td>-0.408041</td>\n",
       "      <td>-0.102376</td>\n",
       "      <td>0.344213</td>\n",
       "      <td>0.401471</td>\n",
       "      <td>0.693431</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>496 rows  13 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         CRIM        ZN     INDUS      CHAS       NOX        RM       AGE  \\\n",
       "0   -0.419782  0.284830 -1.287909 -0.272599 -0.144217  0.413672 -0.120013   \n",
       "1   -0.417339 -0.487722 -0.593381 -0.272599 -0.740262  0.194274  0.367166   \n",
       "2   -0.417342 -0.487722 -0.593381 -0.272599 -0.740262  1.282714 -0.265812   \n",
       "3   -0.416750 -0.487722 -1.306878 -0.272599 -0.835284  1.016303 -0.809889   \n",
       "4   -0.412482 -0.487722 -1.306878 -0.272599 -0.835284  1.228577 -0.511180   \n",
       "..        ...       ...       ...       ...       ...       ...       ...   \n",
       "491 -0.408212 -0.487722  2.422565 -0.272599  0.469104 -0.429726  1.074822   \n",
       "492 -0.407563 -0.487722  2.422565 -0.272599  0.469104 -0.429726  0.530745   \n",
       "493 -0.400349 -0.487722 -0.211099 -0.272599  0.261784 -0.822932 -0.518292   \n",
       "494 -0.387983 -0.487722 -0.211099 -0.272599  0.261784 -0.510932 -0.923682   \n",
       "495 -0.399688 -0.487722 -0.211099 -0.272599  0.261784 -0.875644 -1.414418   \n",
       "\n",
       "          DIS       RAD       TAX   PTRATIO         B     LSTAT  \n",
       "0    0.140214 -0.982843 -0.666608 -1.459000  0.441052 -1.075562  \n",
       "1    0.557160 -0.867883 -0.987329 -0.303094  0.441052 -0.492439  \n",
       "2    0.557160 -0.867883 -0.987329 -0.303094  0.396427 -1.208727  \n",
       "3    1.077737 -0.752922 -1.106115  0.113032  0.416163 -1.361517  \n",
       "4    1.077737 -0.752922 -1.106115  0.113032  0.441052 -1.026501  \n",
       "..        ...       ...       ...       ...       ...       ...  \n",
       "491 -0.916009 -0.637962  1.798194  0.760340  0.366604  0.759313  \n",
       "492 -0.801065 -0.637962  1.798194  0.760340  0.441052  0.097692  \n",
       "493 -0.671859 -0.408041 -0.102376  0.344213  0.441052 -0.090141  \n",
       "494 -0.671859 -0.408041 -0.102376  0.344213  0.441052  0.131334  \n",
       "495 -0.473678 -0.408041 -0.102376  0.344213  0.401471  0.693431  \n",
       "\n",
       "[496 rows x 13 columns]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Apply feature scaling and transformation\n",
    "X_original = X_original.astype(float)\n",
    "scaler = preprocessing.StandardScaler()\n",
    "X_original[tobe_transformed_cols] = scaler.fit_transform(X_original[tobe_transformed_cols])\n",
    "\n",
    "X_original.head(-10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1152x1152 with 16 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Histograms for each attribute after pre-processing\n",
    "X_original.hist(layout=(dispRow,dispCol))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform one-hot-encoding before splitting into trainig and test\n",
    "# X_original = pd.get_dummies(X_original)\n",
    "# print(X_original.info())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.d) Splitting Data into Training and Test Sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train.shape: (379, 13) X_train.type: <class 'numpy.ndarray'>\n",
      "y_train.shape: (379,) y_train.type: <class 'numpy.ndarray'>\n",
      "X_test.shape: (127, 13) X_test.type: <class 'numpy.ndarray'>\n",
      "y_test.shape: (127,) y_test.type: <class 'numpy.ndarray'>\n"
     ]
    }
   ],
   "source": [
    "X_encoded = X_original.to_numpy()\n",
    "y_encoded = y_original.to_numpy()\n",
    "if (splitDataset):\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X_encoded, y_encoded, test_size=splitPercentage, random_state=seedNum)\n",
    "else:\n",
    "    X_train, y_train = X_encoded, y_encoded\n",
    "    X_test, y_test = X_encoded, y_encoded\n",
    "print(\"X_train.shape: {} X_train.type: {}\".format(X_train.shape, type(X_train)))\n",
    "print(\"y_train.shape: {} y_train.type: {}\".format(y_train.shape, type(y_train)))\n",
    "print(\"X_test.shape: {} X_test.type: {}\".format(X_test.shape, type(X_test)))\n",
    "print(\"y_test.shape: {} y_test.type: {}\".format(y_test.shape, type(y_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "if (notifyStatus): email_notify(\"Phase 1 Load Data completed! \"+datetime.now().strftime('%a %B %d, %Y %I:%M:%S %p'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Section 2. Define Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "if (notifyStatus): email_notify(\"Phase 2 Define Model has begun! \"+datetime.now().strftime('%a %B %d, %Y %I:%M:%S %p'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the Keras model required for KerasClassifier\n",
    "def create_default_model():\n",
    "    default_model = Sequential()\n",
    "    default_model.add(Dense(25, input_shape=(13,), activation='relu', kernel_initializer=default_kernel_init))\n",
    "    default_model.add(Dense(1, kernel_initializer=default_kernel_init))\n",
    "    default_model.compile(loss=default_loss, optimizer=default_optimizer)\n",
    "    return default_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "if (notifyStatus): email_notify(\"Phase 2 Define Model completed! \"+datetime.now().strftime('%a %B %d, %Y %I:%M:%S %p'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Section 3. Fit and Evaluate Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "if (notifyStatus): email_notify(\"Phase 3 Fit and Evaluate Model has begun! \"+datetime.now().strftime('%a %B %d, %Y %I:%M:%S %p'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating results using the metrics of mean_squared_error\n",
      "All cross-Validate results: [-46.47742221 -15.92011236 -16.53179696 -28.05810125 -19.02766467]\n",
      "Baseline results [mean (std)]: -25.203019 (11.491294)\n",
      "Average RMSE from all cv folds is: 5.020260898574576\n",
      "Total time for performing cross-validation of the default model: 0:00:20.074639\n"
     ]
    }
   ],
   "source": [
    "startTimeModule = datetime.now()\n",
    "\n",
    "# Initialize the baseline model\n",
    "reset_random(seedNum)\n",
    "cv_model = KerasRegressor(build_fn=create_default_model, epochs=default_epoch, batch_size=default_batch, verbose=0)\n",
    "# Fit and evaluate the Keras model using k-fold cross validation\n",
    "kfold = KFold(n_splits=n_folds, shuffle=True, random_state=seedNum)\n",
    "results = cross_val_score(cv_model, X_train, y_train, cv=kfold)\n",
    "print('Generating results using the metrics of', default_loss)\n",
    "print('All cross-Validate results:', results)\n",
    "print('Baseline results [mean (std)]: %f (%f)' % (results.mean(), results.std()))\n",
    "print('Average RMSE from all cv folds is:', math.sqrt(results.mean()*-1))\n",
    "print('Total time for performing cross-validation of the default model:', (datetime.now() - startTimeModule))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 379 samples, validate on 127 samples\n",
      "Epoch 1/100\n",
      "379/379 [==============================] - 1s 3ms/sample - loss: 581.6962 - val_loss: 579.3236\n",
      "Epoch 2/100\n",
      "379/379 [==============================] - 0s 208us/sample - loss: 545.9003 - val_loss: 517.5367\n",
      "Epoch 3/100\n",
      "379/379 [==============================] - 0s 173us/sample - loss: 472.0630 - val_loss: 422.3842\n",
      "Epoch 4/100\n",
      "379/379 [==============================] - 0s 145us/sample - loss: 375.6238 - val_loss: 313.9327\n",
      "Epoch 5/100\n",
      "379/379 [==============================] - 0s 174us/sample - loss: 278.0222 - val_loss: 217.2983\n",
      "Epoch 6/100\n",
      "379/379 [==============================] - 0s 184us/sample - loss: 198.5361 - val_loss: 149.5430\n",
      "Epoch 7/100\n",
      "379/379 [==============================] - 0s 168us/sample - loss: 143.0606 - val_loss: 108.7005\n",
      "Epoch 8/100\n",
      "379/379 [==============================] - 0s 161us/sample - loss: 109.1332 - val_loss: 84.0668\n",
      "Epoch 9/100\n",
      "379/379 [==============================] - 0s 170us/sample - loss: 87.2091 - val_loss: 67.4945\n",
      "Epoch 10/100\n",
      "379/379 [==============================] - 0s 167us/sample - loss: 71.1026 - val_loss: 55.5456\n",
      "Epoch 11/100\n",
      "379/379 [==============================] - 0s 173us/sample - loss: 59.8519 - val_loss: 46.2792\n",
      "Epoch 12/100\n",
      "379/379 [==============================] - 0s 171us/sample - loss: 50.6813 - val_loss: 39.6291\n",
      "Epoch 13/100\n",
      "379/379 [==============================] - 0s 179us/sample - loss: 44.1152 - val_loss: 35.0675\n",
      "Epoch 14/100\n",
      "379/379 [==============================] - 0s 182us/sample - loss: 40.0574 - val_loss: 31.5245\n",
      "Epoch 15/100\n",
      "379/379 [==============================] - 0s 164us/sample - loss: 36.6296 - val_loss: 29.2952\n",
      "Epoch 16/100\n",
      "379/379 [==============================] - 0s 170us/sample - loss: 34.3418 - val_loss: 27.7941\n",
      "Epoch 17/100\n",
      "379/379 [==============================] - 0s 178us/sample - loss: 32.9937 - val_loss: 26.5577\n",
      "Epoch 18/100\n",
      "379/379 [==============================] - 0s 167us/sample - loss: 31.5869 - val_loss: 25.7924\n",
      "Epoch 19/100\n",
      "379/379 [==============================] - 0s 161us/sample - loss: 30.6943 - val_loss: 25.1822\n",
      "Epoch 20/100\n",
      "379/379 [==============================] - 0s 162us/sample - loss: 29.9498 - val_loss: 24.6680\n",
      "Epoch 21/100\n",
      "379/379 [==============================] - 0s 163us/sample - loss: 29.3159 - val_loss: 24.2578\n",
      "Epoch 22/100\n",
      "379/379 [==============================] - 0s 166us/sample - loss: 28.7474 - val_loss: 23.9425\n",
      "Epoch 23/100\n",
      "379/379 [==============================] - 0s 155us/sample - loss: 28.2861 - val_loss: 23.6446\n",
      "Epoch 24/100\n",
      "379/379 [==============================] - 0s 159us/sample - loss: 27.9012 - val_loss: 23.3494\n",
      "Epoch 25/100\n",
      "379/379 [==============================] - 0s 150us/sample - loss: 27.3940 - val_loss: 23.1075\n",
      "Epoch 26/100\n",
      "379/379 [==============================] - 0s 151us/sample - loss: 27.0556 - val_loss: 22.8553\n",
      "Epoch 27/100\n",
      "379/379 [==============================] - 0s 148us/sample - loss: 26.6796 - val_loss: 22.6338\n",
      "Epoch 28/100\n",
      "379/379 [==============================] - 0s 153us/sample - loss: 26.3418 - val_loss: 22.4195\n",
      "Epoch 29/100\n",
      "379/379 [==============================] - 0s 150us/sample - loss: 26.0234 - val_loss: 22.2099\n",
      "Epoch 30/100\n",
      "379/379 [==============================] - 0s 153us/sample - loss: 25.7399 - val_loss: 22.0019\n",
      "Epoch 31/100\n",
      "379/379 [==============================] - 0s 156us/sample - loss: 25.4505 - val_loss: 21.8410\n",
      "Epoch 32/100\n",
      "379/379 [==============================] - 0s 165us/sample - loss: 25.1419 - val_loss: 21.6292\n",
      "Epoch 33/100\n",
      "379/379 [==============================] - 0s 167us/sample - loss: 24.8615 - val_loss: 21.4469\n",
      "Epoch 34/100\n",
      "379/379 [==============================] - 0s 162us/sample - loss: 24.6295 - val_loss: 21.2763\n",
      "Epoch 35/100\n",
      "379/379 [==============================] - 0s 166us/sample - loss: 24.3568 - val_loss: 21.0905\n",
      "Epoch 36/100\n",
      "379/379 [==============================] - 0s 163us/sample - loss: 24.1055 - val_loss: 20.9107\n",
      "Epoch 37/100\n",
      "379/379 [==============================] - 0s 166us/sample - loss: 23.8614 - val_loss: 20.7409\n",
      "Epoch 38/100\n",
      "379/379 [==============================] - 0s 166us/sample - loss: 23.6203 - val_loss: 20.5785\n",
      "Epoch 39/100\n",
      "379/379 [==============================] - 0s 163us/sample - loss: 23.3793 - val_loss: 20.3946\n",
      "Epoch 40/100\n",
      "379/379 [==============================] - 0s 164us/sample - loss: 23.1702 - val_loss: 20.2386\n",
      "Epoch 41/100\n",
      "379/379 [==============================] - 0s 163us/sample - loss: 22.9507 - val_loss: 20.0352\n",
      "Epoch 42/100\n",
      "379/379 [==============================] - 0s 159us/sample - loss: 22.6975 - val_loss: 19.8448\n",
      "Epoch 43/100\n",
      "379/379 [==============================] - 0s 165us/sample - loss: 22.5143 - val_loss: 19.6872\n",
      "Epoch 44/100\n",
      "379/379 [==============================] - 0s 165us/sample - loss: 22.2499 - val_loss: 19.4789\n",
      "Epoch 45/100\n",
      "379/379 [==============================] - 0s 164us/sample - loss: 22.0371 - val_loss: 19.2976\n",
      "Epoch 46/100\n",
      "379/379 [==============================] - 0s 169us/sample - loss: 21.8221 - val_loss: 19.1177\n",
      "Epoch 47/100\n",
      "379/379 [==============================] - 0s 166us/sample - loss: 21.6491 - val_loss: 18.9118\n",
      "Epoch 48/100\n",
      "379/379 [==============================] - 0s 171us/sample - loss: 21.4125 - val_loss: 18.7097\n",
      "Epoch 49/100\n",
      "379/379 [==============================] - 0s 157us/sample - loss: 21.1557 - val_loss: 18.5218\n",
      "Epoch 50/100\n",
      "379/379 [==============================] - 0s 160us/sample - loss: 20.9756 - val_loss: 18.3002\n",
      "Epoch 51/100\n",
      "379/379 [==============================] - 0s 163us/sample - loss: 20.7559 - val_loss: 18.0967\n",
      "Epoch 52/100\n",
      "379/379 [==============================] - 0s 163us/sample - loss: 20.5572 - val_loss: 17.9222\n",
      "Epoch 53/100\n",
      "379/379 [==============================] - 0s 165us/sample - loss: 20.3550 - val_loss: 17.7347\n",
      "Epoch 54/100\n",
      "379/379 [==============================] - 0s 158us/sample - loss: 20.1473 - val_loss: 17.5586\n",
      "Epoch 55/100\n",
      "379/379 [==============================] - 0s 163us/sample - loss: 19.9550 - val_loss: 17.3723\n",
      "Epoch 56/100\n",
      "379/379 [==============================] - 0s 163us/sample - loss: 19.7954 - val_loss: 17.2152\n",
      "Epoch 57/100\n",
      "379/379 [==============================] - 0s 166us/sample - loss: 19.6312 - val_loss: 17.0398\n",
      "Epoch 58/100\n",
      "379/379 [==============================] - 0s 159us/sample - loss: 19.4461 - val_loss: 16.8513\n",
      "Epoch 59/100\n",
      "379/379 [==============================] - 0s 165us/sample - loss: 19.2393 - val_loss: 16.6989\n",
      "Epoch 60/100\n",
      "379/379 [==============================] - 0s 165us/sample - loss: 19.0842 - val_loss: 16.5533\n",
      "Epoch 61/100\n",
      "379/379 [==============================] - 0s 153us/sample - loss: 18.8877 - val_loss: 16.3713\n",
      "Epoch 62/100\n",
      "379/379 [==============================] - 0s 153us/sample - loss: 18.7270 - val_loss: 16.2246\n",
      "Epoch 63/100\n",
      "379/379 [==============================] - 0s 191us/sample - loss: 18.5787 - val_loss: 16.0539\n",
      "Epoch 64/100\n",
      "379/379 [==============================] - 0s 163us/sample - loss: 18.4055 - val_loss: 15.9011\n",
      "Epoch 65/100\n",
      "379/379 [==============================] - 0s 197us/sample - loss: 18.2616 - val_loss: 15.7510\n",
      "Epoch 66/100\n",
      "379/379 [==============================] - 0s 163us/sample - loss: 18.0902 - val_loss: 15.6041\n",
      "Epoch 67/100\n",
      "379/379 [==============================] - 0s 168us/sample - loss: 17.9397 - val_loss: 15.4625\n",
      "Epoch 68/100\n",
      "379/379 [==============================] - 0s 164us/sample - loss: 17.8488 - val_loss: 15.3138\n",
      "Epoch 69/100\n",
      "379/379 [==============================] - 0s 167us/sample - loss: 17.6655 - val_loss: 15.1786\n",
      "Epoch 70/100\n",
      "379/379 [==============================] - 0s 161us/sample - loss: 17.5296 - val_loss: 15.0175\n",
      "Epoch 71/100\n",
      "379/379 [==============================] - 0s 186us/sample - loss: 17.3820 - val_loss: 14.8866\n",
      "Epoch 72/100\n",
      "379/379 [==============================] - 0s 167us/sample - loss: 17.2599 - val_loss: 14.7458\n",
      "Epoch 73/100\n",
      "379/379 [==============================] - 0s 164us/sample - loss: 17.1312 - val_loss: 14.6114\n",
      "Epoch 74/100\n",
      "379/379 [==============================] - 0s 166us/sample - loss: 17.0493 - val_loss: 14.5123\n",
      "Epoch 75/100\n",
      "379/379 [==============================] - 0s 162us/sample - loss: 16.9122 - val_loss: 14.3785\n",
      "Epoch 76/100\n",
      "379/379 [==============================] - 0s 166us/sample - loss: 16.7565 - val_loss: 14.2482\n",
      "Epoch 77/100\n",
      "379/379 [==============================] - 0s 152us/sample - loss: 16.6779 - val_loss: 14.1467\n",
      "Epoch 78/100\n",
      "379/379 [==============================] - 0s 165us/sample - loss: 16.5632 - val_loss: 14.0294\n",
      "Epoch 79/100\n",
      "379/379 [==============================] - 0s 166us/sample - loss: 16.4201 - val_loss: 13.9183\n",
      "Epoch 80/100\n",
      "379/379 [==============================] - 0s 162us/sample - loss: 16.3374 - val_loss: 13.8146\n",
      "Epoch 81/100\n",
      "379/379 [==============================] - 0s 166us/sample - loss: 16.2160 - val_loss: 13.7101\n",
      "Epoch 82/100\n",
      "379/379 [==============================] - 0s 165us/sample - loss: 16.1215 - val_loss: 13.6249\n",
      "Epoch 83/100\n",
      "379/379 [==============================] - 0s 164us/sample - loss: 16.0380 - val_loss: 13.5266\n",
      "Epoch 84/100\n",
      "379/379 [==============================] - 0s 168us/sample - loss: 15.9269 - val_loss: 13.4137\n",
      "Epoch 85/100\n",
      "379/379 [==============================] - 0s 166us/sample - loss: 15.8329 - val_loss: 13.3273\n",
      "Epoch 86/100\n",
      "379/379 [==============================] - 0s 188us/sample - loss: 15.7171 - val_loss: 13.2156\n",
      "Epoch 87/100\n",
      "379/379 [==============================] - 0s 169us/sample - loss: 15.6218 - val_loss: 13.1337\n",
      "Epoch 88/100\n",
      "379/379 [==============================] - 0s 172us/sample - loss: 15.5216 - val_loss: 13.0379\n",
      "Epoch 89/100\n",
      "379/379 [==============================] - 0s 164us/sample - loss: 15.4177 - val_loss: 12.9454\n",
      "Epoch 90/100\n",
      "379/379 [==============================] - 0s 169us/sample - loss: 15.3207 - val_loss: 12.8568\n",
      "Epoch 91/100\n",
      "379/379 [==============================] - 0s 165us/sample - loss: 15.2492 - val_loss: 12.7650\n",
      "Epoch 92/100\n",
      "379/379 [==============================] - 0s 164us/sample - loss: 15.1544 - val_loss: 12.6818\n",
      "Epoch 93/100\n",
      "379/379 [==============================] - 0s 163us/sample - loss: 15.0930 - val_loss: 12.6051\n",
      "Epoch 94/100\n",
      "379/379 [==============================] - 0s 165us/sample - loss: 14.9625 - val_loss: 12.5020\n",
      "Epoch 95/100\n",
      "379/379 [==============================] - 0s 164us/sample - loss: 14.8920 - val_loss: 12.4260\n",
      "Epoch 96/100\n",
      "379/379 [==============================] - 0s 168us/sample - loss: 14.7905 - val_loss: 12.3588\n",
      "Epoch 97/100\n",
      "379/379 [==============================] - 0s 165us/sample - loss: 14.7247 - val_loss: 12.2873\n",
      "Epoch 98/100\n",
      "379/379 [==============================] - 0s 166us/sample - loss: 14.6528 - val_loss: 12.2206\n",
      "Epoch 99/100\n",
      "379/379 [==============================] - 0s 174us/sample - loss: 14.5457 - val_loss: 12.1175\n",
      "Epoch 100/100\n",
      "379/379 [==============================] - 0s 163us/sample - loss: 14.5338 - val_loss: 12.0493\n",
      "Total time for performing cross-validation of the default model: 0:00:07.684235\n"
     ]
    }
   ],
   "source": [
    "startTimeModule = datetime.now()\n",
    "\n",
    "# Initialize the baseline model\n",
    "reset_random(seedNum)\n",
    "baseline_model = create_default_model()\n",
    "baseline_hist = baseline_model.fit(X_train, y_train, epochs=default_epoch, batch_size=default_batch,\n",
    "                                   validation_data=(X_test, y_test), verbose=1)\n",
    "print('Total time for performing cross-validation of the default model:', (datetime.now() - startTimeModule))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['loss', 'val_loss'])\n"
     ]
    }
   ],
   "source": [
    "# List all data points in the baseline model training history\n",
    "print(baseline_hist.history.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 576x576 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Summarize model training hisotry for accuracy and loss\n",
    "fig, axs = plt.subplots(1, 1, figsize=(8,8))\n",
    "plt.subplot(111)\n",
    "plt.plot(baseline_hist.history['loss'], color='blue', label='train')\n",
    "plt.plot(baseline_hist.history['val_loss'], color='red', label='test')\n",
    "plt.title('Baseline Model Training Performance (Loss)')\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Train', 'Test'], loc='upper right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "if (notifyStatus): email_notify(\"Phase 3 Fit and Evaluate Model completed! \"+datetime.now().strftime('%a %B %d, %Y %I:%M:%S %p'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Section 4. Optimize Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "if (notifyStatus): email_notify(\"Phase 4 Optimize Model has begun! \"+datetime.now().strftime('%a %B %d, %Y %I:%M:%S %p'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the Keras model required for KerasClassifier\n",
    "def create_customized_model(optimizer, kernel_init):\n",
    "    customized_model = Sequential()\n",
    "    customized_model.add(Dense(25, input_shape=(13,), activation='relu', kernel_initializer=kernel_init))\n",
    "    customized_model.add(Dense(1, kernel_initializer=kernel_init))\n",
    "    customized_model.compile(loss=default_loss, optimizer=optimizer)\n",
    "    return customized_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimizer candidate #1 has the object ID of <tensorflow.python.keras.optimizer_v2.adam.Adam object at 0x7f9128349dd0>\n",
      "Optimizer candidate #2 has the object ID of <tensorflow.python.keras.optimizer_v2.adam.Adam object at 0x7f9128349d10>\n",
      "Optimizer candidate #3 has the object ID of <tensorflow.python.keras.optimizer_v2.adam.Adam object at 0x7f9128349c50>\n",
      "Initializer candidate #1 has the object ID of <tensorflow.python.ops.init_ops_v2.RandomNormal object at 0x7f9128349550>\n",
      "Initializer candidate #2 has the object ID of <tensorflow.python.ops.init_ops_v2.Orthogonal object at 0x7f912836c910>\n",
      "Initializer candidate #2 has the object ID of <tensorflow.python.ops.init_ops_v2.GlorotNormal object at 0x7f912836c250>\n",
      "Fitting 5 folds for each of 27 candidates, totalling 135 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   8 tasks      | elapsed:   21.2s\n",
      "[Parallel(n_jobs=-1)]: Done 104 tasks      | elapsed:  1.3min\n",
      "[Parallel(n_jobs=-1)]: Done 135 out of 135 | elapsed:  1.5min finished\n",
      "/usr/local/lib/python3.7/site-packages/sklearn/model_selection/_search.py:814: DeprecationWarning: The default of the `iid` parameter will change from True to False in version 0.22 and will be removed in 0.24. This will change numeric results when test-set sizes are unequal.\n",
      "  DeprecationWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best: -27.176924 using {'batch_size': 32, 'epochs': 100, 'kernel_init': <tensorflow.python.ops.init_ops_v2.GlorotNormal object at 0x7f912836c250>, 'optimizer': <tensorflow.python.keras.optimizer_v2.adam.Adam object at 0x7f9128349dd0>}\n",
      "-28.421273 (9.586597) with: {'batch_size': 32, 'epochs': 100, 'kernel_init': <tensorflow.python.ops.init_ops_v2.RandomNormal object at 0x7f9128349550>, 'optimizer': <tensorflow.python.keras.optimizer_v2.adam.Adam object at 0x7f9128349dd0>}\n",
      "-59.579906 (18.409267) with: {'batch_size': 32, 'epochs': 100, 'kernel_init': <tensorflow.python.ops.init_ops_v2.RandomNormal object at 0x7f9128349550>, 'optimizer': <tensorflow.python.keras.optimizer_v2.adam.Adam object at 0x7f9128349d10>}\n",
      "-524.991070 (78.039593) with: {'batch_size': 32, 'epochs': 100, 'kernel_init': <tensorflow.python.ops.init_ops_v2.RandomNormal object at 0x7f9128349550>, 'optimizer': <tensorflow.python.keras.optimizer_v2.adam.Adam object at 0x7f9128349c50>}\n",
      "-28.893801 (9.397152) with: {'batch_size': 32, 'epochs': 100, 'kernel_init': <tensorflow.python.ops.init_ops_v2.Orthogonal object at 0x7f912836c910>, 'optimizer': <tensorflow.python.keras.optimizer_v2.adam.Adam object at 0x7f9128349dd0>}\n",
      "-81.375632 (21.625109) with: {'batch_size': 32, 'epochs': 100, 'kernel_init': <tensorflow.python.ops.init_ops_v2.Orthogonal object at 0x7f912836c910>, 'optimizer': <tensorflow.python.keras.optimizer_v2.adam.Adam object at 0x7f9128349d10>}\n",
      "-491.940765 (72.737791) with: {'batch_size': 32, 'epochs': 100, 'kernel_init': <tensorflow.python.ops.init_ops_v2.Orthogonal object at 0x7f912836c910>, 'optimizer': <tensorflow.python.keras.optimizer_v2.adam.Adam object at 0x7f9128349c50>}\n",
      "-27.176924 (7.920911) with: {'batch_size': 32, 'epochs': 100, 'kernel_init': <tensorflow.python.ops.init_ops_v2.GlorotNormal object at 0x7f912836c250>, 'optimizer': <tensorflow.python.keras.optimizer_v2.adam.Adam object at 0x7f9128349dd0>}\n",
      "-62.085729 (13.178554) with: {'batch_size': 32, 'epochs': 100, 'kernel_init': <tensorflow.python.ops.init_ops_v2.GlorotNormal object at 0x7f912836c250>, 'optimizer': <tensorflow.python.keras.optimizer_v2.adam.Adam object at 0x7f9128349d10>}\n",
      "-461.160385 (62.838407) with: {'batch_size': 32, 'epochs': 100, 'kernel_init': <tensorflow.python.ops.init_ops_v2.GlorotNormal object at 0x7f912836c250>, 'optimizer': <tensorflow.python.keras.optimizer_v2.adam.Adam object at 0x7f9128349c50>}\n",
      "-53.102243 (15.641435) with: {'batch_size': 64, 'epochs': 100, 'kernel_init': <tensorflow.python.ops.init_ops_v2.RandomNormal object at 0x7f9128349550>, 'optimizer': <tensorflow.python.keras.optimizer_v2.adam.Adam object at 0x7f9128349dd0>}\n",
      "-247.942191 (45.781307) with: {'batch_size': 64, 'epochs': 100, 'kernel_init': <tensorflow.python.ops.init_ops_v2.RandomNormal object at 0x7f9128349550>, 'optimizer': <tensorflow.python.keras.optimizer_v2.adam.Adam object at 0x7f9128349d10>}\n",
      "-568.294990 (82.690172) with: {'batch_size': 64, 'epochs': 100, 'kernel_init': <tensorflow.python.ops.init_ops_v2.RandomNormal object at 0x7f9128349550>, 'optimizer': <tensorflow.python.keras.optimizer_v2.adam.Adam object at 0x7f9128349c50>}\n",
      "-75.668235 (19.461069) with: {'batch_size': 64, 'epochs': 100, 'kernel_init': <tensorflow.python.ops.init_ops_v2.Orthogonal object at 0x7f912836c910>, 'optimizer': <tensorflow.python.keras.optimizer_v2.adam.Adam object at 0x7f9128349dd0>}\n",
      "-267.530144 (44.561486) with: {'batch_size': 64, 'epochs': 100, 'kernel_init': <tensorflow.python.ops.init_ops_v2.Orthogonal object at 0x7f912836c910>, 'optimizer': <tensorflow.python.keras.optimizer_v2.adam.Adam object at 0x7f9128349d10>}\n",
      "-543.968171 (79.014438) with: {'batch_size': 64, 'epochs': 100, 'kernel_init': <tensorflow.python.ops.init_ops_v2.Orthogonal object at 0x7f912836c910>, 'optimizer': <tensorflow.python.keras.optimizer_v2.adam.Adam object at 0x7f9128349c50>}\n",
      "-56.478345 (12.122805) with: {'batch_size': 64, 'epochs': 100, 'kernel_init': <tensorflow.python.ops.init_ops_v2.GlorotNormal object at 0x7f912836c250>, 'optimizer': <tensorflow.python.keras.optimizer_v2.adam.Adam object at 0x7f9128349dd0>}\n",
      "-221.201456 (29.403926) with: {'batch_size': 64, 'epochs': 100, 'kernel_init': <tensorflow.python.ops.init_ops_v2.GlorotNormal object at 0x7f912836c250>, 'optimizer': <tensorflow.python.keras.optimizer_v2.adam.Adam object at 0x7f9128349d10>}\n",
      "-518.393522 (71.054577) with: {'batch_size': 64, 'epochs': 100, 'kernel_init': <tensorflow.python.ops.init_ops_v2.GlorotNormal object at 0x7f912836c250>, 'optimizer': <tensorflow.python.keras.optimizer_v2.adam.Adam object at 0x7f9128349c50>}\n",
      "-183.033903 (33.037786) with: {'batch_size': 128, 'epochs': 100, 'kernel_init': <tensorflow.python.ops.init_ops_v2.RandomNormal object at 0x7f9128349550>, 'optimizer': <tensorflow.python.keras.optimizer_v2.adam.Adam object at 0x7f9128349dd0>}\n",
      "-443.143478 (67.670771) with: {'batch_size': 128, 'epochs': 100, 'kernel_init': <tensorflow.python.ops.init_ops_v2.RandomNormal object at 0x7f9128349550>, 'optimizer': <tensorflow.python.keras.optimizer_v2.adam.Adam object at 0x7f9128349d10>}\n",
      "-579.393785 (83.919886) with: {'batch_size': 128, 'epochs': 100, 'kernel_init': <tensorflow.python.ops.init_ops_v2.RandomNormal object at 0x7f9128349550>, 'optimizer': <tensorflow.python.keras.optimizer_v2.adam.Adam object at 0x7f9128349c50>}\n",
      "-208.008749 (38.054703) with: {'batch_size': 128, 'epochs': 100, 'kernel_init': <tensorflow.python.ops.init_ops_v2.Orthogonal object at 0x7f912836c910>, 'optimizer': <tensorflow.python.keras.optimizer_v2.adam.Adam object at 0x7f9128349dd0>}\n",
      "-413.037568 (61.763847) with: {'batch_size': 128, 'epochs': 100, 'kernel_init': <tensorflow.python.ops.init_ops_v2.Orthogonal object at 0x7f912836c910>, 'optimizer': <tensorflow.python.keras.optimizer_v2.adam.Adam object at 0x7f9128349d10>}\n",
      "-562.358701 (81.281221) with: {'batch_size': 128, 'epochs': 100, 'kernel_init': <tensorflow.python.ops.init_ops_v2.Orthogonal object at 0x7f912836c910>, 'optimizer': <tensorflow.python.keras.optimizer_v2.adam.Adam object at 0x7f9128349c50>}\n",
      "-160.396764 (23.897236) with: {'batch_size': 128, 'epochs': 100, 'kernel_init': <tensorflow.python.ops.init_ops_v2.GlorotNormal object at 0x7f912836c250>, 'optimizer': <tensorflow.python.keras.optimizer_v2.adam.Adam object at 0x7f9128349dd0>}\n",
      "-373.266789 (49.086509) with: {'batch_size': 128, 'epochs': 100, 'kernel_init': <tensorflow.python.ops.init_ops_v2.GlorotNormal object at 0x7f912836c250>, 'optimizer': <tensorflow.python.keras.optimizer_v2.adam.Adam object at 0x7f9128349d10>}\n",
      "-539.808596 (74.494264) with: {'batch_size': 128, 'epochs': 100, 'kernel_init': <tensorflow.python.ops.init_ops_v2.GlorotNormal object at 0x7f912836c250>, 'optimizer': <tensorflow.python.keras.optimizer_v2.adam.Adam object at 0x7f9128349c50>}\n",
      "The best RMSE obtained from the models is: 5.21314913732299\n",
      "Total time for performing grid-search of the best parameters: 0:01:34.545577\n"
     ]
    }
   ],
   "source": [
    "# Perform model hyperparameter tuning via SciKit Learn's GridSearchCV\n",
    "startTimeModule = datetime.now()\n",
    "\n",
    "# Perform grid search using different epochs, batch sizes, and optimizers\n",
    "optz_1 = tf.optimizers.Adam(learning_rate=0.0010)\n",
    "optz_2 = tf.optimizers.Adam(learning_rate=0.0005)\n",
    "optz_3 = tf.optimizers.Adam(learning_rate=0.0001)\n",
    "optimizer_grid = [optz_1, optz_2, optz_3]\n",
    "print('Optimizer candidate #1 has the object ID of', optz_1)\n",
    "print('Optimizer candidate #2 has the object ID of', optz_2)\n",
    "print('Optimizer candidate #3 has the object ID of', optz_3)\n",
    "\n",
    "init_1 = tf.initializers.RandomNormal(seed=seedNum)\n",
    "init_2 = tf.initializers.Orthogonal(seed=seedNum)\n",
    "init_3 = tf.initializers.GlorotNormal(seed=seedNum)\n",
    "init_grid = [init_1, init_2, init_3]\n",
    "print('Initializer candidate #1 has the object ID of', init_1)\n",
    "print('Initializer candidate #2 has the object ID of', init_2)\n",
    "print('Initializer candidate #2 has the object ID of', init_3)\n",
    "\n",
    "epoch_grid = [default_epoch]\n",
    "batch_grid = [default_batch, int(default_batch*2), int(default_batch*4)]\n",
    "\n",
    "# Create grid model\n",
    "param_grid = dict(optimizer=optimizer_grid, kernel_init=init_grid, epochs=epoch_grid, batch_size=batch_grid)\n",
    "reset_random(seedNum)\n",
    "grid_model = KerasRegressor(build_fn=create_customized_model, verbose=0)\n",
    "grid = GridSearchCV(estimator=grid_model, param_grid=param_grid, cv=n_folds, n_jobs=n_jobs, verbose=3)\n",
    "# n_iter = int(len(optimizer_grid) * len(init_grid) * len(epoch_grid) * len(batch_grid) * 0.5)\n",
    "# grid = RandomizedSearchCV(estimator=grid_model, param_distributions=param_grid, n_iter=n_iter, cv=n_folds, n_jobs=n_jobs, verbose=3)\n",
    "grid_result = grid.fit(X_train, y_train)\n",
    "\n",
    "# summarize results\n",
    "print(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))\n",
    "means = grid_result.cv_results_['mean_test_score']\n",
    "stds = grid_result.cv_results_['std_test_score']\n",
    "params = grid_result.cv_results_['params']\n",
    "for mean, stdev, param in zip(means, stds, params):\n",
    "\tprint(\"%f (%f) with: %r\" % (mean, stdev, param))\n",
    "print('The best RMSE obtained from the models is:', math.sqrt(grid_result.best_score_*-1))\n",
    "print('Total time for performing grid-search of the best parameters:', (datetime.now() - startTimeModule))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_optimizer = grid_result.best_params_[\"optimizer\"]\n",
    "best_kernel_init = grid_result.best_params_[\"kernel_init\"]\n",
    "best_epoch = grid_result.best_params_[\"epochs\"]\n",
    "best_batch = grid_result.best_params_[\"batch_size\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimizer candidate #1 has the object ID of <tensorflow.python.keras.optimizer_v2.adam.Adam object at 0x7f9132bd52d0>\n",
      "Optimizer candidate #2 has the object ID of <tensorflow.python.keras.optimizer_v2.adam.Adam object at 0x7f9132bd5690>\n",
      "Optimizer candidate #3 has the object ID of <tensorflow.python.keras.optimizer_v2.adam.Adam object at 0x7f9132bd5290>\n",
      "Initializer candidate #1 has the object ID of <tensorflow.python.ops.init_ops_v2.RandomNormal object at 0x7f9132bbbed0>\n",
      "Initializer candidate #2 has the object ID of <tensorflow.python.ops.init_ops_v2.Orthogonal object at 0x7f9132bbb090>\n",
      "Initializer candidate #2 has the object ID of <tensorflow.python.ops.init_ops_v2.GlorotNormal object at 0x7f9132ed1050>\n",
      "\n",
      "Forming the grid-search model #0 using: optimizer=<tensorflow.python.keras.optimizer_v2.adam.Adam object at 0x7f9132bd52d0>, kernel=<tensorflow.python.ops.init_ops_v2.RandomNormal object at 0x7f9132bbbed0>, epochs=100, batch_size=32\n",
      "Train on 379 samples, validate on 127 samples\n",
      "Epoch 1/100\n",
      "379/379 [==============================] - 1s 3ms/sample - loss: 588.0070 - val_loss: 599.6758\n",
      "Epoch 2/100\n",
      "379/379 [==============================] - 0s 205us/sample - loss: 585.0492 - val_loss: 596.2761\n",
      "Epoch 3/100\n",
      "379/379 [==============================] - 0s 169us/sample - loss: 581.5172 - val_loss: 592.1656\n",
      "Epoch 4/100\n",
      "379/379 [==============================] - 0s 163us/sample - loss: 577.1640 - val_loss: 586.8532\n",
      "Epoch 5/100\n",
      "379/379 [==============================] - 0s 166us/sample - loss: 571.6676 - val_loss: 580.0342\n",
      "Epoch 6/100\n",
      "379/379 [==============================] - 0s 168us/sample - loss: 564.5657 - val_loss: 571.5674\n",
      "Epoch 7/100\n",
      "379/379 [==============================] - 0s 170us/sample - loss: 555.7447 - val_loss: 561.0177\n",
      "Epoch 8/100\n",
      "379/379 [==============================] - 0s 164us/sample - loss: 545.2138 - val_loss: 548.5564\n",
      "Epoch 9/100\n",
      "379/379 [==============================] - 0s 174us/sample - loss: 532.7165 - val_loss: 534.2733\n",
      "Epoch 10/100\n",
      "379/379 [==============================] - 0s 163us/sample - loss: 518.6714 - val_loss: 517.8879\n",
      "Epoch 11/100\n",
      "379/379 [==============================] - 0s 163us/sample - loss: 502.4402 - val_loss: 499.9385\n",
      "Epoch 12/100\n",
      "379/379 [==============================] - 0s 160us/sample - loss: 484.8101 - val_loss: 480.0660\n",
      "Epoch 13/100\n",
      "379/379 [==============================] - 0s 167us/sample - loss: 465.2512 - val_loss: 458.8218\n",
      "Epoch 14/100\n",
      "379/379 [==============================] - 0s 162us/sample - loss: 444.6393 - val_loss: 436.1631\n",
      "Epoch 15/100\n",
      "379/379 [==============================] - 0s 163us/sample - loss: 422.3718 - val_loss: 412.6398\n",
      "Epoch 16/100\n",
      "379/379 [==============================] - 0s 164us/sample - loss: 399.7048 - val_loss: 388.0147\n",
      "Epoch 17/100\n",
      "379/379 [==============================] - 0s 163us/sample - loss: 376.0105 - val_loss: 363.3008\n",
      "Epoch 18/100\n",
      "379/379 [==============================] - 0s 161us/sample - loss: 352.2421 - val_loss: 338.4001\n",
      "Epoch 19/100\n",
      "379/379 [==============================] - 0s 161us/sample - loss: 328.7741 - val_loss: 313.5278\n",
      "Epoch 20/100\n",
      "379/379 [==============================] - 0s 165us/sample - loss: 305.7705 - val_loss: 288.7648\n",
      "Epoch 21/100\n",
      "379/379 [==============================] - 0s 168us/sample - loss: 282.6395 - val_loss: 265.3057\n",
      "Epoch 22/100\n",
      "379/379 [==============================] - 0s 179us/sample - loss: 261.2614 - val_loss: 242.4154\n",
      "Epoch 23/100\n",
      "379/379 [==============================] - 0s 174us/sample - loss: 239.2064 - val_loss: 221.9730\n",
      "Epoch 24/100\n",
      "379/379 [==============================] - 0s 169us/sample - loss: 220.3528 - val_loss: 201.6797\n",
      "Epoch 25/100\n",
      "379/379 [==============================] - 0s 163us/sample - loss: 201.5923 - val_loss: 183.1327\n",
      "Epoch 26/100\n",
      "379/379 [==============================] - 0s 163us/sample - loss: 184.2920 - val_loss: 166.1993\n",
      "Epoch 27/100\n",
      "379/379 [==============================] - 0s 163us/sample - loss: 168.6529 - val_loss: 150.6333\n",
      "Epoch 28/100\n",
      "379/379 [==============================] - 0s 164us/sample - loss: 154.1036 - val_loss: 136.7570\n",
      "Epoch 29/100\n",
      "379/379 [==============================] - 0s 163us/sample - loss: 141.1995 - val_loss: 124.1296\n",
      "Epoch 30/100\n",
      "379/379 [==============================] - 0s 166us/sample - loss: 129.2039 - val_loss: 113.0650\n",
      "Epoch 31/100\n",
      "379/379 [==============================] - 0s 165us/sample - loss: 118.3648 - val_loss: 103.3347\n",
      "Epoch 32/100\n",
      "379/379 [==============================] - 0s 166us/sample - loss: 109.3387 - val_loss: 94.1419\n",
      "Epoch 33/100\n",
      "379/379 [==============================] - 0s 162us/sample - loss: 100.2161 - val_loss: 86.5475\n",
      "Epoch 34/100\n",
      "379/379 [==============================] - 0s 148us/sample - loss: 92.7064 - val_loss: 79.4916\n",
      "Epoch 35/100\n",
      "379/379 [==============================] - 0s 148us/sample - loss: 85.8201 - val_loss: 73.1106\n",
      "Epoch 36/100\n",
      "379/379 [==============================] - 0s 152us/sample - loss: 79.2948 - val_loss: 67.5529\n",
      "Epoch 37/100\n",
      "379/379 [==============================] - 0s 177us/sample - loss: 73.5501 - val_loss: 62.5593\n",
      "Epoch 38/100\n",
      "379/379 [==============================] - 0s 149us/sample - loss: 68.5274 - val_loss: 57.9683\n",
      "Epoch 39/100\n",
      "379/379 [==============================] - 0s 150us/sample - loss: 63.7902 - val_loss: 53.9109\n",
      "Epoch 40/100\n",
      "379/379 [==============================] - 0s 147us/sample - loss: 59.5812 - val_loss: 50.3727\n",
      "Epoch 41/100\n",
      "379/379 [==============================] - 0s 161us/sample - loss: 55.9070 - val_loss: 47.1661\n",
      "Epoch 42/100\n",
      "379/379 [==============================] - 0s 164us/sample - loss: 52.6237 - val_loss: 44.3133\n",
      "Epoch 43/100\n",
      "379/379 [==============================] - 0s 192us/sample - loss: 49.6026 - val_loss: 41.8495\n",
      "Epoch 44/100\n",
      "379/379 [==============================] - 0s 201us/sample - loss: 47.0323 - val_loss: 39.5961\n",
      "Epoch 45/100\n",
      "379/379 [==============================] - 0s 164us/sample - loss: 44.6579 - val_loss: 37.5945\n",
      "Epoch 46/100\n",
      "379/379 [==============================] - 0s 191us/sample - loss: 42.5793 - val_loss: 35.7981\n",
      "Epoch 47/100\n",
      "379/379 [==============================] - 0s 172us/sample - loss: 40.7914 - val_loss: 34.1587\n",
      "Epoch 48/100\n",
      "379/379 [==============================] - 0s 163us/sample - loss: 39.0264 - val_loss: 32.7960\n",
      "Epoch 49/100\n",
      "379/379 [==============================] - 0s 160us/sample - loss: 37.5869 - val_loss: 31.5608\n",
      "Epoch 50/100\n",
      "379/379 [==============================] - 0s 163us/sample - loss: 36.3013 - val_loss: 30.4777\n",
      "Epoch 51/100\n",
      "379/379 [==============================] - 0s 210us/sample - loss: 35.1613 - val_loss: 29.4762\n",
      "Epoch 52/100\n",
      "379/379 [==============================] - 0s 173us/sample - loss: 34.1682 - val_loss: 28.6073\n",
      "Epoch 53/100\n",
      "379/379 [==============================] - 0s 164us/sample - loss: 33.2819 - val_loss: 27.8561\n",
      "Epoch 54/100\n",
      "379/379 [==============================] - 0s 168us/sample - loss: 32.4593 - val_loss: 27.2156\n",
      "Epoch 55/100\n",
      "379/379 [==============================] - 0s 171us/sample - loss: 31.7744 - val_loss: 26.6467\n",
      "Epoch 56/100\n",
      "379/379 [==============================] - 0s 168us/sample - loss: 31.2069 - val_loss: 26.1274\n",
      "Epoch 57/100\n",
      "379/379 [==============================] - 0s 169us/sample - loss: 30.6137 - val_loss: 25.7265\n",
      "Epoch 58/100\n",
      "379/379 [==============================] - 0s 178us/sample - loss: 30.1393 - val_loss: 25.3441\n",
      "Epoch 59/100\n",
      "379/379 [==============================] - 0s 166us/sample - loss: 29.6953 - val_loss: 25.0229\n",
      "Epoch 60/100\n",
      "379/379 [==============================] - 0s 170us/sample - loss: 29.3405 - val_loss: 24.7024\n",
      "Epoch 61/100\n",
      "379/379 [==============================] - 0s 165us/sample - loss: 28.9393 - val_loss: 24.4425\n",
      "Epoch 62/100\n",
      "379/379 [==============================] - 0s 169us/sample - loss: 28.6206 - val_loss: 24.1978\n",
      "Epoch 63/100\n",
      "379/379 [==============================] - 0s 161us/sample - loss: 28.3019 - val_loss: 24.0032\n",
      "Epoch 64/100\n",
      "379/379 [==============================] - 0s 164us/sample - loss: 28.0258 - val_loss: 23.7934\n",
      "Epoch 65/100\n",
      "379/379 [==============================] - 0s 163us/sample - loss: 27.7497 - val_loss: 23.6153\n",
      "Epoch 66/100\n",
      "379/379 [==============================] - 0s 155us/sample - loss: 27.4845 - val_loss: 23.4337\n",
      "Epoch 67/100\n",
      "379/379 [==============================] - 0s 164us/sample - loss: 27.2518 - val_loss: 23.2556\n",
      "Epoch 68/100\n",
      "379/379 [==============================] - 0s 172us/sample - loss: 27.0144 - val_loss: 23.1003\n",
      "Epoch 69/100\n",
      "379/379 [==============================] - 0s 164us/sample - loss: 26.7775 - val_loss: 22.9414\n",
      "Epoch 70/100\n",
      "379/379 [==============================] - 0s 158us/sample - loss: 26.5472 - val_loss: 22.7909\n",
      "Epoch 71/100\n",
      "379/379 [==============================] - 0s 161us/sample - loss: 26.3222 - val_loss: 22.6474\n",
      "Epoch 72/100\n",
      "379/379 [==============================] - 0s 168us/sample - loss: 26.1092 - val_loss: 22.5073\n",
      "Epoch 73/100\n",
      "379/379 [==============================] - 0s 161us/sample - loss: 25.9081 - val_loss: 22.3668\n",
      "Epoch 74/100\n",
      "379/379 [==============================] - 0s 163us/sample - loss: 25.7492 - val_loss: 22.2400\n",
      "Epoch 75/100\n",
      "379/379 [==============================] - 0s 166us/sample - loss: 25.5189 - val_loss: 22.1307\n",
      "Epoch 76/100\n",
      "379/379 [==============================] - 0s 161us/sample - loss: 25.3124 - val_loss: 21.9899\n",
      "Epoch 77/100\n",
      "379/379 [==============================] - 0s 165us/sample - loss: 25.1315 - val_loss: 21.8671\n",
      "Epoch 78/100\n",
      "379/379 [==============================] - 0s 164us/sample - loss: 24.9382 - val_loss: 21.7429\n",
      "Epoch 79/100\n",
      "379/379 [==============================] - 0s 164us/sample - loss: 24.7345 - val_loss: 21.6183\n",
      "Epoch 80/100\n",
      "379/379 [==============================] - 0s 162us/sample - loss: 24.5633 - val_loss: 21.4986\n",
      "Epoch 81/100\n",
      "379/379 [==============================] - 0s 162us/sample - loss: 24.3764 - val_loss: 21.3699\n",
      "Epoch 82/100\n",
      "379/379 [==============================] - 0s 162us/sample - loss: 24.2045 - val_loss: 21.2585\n",
      "Epoch 83/100\n",
      "379/379 [==============================] - 0s 182us/sample - loss: 24.0428 - val_loss: 21.1422\n",
      "Epoch 84/100\n",
      "379/379 [==============================] - 0s 183us/sample - loss: 23.8733 - val_loss: 21.0177\n",
      "Epoch 85/100\n",
      "379/379 [==============================] - 0s 165us/sample - loss: 23.7004 - val_loss: 20.9217\n",
      "Epoch 86/100\n",
      "379/379 [==============================] - 0s 183us/sample - loss: 23.5229 - val_loss: 20.8014\n",
      "Epoch 87/100\n",
      "379/379 [==============================] - 0s 161us/sample - loss: 23.3600 - val_loss: 20.6888\n",
      "Epoch 88/100\n",
      "379/379 [==============================] - 0s 189us/sample - loss: 23.2098 - val_loss: 20.5731\n",
      "Epoch 89/100\n",
      "379/379 [==============================] - 0s 163us/sample - loss: 23.0385 - val_loss: 20.4628\n",
      "Epoch 90/100\n",
      "379/379 [==============================] - 0s 168us/sample - loss: 22.8823 - val_loss: 20.3570\n",
      "Epoch 91/100\n",
      "379/379 [==============================] - 0s 164us/sample - loss: 22.7549 - val_loss: 20.2346\n",
      "Epoch 92/100\n",
      "379/379 [==============================] - 0s 167us/sample - loss: 22.5944 - val_loss: 20.1346\n",
      "Epoch 93/100\n",
      "379/379 [==============================] - 0s 165us/sample - loss: 22.4637 - val_loss: 20.0393\n",
      "Epoch 94/100\n",
      "379/379 [==============================] - 0s 165us/sample - loss: 22.2854 - val_loss: 19.9292\n",
      "Epoch 95/100\n",
      "379/379 [==============================] - 0s 163us/sample - loss: 22.1492 - val_loss: 19.8199\n",
      "Epoch 96/100\n",
      "379/379 [==============================] - 0s 160us/sample - loss: 22.0042 - val_loss: 19.7211\n",
      "Epoch 97/100\n",
      "379/379 [==============================] - 0s 163us/sample - loss: 21.8817 - val_loss: 19.6201\n",
      "Epoch 98/100\n",
      "379/379 [==============================] - 0s 165us/sample - loss: 21.7446 - val_loss: 19.5119\n",
      "Epoch 99/100\n",
      "379/379 [==============================] - 0s 166us/sample - loss: 21.5822 - val_loss: 19.4065\n",
      "Epoch 100/100\n",
      "379/379 [==============================] - 0s 164us/sample - loss: 21.5227 - val_loss: 19.3123\n",
      "\n",
      "Forming the grid-search model #1 using: optimizer=<tensorflow.python.keras.optimizer_v2.adam.Adam object at 0x7f9132bd52d0>, kernel=<tensorflow.python.ops.init_ops_v2.RandomNormal object at 0x7f9132bbbed0>, epochs=100, batch_size=64\n",
      "Train on 379 samples, validate on 127 samples\n",
      "Epoch 1/100\n",
      "379/379 [==============================] - 1s 2ms/sample - loss: 587.0930 - val_loss: 595.9889\n",
      "Epoch 2/100\n",
      "379/379 [==============================] - 0s 105us/sample - loss: 578.9861 - val_loss: 584.3828\n",
      "Epoch 3/100\n",
      "379/379 [==============================] - 0s 110us/sample - loss: 565.4440 - val_loss: 566.4847\n",
      "Epoch 4/100\n",
      "379/379 [==============================] - 0s 107us/sample - loss: 545.5945 - val_loss: 541.1408\n",
      "Epoch 5/100\n",
      "379/379 [==============================] - 0s 109us/sample - loss: 519.0866 - val_loss: 508.8276\n",
      "Epoch 6/100\n",
      "379/379 [==============================] - 0s 105us/sample - loss: 486.0994 - val_loss: 471.3189\n",
      "Epoch 7/100\n",
      "379/379 [==============================] - 0s 114us/sample - loss: 448.4164 - val_loss: 430.2583\n",
      "Epoch 8/100\n",
      "379/379 [==============================] - 0s 112us/sample - loss: 409.0239 - val_loss: 387.9767\n",
      "Epoch 9/100\n",
      "379/379 [==============================] - 0s 104us/sample - loss: 368.8489 - val_loss: 346.3479\n",
      "Epoch 10/100\n",
      "379/379 [==============================] - 0s 102us/sample - loss: 330.3611 - val_loss: 306.4662\n",
      "Epoch 11/100\n",
      "379/379 [==============================] - 0s 110us/sample - loss: 293.9164 - val_loss: 269.6545\n",
      "Epoch 12/100\n",
      "379/379 [==============================] - 0s 103us/sample - loss: 260.7250 - val_loss: 236.1501\n",
      "Epoch 13/100\n",
      "379/379 [==============================] - 0s 99us/sample - loss: 229.7862 - val_loss: 206.6162\n",
      "Epoch 14/100\n",
      "379/379 [==============================] - 0s 102us/sample - loss: 203.6524 - val_loss: 180.4032\n",
      "Epoch 15/100\n",
      "379/379 [==============================] - 0s 104us/sample - loss: 179.7747 - val_loss: 157.7345\n",
      "Epoch 16/100\n",
      "379/379 [==============================] - 0s 121us/sample - loss: 159.7808 - val_loss: 138.1013\n",
      "Epoch 17/100\n",
      "379/379 [==============================] - 0s 110us/sample - loss: 141.4719 - val_loss: 121.8160\n",
      "Epoch 18/100\n",
      "379/379 [==============================] - 0s 106us/sample - loss: 126.6699 - val_loss: 107.9094\n",
      "Epoch 19/100\n",
      "379/379 [==============================] - 0s 113us/sample - loss: 113.7674 - val_loss: 96.2916\n",
      "Epoch 20/100\n",
      "379/379 [==============================] - 0s 111us/sample - loss: 103.0933 - val_loss: 86.4143\n",
      "Epoch 21/100\n",
      "379/379 [==============================] - 0s 112us/sample - loss: 93.3608 - val_loss: 78.1973\n",
      "Epoch 22/100\n",
      "379/379 [==============================] - 0s 136us/sample - loss: 85.3537 - val_loss: 71.1675\n",
      "Epoch 23/100\n",
      "379/379 [==============================] - 0s 111us/sample - loss: 78.0921 - val_loss: 65.0795\n",
      "Epoch 24/100\n",
      "379/379 [==============================] - 0s 118us/sample - loss: 72.1275 - val_loss: 59.7096\n",
      "Epoch 25/100\n",
      "379/379 [==============================] - 0s 113us/sample - loss: 66.2289 - val_loss: 55.1848\n",
      "Epoch 26/100\n",
      "379/379 [==============================] - 0s 110us/sample - loss: 61.5548 - val_loss: 51.1128\n",
      "Epoch 27/100\n",
      "379/379 [==============================] - 0s 107us/sample - loss: 57.2623 - val_loss: 47.5245\n",
      "Epoch 28/100\n",
      "379/379 [==============================] - 0s 111us/sample - loss: 53.4958 - val_loss: 44.3390\n",
      "Epoch 29/100\n",
      "379/379 [==============================] - 0s 107us/sample - loss: 50.1694 - val_loss: 41.4968\n",
      "Epoch 30/100\n",
      "379/379 [==============================] - 0s 110us/sample - loss: 47.0966 - val_loss: 38.9953\n",
      "Epoch 31/100\n",
      "379/379 [==============================] - 0s 111us/sample - loss: 44.2291 - val_loss: 36.8408\n",
      "Epoch 32/100\n",
      "379/379 [==============================] - 0s 114us/sample - loss: 41.9599 - val_loss: 34.8999\n",
      "Epoch 33/100\n",
      "379/379 [==============================] - 0s 107us/sample - loss: 39.8291 - val_loss: 33.2084\n",
      "Epoch 34/100\n",
      "379/379 [==============================] - 0s 106us/sample - loss: 37.9952 - val_loss: 31.7406\n",
      "Epoch 35/100\n",
      "379/379 [==============================] - 0s 109us/sample - loss: 36.3968 - val_loss: 30.4588\n",
      "Epoch 36/100\n",
      "379/379 [==============================] - 0s 112us/sample - loss: 35.0422 - val_loss: 29.3119\n",
      "Epoch 37/100\n",
      "379/379 [==============================] - 0s 135us/sample - loss: 33.7335 - val_loss: 28.3580\n",
      "Epoch 38/100\n",
      "379/379 [==============================] - 0s 109us/sample - loss: 32.7549 - val_loss: 27.4971\n",
      "Epoch 39/100\n",
      "379/379 [==============================] - 0s 109us/sample - loss: 31.7932 - val_loss: 26.7461\n",
      "Epoch 40/100\n",
      "379/379 [==============================] - 0s 107us/sample - loss: 31.0130 - val_loss: 26.0997\n",
      "Epoch 41/100\n",
      "379/379 [==============================] - 0s 111us/sample - loss: 30.2629 - val_loss: 25.5408\n",
      "Epoch 42/100\n",
      "379/379 [==============================] - 0s 110us/sample - loss: 29.6581 - val_loss: 25.0455\n",
      "Epoch 43/100\n",
      "379/379 [==============================] - 0s 110us/sample - loss: 29.1085 - val_loss: 24.6096\n",
      "Epoch 44/100\n",
      "379/379 [==============================] - 0s 109us/sample - loss: 28.5979 - val_loss: 24.2241\n",
      "Epoch 45/100\n",
      "379/379 [==============================] - 0s 109us/sample - loss: 28.1354 - val_loss: 23.8832\n",
      "Epoch 46/100\n",
      "379/379 [==============================] - 0s 110us/sample - loss: 27.7520 - val_loss: 23.5739\n",
      "Epoch 47/100\n",
      "379/379 [==============================] - 0s 110us/sample - loss: 27.3807 - val_loss: 23.2931\n",
      "Epoch 48/100\n",
      "379/379 [==============================] - 0s 107us/sample - loss: 27.0270 - val_loss: 23.0460\n",
      "Epoch 49/100\n",
      "379/379 [==============================] - 0s 110us/sample - loss: 26.7003 - val_loss: 22.8202\n",
      "Epoch 50/100\n",
      "379/379 [==============================] - 0s 109us/sample - loss: 26.4208 - val_loss: 22.6045\n",
      "Epoch 51/100\n",
      "379/379 [==============================] - 0s 110us/sample - loss: 26.1467 - val_loss: 22.3958\n",
      "Epoch 52/100\n",
      "379/379 [==============================] - 0s 110us/sample - loss: 25.9032 - val_loss: 22.2074\n",
      "Epoch 53/100\n",
      "379/379 [==============================] - 0s 108us/sample - loss: 25.6708 - val_loss: 22.0234\n",
      "Epoch 54/100\n",
      "379/379 [==============================] - 0s 110us/sample - loss: 25.4294 - val_loss: 21.8647\n",
      "Epoch 55/100\n",
      "379/379 [==============================] - 0s 116us/sample - loss: 25.2183 - val_loss: 21.7076\n",
      "Epoch 56/100\n",
      "379/379 [==============================] - 0s 122us/sample - loss: 25.0385 - val_loss: 21.5616\n",
      "Epoch 57/100\n",
      "379/379 [==============================] - 0s 105us/sample - loss: 24.8390 - val_loss: 21.4250\n",
      "Epoch 58/100\n",
      "379/379 [==============================] - 0s 107us/sample - loss: 24.6476 - val_loss: 21.2882\n",
      "Epoch 59/100\n",
      "379/379 [==============================] - 0s 111us/sample - loss: 24.4620 - val_loss: 21.1619\n",
      "Epoch 60/100\n",
      "379/379 [==============================] - 0s 110us/sample - loss: 24.3111 - val_loss: 21.0395\n",
      "Epoch 61/100\n",
      "379/379 [==============================] - 0s 112us/sample - loss: 24.1316 - val_loss: 20.9203\n",
      "Epoch 62/100\n",
      "379/379 [==============================] - 0s 110us/sample - loss: 23.9773 - val_loss: 20.8045\n",
      "Epoch 63/100\n",
      "379/379 [==============================] - 0s 107us/sample - loss: 23.8209 - val_loss: 20.6981\n",
      "Epoch 64/100\n",
      "379/379 [==============================] - 0s 110us/sample - loss: 23.6747 - val_loss: 20.5880\n",
      "Epoch 65/100\n",
      "379/379 [==============================] - 0s 110us/sample - loss: 23.5270 - val_loss: 20.4866\n",
      "Epoch 66/100\n",
      "379/379 [==============================] - 0s 110us/sample - loss: 23.3919 - val_loss: 20.3849\n",
      "Epoch 67/100\n",
      "379/379 [==============================] - 0s 109us/sample - loss: 23.2644 - val_loss: 20.2870\n",
      "Epoch 68/100\n",
      "379/379 [==============================] - 0s 112us/sample - loss: 23.1355 - val_loss: 20.1909\n",
      "Epoch 69/100\n",
      "379/379 [==============================] - 0s 110us/sample - loss: 23.0133 - val_loss: 20.0977\n",
      "Epoch 70/100\n",
      "379/379 [==============================] - 0s 108us/sample - loss: 22.8945 - val_loss: 20.0007\n",
      "Epoch 71/100\n",
      "379/379 [==============================] - 0s 108us/sample - loss: 22.7599 - val_loss: 19.9088\n",
      "Epoch 72/100\n",
      "379/379 [==============================] - 0s 112us/sample - loss: 22.6384 - val_loss: 19.8165\n",
      "Epoch 73/100\n",
      "379/379 [==============================] - 0s 107us/sample - loss: 22.5302 - val_loss: 19.7250\n",
      "Epoch 74/100\n",
      "379/379 [==============================] - 0s 124us/sample - loss: 22.4374 - val_loss: 19.6450\n",
      "Epoch 75/100\n",
      "379/379 [==============================] - 0s 129us/sample - loss: 22.3110 - val_loss: 19.5629\n",
      "Epoch 76/100\n",
      "379/379 [==============================] - 0s 111us/sample - loss: 22.1941 - val_loss: 19.4725\n",
      "Epoch 77/100\n",
      "379/379 [==============================] - 0s 109us/sample - loss: 22.1009 - val_loss: 19.3897\n",
      "Epoch 78/100\n",
      "379/379 [==============================] - 0s 110us/sample - loss: 21.9825 - val_loss: 19.3070\n",
      "Epoch 79/100\n",
      "379/379 [==============================] - 0s 112us/sample - loss: 21.8757 - val_loss: 19.2227\n",
      "Epoch 80/100\n",
      "379/379 [==============================] - 0s 108us/sample - loss: 21.7776 - val_loss: 19.1422\n",
      "Epoch 81/100\n",
      "379/379 [==============================] - 0s 106us/sample - loss: 21.6703 - val_loss: 19.0592\n",
      "Epoch 82/100\n",
      "379/379 [==============================] - 0s 110us/sample - loss: 21.5773 - val_loss: 18.9859\n",
      "Epoch 83/100\n",
      "379/379 [==============================] - 0s 111us/sample - loss: 21.4897 - val_loss: 18.9065\n",
      "Epoch 84/100\n",
      "379/379 [==============================] - 0s 109us/sample - loss: 21.3935 - val_loss: 18.8234\n",
      "Epoch 85/100\n",
      "379/379 [==============================] - 0s 107us/sample - loss: 21.2951 - val_loss: 18.7530\n",
      "Epoch 86/100\n",
      "379/379 [==============================] - 0s 113us/sample - loss: 21.1939 - val_loss: 18.6719\n",
      "Epoch 87/100\n",
      "379/379 [==============================] - 0s 114us/sample - loss: 21.0973 - val_loss: 18.5935\n",
      "Epoch 88/100\n",
      "379/379 [==============================] - 0s 110us/sample - loss: 21.0054 - val_loss: 18.5157\n",
      "Epoch 89/100\n",
      "379/379 [==============================] - 0s 111us/sample - loss: 20.9096 - val_loss: 18.4406\n",
      "Epoch 90/100\n",
      "379/379 [==============================] - 0s 111us/sample - loss: 20.8164 - val_loss: 18.3659\n",
      "Epoch 91/100\n",
      "379/379 [==============================] - 0s 138us/sample - loss: 20.7365 - val_loss: 18.2877\n",
      "Epoch 92/100\n",
      "379/379 [==============================] - 0s 108us/sample - loss: 20.6484 - val_loss: 18.2140\n",
      "Epoch 93/100\n",
      "379/379 [==============================] - 0s 114us/sample - loss: 20.5705 - val_loss: 18.1437\n",
      "Epoch 94/100\n",
      "379/379 [==============================] - 0s 133us/sample - loss: 20.4710 - val_loss: 18.0709\n",
      "Epoch 95/100\n",
      "379/379 [==============================] - 0s 108us/sample - loss: 20.3897 - val_loss: 18.0020\n",
      "Epoch 96/100\n",
      "379/379 [==============================] - 0s 112us/sample - loss: 20.3091 - val_loss: 17.9354\n",
      "Epoch 97/100\n",
      "379/379 [==============================] - 0s 109us/sample - loss: 20.2284 - val_loss: 17.8658\n",
      "Epoch 98/100\n",
      "379/379 [==============================] - 0s 107us/sample - loss: 20.1620 - val_loss: 17.7978\n",
      "Epoch 99/100\n",
      "379/379 [==============================] - 0s 105us/sample - loss: 20.0638 - val_loss: 17.7260\n",
      "Epoch 100/100\n",
      "379/379 [==============================] - 0s 110us/sample - loss: 20.0213 - val_loss: 17.6529\n",
      "\n",
      "Forming the grid-search model #2 using: optimizer=<tensorflow.python.keras.optimizer_v2.adam.Adam object at 0x7f9132bd52d0>, kernel=<tensorflow.python.ops.init_ops_v2.Orthogonal object at 0x7f9132bbb090>, epochs=100, batch_size=32\n",
      "Train on 379 samples, validate on 127 samples\n",
      "Epoch 1/100\n",
      "379/379 [==============================] - 1s 3ms/sample - loss: 567.5773 - val_loss: 544.7454\n",
      "Epoch 2/100\n",
      "379/379 [==============================] - 0s 168us/sample - loss: 508.7827 - val_loss: 470.3247\n",
      "Epoch 3/100\n",
      "379/379 [==============================] - 0s 172us/sample - loss: 435.9812 - val_loss: 388.3724\n",
      "Epoch 4/100\n",
      "379/379 [==============================] - 0s 165us/sample - loss: 361.7272 - val_loss: 307.3372\n",
      "Epoch 5/100\n",
      "379/379 [==============================] - 0s 178us/sample - loss: 293.3433 - val_loss: 237.6712\n",
      "Epoch 6/100\n",
      "379/379 [==============================] - 0s 170us/sample - loss: 235.9942 - val_loss: 185.0312\n",
      "Epoch 7/100\n",
      "379/379 [==============================] - 0s 167us/sample - loss: 190.4797 - val_loss: 147.4449\n",
      "Epoch 8/100\n",
      "379/379 [==============================] - 0s 170us/sample - loss: 156.6657 - val_loss: 120.2512\n",
      "Epoch 9/100\n",
      "379/379 [==============================] - 0s 165us/sample - loss: 130.6439 - val_loss: 99.5783\n",
      "Epoch 10/100\n",
      "379/379 [==============================] - 0s 168us/sample - loss: 109.3158 - val_loss: 83.5056\n",
      "Epoch 11/100\n",
      "379/379 [==============================] - 0s 207us/sample - loss: 92.7346 - val_loss: 70.1786\n",
      "Epoch 12/100\n",
      "379/379 [==============================] - 0s 167us/sample - loss: 78.5085 - val_loss: 59.6273\n",
      "Epoch 13/100\n",
      "379/379 [==============================] - 0s 165us/sample - loss: 67.0269 - val_loss: 51.5605\n",
      "Epoch 14/100\n",
      "379/379 [==============================] - 0s 164us/sample - loss: 58.7245 - val_loss: 44.8607\n",
      "Epoch 15/100\n",
      "379/379 [==============================] - 0s 165us/sample - loss: 51.5039 - val_loss: 39.9698\n",
      "Epoch 16/100\n",
      "379/379 [==============================] - 0s 165us/sample - loss: 46.1934 - val_loss: 36.1222\n",
      "Epoch 17/100\n",
      "379/379 [==============================] - 0s 165us/sample - loss: 42.2977 - val_loss: 32.9885\n",
      "Epoch 18/100\n",
      "379/379 [==============================] - 0s 175us/sample - loss: 38.8481 - val_loss: 30.7641\n",
      "Epoch 19/100\n",
      "379/379 [==============================] - 0s 171us/sample - loss: 36.3713 - val_loss: 28.9964\n",
      "Epoch 20/100\n",
      "379/379 [==============================] - 0s 168us/sample - loss: 34.4299 - val_loss: 27.5899\n",
      "Epoch 21/100\n",
      "379/379 [==============================] - 0s 166us/sample - loss: 32.8859 - val_loss: 26.5365\n",
      "Epoch 22/100\n",
      "379/379 [==============================] - 0s 171us/sample - loss: 31.6143 - val_loss: 25.7291\n",
      "Epoch 23/100\n",
      "379/379 [==============================] - 0s 166us/sample - loss: 30.6153 - val_loss: 25.1012\n",
      "Epoch 24/100\n",
      "379/379 [==============================] - 0s 177us/sample - loss: 29.8314 - val_loss: 24.5047\n",
      "Epoch 25/100\n",
      "379/379 [==============================] - 0s 177us/sample - loss: 29.0047 - val_loss: 24.1058\n",
      "Epoch 26/100\n",
      "379/379 [==============================] - 0s 164us/sample - loss: 28.4462 - val_loss: 23.7051\n",
      "Epoch 27/100\n",
      "379/379 [==============================] - 0s 166us/sample - loss: 27.8828 - val_loss: 23.3816\n",
      "Epoch 28/100\n",
      "379/379 [==============================] - 0s 167us/sample - loss: 27.3919 - val_loss: 23.0366\n",
      "Epoch 29/100\n",
      "379/379 [==============================] - 0s 161us/sample - loss: 26.9320 - val_loss: 22.7578\n",
      "Epoch 30/100\n",
      "379/379 [==============================] - 0s 165us/sample - loss: 26.5288 - val_loss: 22.4581\n",
      "Epoch 31/100\n",
      "379/379 [==============================] - 0s 163us/sample - loss: 26.1162 - val_loss: 22.2569\n",
      "Epoch 32/100\n",
      "379/379 [==============================] - 0s 163us/sample - loss: 25.7249 - val_loss: 21.9978\n",
      "Epoch 33/100\n",
      "379/379 [==============================] - 0s 170us/sample - loss: 25.3442 - val_loss: 21.7565\n",
      "Epoch 34/100\n",
      "379/379 [==============================] - 0s 167us/sample - loss: 25.0067 - val_loss: 21.5521\n",
      "Epoch 35/100\n",
      "379/379 [==============================] - 0s 168us/sample - loss: 24.6688 - val_loss: 21.3013\n",
      "Epoch 36/100\n",
      "379/379 [==============================] - 0s 169us/sample - loss: 24.3332 - val_loss: 21.0874\n",
      "Epoch 37/100\n",
      "379/379 [==============================] - 0s 168us/sample - loss: 24.0121 - val_loss: 20.8983\n",
      "Epoch 38/100\n",
      "379/379 [==============================] - 0s 202us/sample - loss: 23.7084 - val_loss: 20.6716\n",
      "Epoch 39/100\n",
      "379/379 [==============================] - 0s 164us/sample - loss: 23.4015 - val_loss: 20.4468\n",
      "Epoch 40/100\n",
      "379/379 [==============================] - 0s 163us/sample - loss: 23.1248 - val_loss: 20.2637\n",
      "Epoch 41/100\n",
      "379/379 [==============================] - 0s 162us/sample - loss: 22.8522 - val_loss: 20.0382\n",
      "Epoch 42/100\n",
      "379/379 [==============================] - 0s 163us/sample - loss: 22.5634 - val_loss: 19.8427\n",
      "Epoch 43/100\n",
      "379/379 [==============================] - 0s 161us/sample - loss: 22.3312 - val_loss: 19.7016\n",
      "Epoch 44/100\n",
      "379/379 [==============================] - 0s 167us/sample - loss: 22.0433 - val_loss: 19.4913\n",
      "Epoch 45/100\n",
      "379/379 [==============================] - 0s 173us/sample - loss: 21.8060 - val_loss: 19.2982\n",
      "Epoch 46/100\n",
      "379/379 [==============================] - 0s 164us/sample - loss: 21.5633 - val_loss: 19.1029\n",
      "Epoch 47/100\n",
      "379/379 [==============================] - 0s 169us/sample - loss: 21.3674 - val_loss: 18.8896\n",
      "Epoch 48/100\n",
      "379/379 [==============================] - 0s 167us/sample - loss: 21.1075 - val_loss: 18.6998\n",
      "Epoch 49/100\n",
      "379/379 [==============================] - 0s 175us/sample - loss: 20.8413 - val_loss: 18.5181\n",
      "Epoch 50/100\n",
      "379/379 [==============================] - 0s 167us/sample - loss: 20.6387 - val_loss: 18.3298\n",
      "Epoch 51/100\n",
      "379/379 [==============================] - 0s 161us/sample - loss: 20.4029 - val_loss: 18.1313\n",
      "Epoch 52/100\n",
      "379/379 [==============================] - 0s 166us/sample - loss: 20.2010 - val_loss: 17.9274\n",
      "Epoch 53/100\n",
      "379/379 [==============================] - 0s 167us/sample - loss: 19.9754 - val_loss: 17.7273\n",
      "Epoch 54/100\n",
      "379/379 [==============================] - 0s 170us/sample - loss: 19.7473 - val_loss: 17.5798\n",
      "Epoch 55/100\n",
      "379/379 [==============================] - 0s 165us/sample - loss: 19.5495 - val_loss: 17.3911\n",
      "Epoch 56/100\n",
      "379/379 [==============================] - 0s 168us/sample - loss: 19.3597 - val_loss: 17.2169\n",
      "Epoch 57/100\n",
      "379/379 [==============================] - 0s 172us/sample - loss: 19.1815 - val_loss: 17.0827\n",
      "Epoch 58/100\n",
      "379/379 [==============================] - 0s 164us/sample - loss: 18.9691 - val_loss: 16.8613\n",
      "Epoch 59/100\n",
      "379/379 [==============================] - 0s 170us/sample - loss: 18.7494 - val_loss: 16.6892\n",
      "Epoch 60/100\n",
      "379/379 [==============================] - 0s 160us/sample - loss: 18.5666 - val_loss: 16.5275\n",
      "Epoch 61/100\n",
      "379/379 [==============================] - 0s 166us/sample - loss: 18.3656 - val_loss: 16.3643\n",
      "Epoch 62/100\n",
      "379/379 [==============================] - 0s 163us/sample - loss: 18.1826 - val_loss: 16.2181\n",
      "Epoch 63/100\n",
      "379/379 [==============================] - 0s 167us/sample - loss: 18.0136 - val_loss: 16.0557\n",
      "Epoch 64/100\n",
      "379/379 [==============================] - 0s 151us/sample - loss: 17.8272 - val_loss: 15.8809\n",
      "Epoch 65/100\n",
      "379/379 [==============================] - 0s 165us/sample - loss: 17.6523 - val_loss: 15.7370\n",
      "Epoch 66/100\n",
      "379/379 [==============================] - 0s 158us/sample - loss: 17.4738 - val_loss: 15.5884\n",
      "Epoch 67/100\n",
      "379/379 [==============================] - 0s 149us/sample - loss: 17.2898 - val_loss: 15.4508\n",
      "Epoch 68/100\n",
      "379/379 [==============================] - 0s 147us/sample - loss: 17.1651 - val_loss: 15.2612\n",
      "Epoch 69/100\n",
      "379/379 [==============================] - 0s 150us/sample - loss: 16.9773 - val_loss: 15.1316\n",
      "Epoch 70/100\n",
      "379/379 [==============================] - 0s 145us/sample - loss: 16.8195 - val_loss: 14.9706\n",
      "Epoch 71/100\n",
      "379/379 [==============================] - 0s 147us/sample - loss: 16.6482 - val_loss: 14.8564\n",
      "Epoch 72/100\n",
      "379/379 [==============================] - 0s 180us/sample - loss: 16.4991 - val_loss: 14.6850\n",
      "Epoch 73/100\n",
      "379/379 [==============================] - 0s 163us/sample - loss: 16.3373 - val_loss: 14.5697\n",
      "Epoch 74/100\n",
      "379/379 [==============================] - 0s 180us/sample - loss: 16.2418 - val_loss: 14.4438\n",
      "Epoch 75/100\n",
      "379/379 [==============================] - 0s 164us/sample - loss: 16.0586 - val_loss: 14.3147\n",
      "Epoch 76/100\n",
      "379/379 [==============================] - 0s 169us/sample - loss: 15.8743 - val_loss: 14.1528\n",
      "Epoch 77/100\n",
      "379/379 [==============================] - 0s 168us/sample - loss: 15.7564 - val_loss: 14.0167\n",
      "Epoch 78/100\n",
      "379/379 [==============================] - 0s 166us/sample - loss: 15.5991 - val_loss: 13.8780\n",
      "Epoch 79/100\n",
      "379/379 [==============================] - 0s 188us/sample - loss: 15.4368 - val_loss: 13.7582\n",
      "Epoch 80/100\n",
      "379/379 [==============================] - 0s 166us/sample - loss: 15.3245 - val_loss: 13.6349\n",
      "Epoch 81/100\n",
      "379/379 [==============================] - 0s 170us/sample - loss: 15.1703 - val_loss: 13.4821\n",
      "Epoch 82/100\n",
      "379/379 [==============================] - 0s 165us/sample - loss: 15.0513 - val_loss: 13.3841\n",
      "Epoch 83/100\n",
      "379/379 [==============================] - 0s 166us/sample - loss: 14.9353 - val_loss: 13.2340\n",
      "Epoch 84/100\n",
      "379/379 [==============================] - 0s 170us/sample - loss: 14.8017 - val_loss: 13.0789\n",
      "Epoch 85/100\n",
      "379/379 [==============================] - 0s 165us/sample - loss: 14.6812 - val_loss: 12.9853\n",
      "Epoch 86/100\n",
      "379/379 [==============================] - 0s 168us/sample - loss: 14.5580 - val_loss: 12.8422\n",
      "Epoch 87/100\n",
      "379/379 [==============================] - 0s 169us/sample - loss: 14.4536 - val_loss: 12.6955\n",
      "Epoch 88/100\n",
      "379/379 [==============================] - 0s 162us/sample - loss: 14.3342 - val_loss: 12.6135\n",
      "Epoch 89/100\n",
      "379/379 [==============================] - 0s 170us/sample - loss: 14.2258 - val_loss: 12.4918\n",
      "Epoch 90/100\n",
      "379/379 [==============================] - 0s 180us/sample - loss: 14.1207 - val_loss: 12.3891\n",
      "Epoch 91/100\n",
      "379/379 [==============================] - 0s 177us/sample - loss: 14.0415 - val_loss: 12.2700\n",
      "Epoch 92/100\n",
      "379/379 [==============================] - 0s 163us/sample - loss: 13.9375 - val_loss: 12.1758\n",
      "Epoch 93/100\n",
      "379/379 [==============================] - 0s 162us/sample - loss: 13.8686 - val_loss: 12.1087\n",
      "Epoch 94/100\n",
      "379/379 [==============================] - 0s 164us/sample - loss: 13.7405 - val_loss: 11.9904\n",
      "Epoch 95/100\n",
      "379/379 [==============================] - 0s 170us/sample - loss: 13.6719 - val_loss: 11.8867\n",
      "Epoch 96/100\n",
      "379/379 [==============================] - 0s 167us/sample - loss: 13.5729 - val_loss: 11.8374\n",
      "Epoch 97/100\n",
      "379/379 [==============================] - 0s 163us/sample - loss: 13.5134 - val_loss: 11.7715\n",
      "Epoch 98/100\n",
      "379/379 [==============================] - 0s 161us/sample - loss: 13.4490 - val_loss: 11.6497\n",
      "Epoch 99/100\n",
      "379/379 [==============================] - 0s 169us/sample - loss: 13.3343 - val_loss: 11.5643\n",
      "Epoch 100/100\n",
      "379/379 [==============================] - 0s 176us/sample - loss: 13.3225 - val_loss: 11.5352\n",
      "\n",
      "Forming the grid-search model #3 using: optimizer=<tensorflow.python.keras.optimizer_v2.adam.Adam object at 0x7f9132bd52d0>, kernel=<tensorflow.python.ops.init_ops_v2.Orthogonal object at 0x7f9132bbb090>, epochs=100, batch_size=64\n",
      "Train on 379 samples, validate on 127 samples\n",
      "Epoch 1/100\n",
      "379/379 [==============================] - 1s 2ms/sample - loss: 579.1387 - val_loss: 573.3746\n",
      "Epoch 2/100\n",
      "379/379 [==============================] - 0s 108us/sample - loss: 551.3690 - val_loss: 538.7910\n",
      "Epoch 3/100\n",
      "379/379 [==============================] - 0s 107us/sample - loss: 516.9361 - val_loss: 499.0780\n",
      "Epoch 4/100\n",
      "379/379 [==============================] - 0s 112us/sample - loss: 479.2007 - val_loss: 454.9653\n",
      "Epoch 5/100\n",
      "379/379 [==============================] - 0s 109us/sample - loss: 438.6757 - val_loss: 407.8095\n",
      "Epoch 6/100\n",
      "379/379 [==============================] - 0s 109us/sample - loss: 395.4978 - val_loss: 360.5852\n",
      "Epoch 7/100\n",
      "379/379 [==============================] - 0s 118us/sample - loss: 352.7172 - val_loss: 314.6471\n",
      "Epoch 8/100\n",
      "379/379 [==============================] - 0s 110us/sample - loss: 312.9649 - val_loss: 271.9920\n",
      "Epoch 9/100\n",
      "379/379 [==============================] - 0s 110us/sample - loss: 275.8333 - val_loss: 234.3475\n",
      "Epoch 10/100\n",
      "379/379 [==============================] - 0s 110us/sample - loss: 243.4738 - val_loss: 202.0039\n",
      "Epoch 11/100\n",
      "379/379 [==============================] - 0s 112us/sample - loss: 215.3210 - val_loss: 175.4288\n",
      "Epoch 12/100\n",
      "379/379 [==============================] - 0s 110us/sample - loss: 191.1578 - val_loss: 153.9102\n",
      "Epoch 13/100\n",
      "379/379 [==============================] - 0s 109us/sample - loss: 170.3634 - val_loss: 136.6834\n",
      "Epoch 14/100\n",
      "379/379 [==============================] - 0s 121us/sample - loss: 153.6039 - val_loss: 122.2808\n",
      "Epoch 15/100\n",
      "379/379 [==============================] - 0s 109us/sample - loss: 138.4911 - val_loss: 110.1454\n",
      "Epoch 16/100\n",
      "379/379 [==============================] - 0s 113us/sample - loss: 125.4849 - val_loss: 99.5862\n",
      "Epoch 17/100\n",
      "379/379 [==============================] - 0s 120us/sample - loss: 113.6215 - val_loss: 90.3049\n",
      "Epoch 18/100\n",
      "379/379 [==============================] - 0s 108us/sample - loss: 103.0339 - val_loss: 82.0385\n",
      "Epoch 19/100\n",
      "379/379 [==============================] - 0s 109us/sample - loss: 93.6983 - val_loss: 74.6066\n",
      "Epoch 20/100\n",
      "379/379 [==============================] - 0s 108us/sample - loss: 85.3779 - val_loss: 67.9918\n",
      "Epoch 21/100\n",
      "379/379 [==============================] - 0s 111us/sample - loss: 77.8219 - val_loss: 62.1842\n",
      "Epoch 22/100\n",
      "379/379 [==============================] - 0s 107us/sample - loss: 71.4978 - val_loss: 56.9892\n",
      "Epoch 23/100\n",
      "379/379 [==============================] - 0s 111us/sample - loss: 65.5478 - val_loss: 52.4897\n",
      "Epoch 24/100\n",
      "379/379 [==============================] - 0s 109us/sample - loss: 60.8388 - val_loss: 48.4941\n",
      "Epoch 25/100\n",
      "379/379 [==============================] - 0s 107us/sample - loss: 56.2076 - val_loss: 45.1642\n",
      "Epoch 26/100\n",
      "379/379 [==============================] - 0s 106us/sample - loss: 52.5634 - val_loss: 42.2595\n",
      "Epoch 27/100\n",
      "379/379 [==============================] - 0s 109us/sample - loss: 49.3569 - val_loss: 39.7632\n",
      "Epoch 28/100\n",
      "379/379 [==============================] - 0s 110us/sample - loss: 46.7014 - val_loss: 37.5584\n",
      "Epoch 29/100\n",
      "379/379 [==============================] - 0s 140us/sample - loss: 44.3261 - val_loss: 35.6669\n",
      "Epoch 30/100\n",
      "379/379 [==============================] - 0s 109us/sample - loss: 42.3361 - val_loss: 34.0105\n",
      "Epoch 31/100\n",
      "379/379 [==============================] - 0s 118us/sample - loss: 40.4035 - val_loss: 32.6576\n",
      "Epoch 32/100\n",
      "379/379 [==============================] - 0s 108us/sample - loss: 38.9321 - val_loss: 31.4488\n",
      "Epoch 33/100\n",
      "379/379 [==============================] - 0s 109us/sample - loss: 37.6188 - val_loss: 30.3915\n",
      "Epoch 34/100\n",
      "379/379 [==============================] - 0s 106us/sample - loss: 36.3846 - val_loss: 29.5119\n",
      "Epoch 35/100\n",
      "379/379 [==============================] - 0s 106us/sample - loss: 35.4068 - val_loss: 28.7246\n",
      "Epoch 36/100\n",
      "379/379 [==============================] - 0s 112us/sample - loss: 34.4962 - val_loss: 28.0440\n",
      "Epoch 37/100\n",
      "379/379 [==============================] - 0s 112us/sample - loss: 33.6299 - val_loss: 27.4804\n",
      "Epoch 38/100\n",
      "379/379 [==============================] - 0s 110us/sample - loss: 32.9819 - val_loss: 26.9401\n",
      "Epoch 39/100\n",
      "379/379 [==============================] - 0s 130us/sample - loss: 32.2907 - val_loss: 26.4647\n",
      "Epoch 40/100\n",
      "379/379 [==============================] - 0s 110us/sample - loss: 31.7041 - val_loss: 26.0469\n",
      "Epoch 41/100\n",
      "379/379 [==============================] - 0s 113us/sample - loss: 31.1340 - val_loss: 25.6716\n",
      "Epoch 42/100\n",
      "379/379 [==============================] - 0s 106us/sample - loss: 30.6353 - val_loss: 25.3285\n",
      "Epoch 43/100\n",
      "379/379 [==============================] - 0s 109us/sample - loss: 30.1568 - val_loss: 25.0373\n",
      "Epoch 44/100\n",
      "379/379 [==============================] - 0s 109us/sample - loss: 29.7177 - val_loss: 24.7555\n",
      "Epoch 45/100\n",
      "379/379 [==============================] - 0s 126us/sample - loss: 29.3109 - val_loss: 24.4922\n",
      "Epoch 46/100\n",
      "379/379 [==============================] - 0s 112us/sample - loss: 28.9256 - val_loss: 24.2537\n",
      "Epoch 47/100\n",
      "379/379 [==============================] - 0s 110us/sample - loss: 28.5584 - val_loss: 24.0217\n",
      "Epoch 48/100\n",
      "379/379 [==============================] - 0s 108us/sample - loss: 28.2126 - val_loss: 23.7971\n",
      "Epoch 49/100\n",
      "379/379 [==============================] - 0s 108us/sample - loss: 27.8528 - val_loss: 23.5916\n",
      "Epoch 50/100\n",
      "379/379 [==============================] - 0s 110us/sample - loss: 27.5259 - val_loss: 23.3974\n",
      "Epoch 51/100\n",
      "379/379 [==============================] - 0s 107us/sample - loss: 27.1971 - val_loss: 23.2020\n",
      "Epoch 52/100\n",
      "379/379 [==============================] - 0s 106us/sample - loss: 26.9391 - val_loss: 23.0001\n",
      "Epoch 53/100\n",
      "379/379 [==============================] - 0s 109us/sample - loss: 26.6410 - val_loss: 22.8038\n",
      "Epoch 54/100\n",
      "379/379 [==============================] - 0s 109us/sample - loss: 26.3437 - val_loss: 22.6395\n",
      "Epoch 55/100\n",
      "379/379 [==============================] - 0s 109us/sample - loss: 26.0868 - val_loss: 22.4678\n",
      "Epoch 56/100\n",
      "379/379 [==============================] - 0s 110us/sample - loss: 25.8342 - val_loss: 22.3051\n",
      "Epoch 57/100\n",
      "379/379 [==============================] - 0s 105us/sample - loss: 25.5766 - val_loss: 22.1594\n",
      "Epoch 58/100\n",
      "379/379 [==============================] - 0s 107us/sample - loss: 25.3397 - val_loss: 21.9829\n",
      "Epoch 59/100\n",
      "379/379 [==============================] - 0s 109us/sample - loss: 25.1009 - val_loss: 21.8192\n",
      "Epoch 60/100\n",
      "379/379 [==============================] - 0s 110us/sample - loss: 24.8746 - val_loss: 21.6563\n",
      "Epoch 61/100\n",
      "379/379 [==============================] - 0s 110us/sample - loss: 24.6365 - val_loss: 21.5041\n",
      "Epoch 62/100\n",
      "379/379 [==============================] - 0s 111us/sample - loss: 24.4148 - val_loss: 21.3549\n",
      "Epoch 63/100\n",
      "379/379 [==============================] - 0s 110us/sample - loss: 24.2039 - val_loss: 21.2137\n",
      "Epoch 64/100\n",
      "379/379 [==============================] - 0s 106us/sample - loss: 23.9944 - val_loss: 21.0658\n",
      "Epoch 65/100\n",
      "379/379 [==============================] - 0s 102us/sample - loss: 23.7860 - val_loss: 20.9224\n",
      "Epoch 66/100\n",
      "379/379 [==============================] - 0s 104us/sample - loss: 23.5835 - val_loss: 20.7742\n",
      "Epoch 67/100\n",
      "379/379 [==============================] - 0s 103us/sample - loss: 23.3857 - val_loss: 20.6360\n",
      "Epoch 68/100\n",
      "379/379 [==============================] - 0s 109us/sample - loss: 23.2053 - val_loss: 20.4863\n",
      "Epoch 69/100\n",
      "379/379 [==============================] - 0s 108us/sample - loss: 23.0086 - val_loss: 20.3443\n",
      "Epoch 70/100\n",
      "379/379 [==============================] - 0s 104us/sample - loss: 22.8273 - val_loss: 20.1966\n",
      "Epoch 71/100\n",
      "379/379 [==============================] - 0s 101us/sample - loss: 22.6246 - val_loss: 20.0635\n",
      "Epoch 72/100\n",
      "379/379 [==============================] - 0s 102us/sample - loss: 22.4400 - val_loss: 19.9175\n",
      "Epoch 73/100\n",
      "379/379 [==============================] - 0s 108us/sample - loss: 22.2600 - val_loss: 19.7871\n",
      "Epoch 74/100\n",
      "379/379 [==============================] - 0s 118us/sample - loss: 22.1194 - val_loss: 19.6519\n",
      "Epoch 75/100\n",
      "379/379 [==============================] - 0s 111us/sample - loss: 21.9222 - val_loss: 19.5253\n",
      "Epoch 76/100\n",
      "379/379 [==============================] - 0s 114us/sample - loss: 21.7369 - val_loss: 19.3898\n",
      "Epoch 77/100\n",
      "379/379 [==============================] - 0s 109us/sample - loss: 21.5970 - val_loss: 19.2545\n",
      "Epoch 78/100\n",
      "379/379 [==============================] - 0s 108us/sample - loss: 21.4112 - val_loss: 19.1211\n",
      "Epoch 79/100\n",
      "379/379 [==============================] - 0s 107us/sample - loss: 21.2370 - val_loss: 18.9942\n",
      "Epoch 80/100\n",
      "379/379 [==============================] - 0s 111us/sample - loss: 21.0808 - val_loss: 18.8670\n",
      "Epoch 81/100\n",
      "379/379 [==============================] - 0s 107us/sample - loss: 20.9148 - val_loss: 18.7349\n",
      "Epoch 82/100\n",
      "379/379 [==============================] - 0s 114us/sample - loss: 20.7664 - val_loss: 18.6033\n",
      "Epoch 83/100\n",
      "379/379 [==============================] - 0s 118us/sample - loss: 20.6120 - val_loss: 18.4726\n",
      "Epoch 84/100\n",
      "379/379 [==============================] - 0s 107us/sample - loss: 20.4582 - val_loss: 18.3331\n",
      "Epoch 85/100\n",
      "379/379 [==============================] - 0s 111us/sample - loss: 20.2966 - val_loss: 18.2052\n",
      "Epoch 86/100\n",
      "379/379 [==============================] - 0s 108us/sample - loss: 20.1383 - val_loss: 18.0642\n",
      "Epoch 87/100\n",
      "379/379 [==============================] - 0s 111us/sample - loss: 19.9811 - val_loss: 17.9192\n",
      "Epoch 88/100\n",
      "379/379 [==============================] - 0s 106us/sample - loss: 19.8312 - val_loss: 17.7849\n",
      "Epoch 89/100\n",
      "379/379 [==============================] - 0s 112us/sample - loss: 19.6752 - val_loss: 17.6518\n",
      "Epoch 90/100\n",
      "379/379 [==============================] - 0s 111us/sample - loss: 19.5311 - val_loss: 17.5256\n",
      "Epoch 91/100\n",
      "379/379 [==============================] - 0s 115us/sample - loss: 19.3954 - val_loss: 17.3958\n",
      "Epoch 92/100\n",
      "379/379 [==============================] - 0s 111us/sample - loss: 19.2511 - val_loss: 17.2777\n",
      "Epoch 93/100\n",
      "379/379 [==============================] - 0s 109us/sample - loss: 19.1199 - val_loss: 17.1611\n",
      "Epoch 94/100\n",
      "379/379 [==============================] - 0s 110us/sample - loss: 18.9710 - val_loss: 17.0416\n",
      "Epoch 95/100\n",
      "379/379 [==============================] - 0s 118us/sample - loss: 18.8383 - val_loss: 16.9230\n",
      "Epoch 96/100\n",
      "379/379 [==============================] - 0s 98us/sample - loss: 18.7078 - val_loss: 16.8121\n",
      "Epoch 97/100\n",
      "379/379 [==============================] - 0s 101us/sample - loss: 18.5738 - val_loss: 16.7052\n",
      "Epoch 98/100\n",
      "379/379 [==============================] - 0s 106us/sample - loss: 18.4628 - val_loss: 16.5857\n",
      "Epoch 99/100\n",
      "379/379 [==============================] - 0s 108us/sample - loss: 18.3072 - val_loss: 16.4674\n",
      "Epoch 100/100\n",
      "379/379 [==============================] - 0s 107us/sample - loss: 18.2125 - val_loss: 16.3637\n",
      "\n",
      "Forming the grid-search model #4 using: optimizer=<tensorflow.python.keras.optimizer_v2.adam.Adam object at 0x7f9132bd52d0>, kernel=<tensorflow.python.ops.init_ops_v2.GlorotNormal object at 0x7f9132ed1050>, epochs=100, batch_size=32\n",
      "Train on 379 samples, validate on 127 samples\n",
      "Epoch 1/100\n",
      "379/379 [==============================] - 1s 2ms/sample - loss: 543.8647 - val_loss: 522.1878\n",
      "Epoch 2/100\n",
      "379/379 [==============================] - 0s 168us/sample - loss: 468.4231 - val_loss: 432.3887\n",
      "Epoch 3/100\n",
      "379/379 [==============================] - 0s 164us/sample - loss: 379.4935 - val_loss: 338.0024\n",
      "Epoch 4/100\n",
      "379/379 [==============================] - 0s 166us/sample - loss: 291.9668 - val_loss: 247.9587\n",
      "Epoch 5/100\n",
      "379/379 [==============================] - 0s 165us/sample - loss: 215.1834 - val_loss: 175.2392\n",
      "Epoch 6/100\n",
      "379/379 [==============================] - 0s 166us/sample - loss: 156.5292 - val_loss: 126.1792\n",
      "Epoch 7/100\n",
      "379/379 [==============================] - 0s 166us/sample - loss: 116.3793 - val_loss: 96.6524\n",
      "Epoch 8/100\n",
      "379/379 [==============================] - 0s 163us/sample - loss: 91.9061 - val_loss: 78.6772\n",
      "Epoch 9/100\n",
      "379/379 [==============================] - 0s 165us/sample - loss: 76.4196 - val_loss: 66.3334\n",
      "Epoch 10/100\n",
      "379/379 [==============================] - 0s 164us/sample - loss: 65.1530 - val_loss: 57.1487\n",
      "Epoch 11/100\n",
      "379/379 [==============================] - 0s 173us/sample - loss: 56.9095 - val_loss: 49.7433\n",
      "Epoch 12/100\n",
      "379/379 [==============================] - 0s 164us/sample - loss: 50.3173 - val_loss: 43.8547\n",
      "Epoch 13/100\n",
      "379/379 [==============================] - 0s 168us/sample - loss: 45.0571 - val_loss: 39.4690\n",
      "Epoch 14/100\n",
      "379/379 [==============================] - 0s 163us/sample - loss: 41.3177 - val_loss: 35.8396\n",
      "Epoch 15/100\n",
      "379/379 [==============================] - 0s 168us/sample - loss: 38.1880 - val_loss: 33.1079\n",
      "Epoch 16/100\n",
      "379/379 [==============================] - 0s 163us/sample - loss: 35.7169 - val_loss: 31.0077\n",
      "Epoch 17/100\n",
      "379/379 [==============================] - 0s 166us/sample - loss: 33.9983 - val_loss: 29.2656\n",
      "Epoch 18/100\n",
      "379/379 [==============================] - 0s 168us/sample - loss: 32.3314 - val_loss: 28.0170\n",
      "Epoch 19/100\n",
      "379/379 [==============================] - 0s 164us/sample - loss: 31.1388 - val_loss: 26.9391\n",
      "Epoch 20/100\n",
      "379/379 [==============================] - 0s 156us/sample - loss: 30.1427 - val_loss: 26.0242\n",
      "Epoch 21/100\n",
      "379/379 [==============================] - 0s 172us/sample - loss: 29.2617 - val_loss: 25.3253\n",
      "Epoch 22/100\n",
      "379/379 [==============================] - 0s 149us/sample - loss: 28.4515 - val_loss: 24.7558\n",
      "Epoch 23/100\n",
      "379/379 [==============================] - 0s 156us/sample - loss: 27.8068 - val_loss: 24.2282\n",
      "Epoch 24/100\n",
      "379/379 [==============================] - 0s 145us/sample - loss: 27.2045 - val_loss: 23.7555\n",
      "Epoch 25/100\n",
      "379/379 [==============================] - 0s 146us/sample - loss: 26.5638 - val_loss: 23.3641\n",
      "Epoch 26/100\n",
      "379/379 [==============================] - 0s 151us/sample - loss: 26.0708 - val_loss: 22.9931\n",
      "Epoch 27/100\n",
      "379/379 [==============================] - 0s 148us/sample - loss: 25.5568 - val_loss: 22.6886\n",
      "Epoch 28/100\n",
      "379/379 [==============================] - 0s 152us/sample - loss: 25.0921 - val_loss: 22.3192\n",
      "Epoch 29/100\n",
      "379/379 [==============================] - 0s 163us/sample - loss: 24.6652 - val_loss: 22.0043\n",
      "Epoch 30/100\n",
      "379/379 [==============================] - 0s 162us/sample - loss: 24.2629 - val_loss: 21.7411\n",
      "Epoch 31/100\n",
      "379/379 [==============================] - 0s 166us/sample - loss: 23.8711 - val_loss: 21.5343\n",
      "Epoch 32/100\n",
      "379/379 [==============================] - 0s 166us/sample - loss: 23.5082 - val_loss: 21.2218\n",
      "Epoch 33/100\n",
      "379/379 [==============================] - 0s 162us/sample - loss: 23.1556 - val_loss: 20.9779\n",
      "Epoch 34/100\n",
      "379/379 [==============================] - 0s 166us/sample - loss: 22.8407 - val_loss: 20.7595\n",
      "Epoch 35/100\n",
      "379/379 [==============================] - 0s 164us/sample - loss: 22.5273 - val_loss: 20.5088\n",
      "Epoch 36/100\n",
      "379/379 [==============================] - 0s 164us/sample - loss: 22.2332 - val_loss: 20.2888\n",
      "Epoch 37/100\n",
      "379/379 [==============================] - 0s 161us/sample - loss: 21.9414 - val_loss: 20.0926\n",
      "Epoch 38/100\n",
      "379/379 [==============================] - 0s 163us/sample - loss: 21.6600 - val_loss: 19.9118\n",
      "Epoch 39/100\n",
      "379/379 [==============================] - 0s 166us/sample - loss: 21.4004 - val_loss: 19.7106\n",
      "Epoch 40/100\n",
      "379/379 [==============================] - 0s 166us/sample - loss: 21.1589 - val_loss: 19.4978\n",
      "Epoch 41/100\n",
      "379/379 [==============================] - 0s 163us/sample - loss: 20.9430 - val_loss: 19.2946\n",
      "Epoch 42/100\n",
      "379/379 [==============================] - 0s 164us/sample - loss: 20.6819 - val_loss: 19.1061\n",
      "Epoch 43/100\n",
      "379/379 [==============================] - 0s 169us/sample - loss: 20.4842 - val_loss: 18.9986\n",
      "Epoch 44/100\n",
      "379/379 [==============================] - 0s 166us/sample - loss: 20.2368 - val_loss: 18.7952\n",
      "Epoch 45/100\n",
      "379/379 [==============================] - 0s 164us/sample - loss: 20.0169 - val_loss: 18.6501\n",
      "Epoch 46/100\n",
      "379/379 [==============================] - 0s 161us/sample - loss: 19.8138 - val_loss: 18.4900\n",
      "Epoch 47/100\n",
      "379/379 [==============================] - 0s 165us/sample - loss: 19.6535 - val_loss: 18.2208\n",
      "Epoch 48/100\n",
      "379/379 [==============================] - 0s 168us/sample - loss: 19.4275 - val_loss: 18.1219\n",
      "Epoch 49/100\n",
      "379/379 [==============================] - 0s 166us/sample - loss: 19.2111 - val_loss: 17.9715\n",
      "Epoch 50/100\n",
      "379/379 [==============================] - 0s 161us/sample - loss: 19.0446 - val_loss: 17.7956\n",
      "Epoch 51/100\n",
      "379/379 [==============================] - 0s 167us/sample - loss: 18.8601 - val_loss: 17.6365\n",
      "Epoch 52/100\n",
      "379/379 [==============================] - 0s 163us/sample - loss: 18.7073 - val_loss: 17.4689\n",
      "Epoch 53/100\n",
      "379/379 [==============================] - 0s 161us/sample - loss: 18.5384 - val_loss: 17.3206\n",
      "Epoch 54/100\n",
      "379/379 [==============================] - 0s 162us/sample - loss: 18.3660 - val_loss: 17.2198\n",
      "Epoch 55/100\n",
      "379/379 [==============================] - 0s 151us/sample - loss: 18.2097 - val_loss: 17.0870\n",
      "Epoch 56/100\n",
      "379/379 [==============================] - 0s 153us/sample - loss: 18.0747 - val_loss: 16.9984\n",
      "Epoch 57/100\n",
      "379/379 [==============================] - 0s 152us/sample - loss: 17.9570 - val_loss: 16.8715\n",
      "Epoch 58/100\n",
      "379/379 [==============================] - 0s 146us/sample - loss: 17.7976 - val_loss: 16.6255\n",
      "Epoch 59/100\n",
      "379/379 [==============================] - 0s 156us/sample - loss: 17.6270 - val_loss: 16.5239\n",
      "Epoch 60/100\n",
      "379/379 [==============================] - 0s 148us/sample - loss: 17.4928 - val_loss: 16.4532\n",
      "Epoch 61/100\n",
      "379/379 [==============================] - 0s 165us/sample - loss: 17.3392 - val_loss: 16.3059\n",
      "Epoch 62/100\n",
      "379/379 [==============================] - 0s 150us/sample - loss: 17.2071 - val_loss: 16.2319\n",
      "Epoch 63/100\n",
      "379/379 [==============================] - 0s 169us/sample - loss: 17.0847 - val_loss: 16.0659\n",
      "Epoch 64/100\n",
      "379/379 [==============================] - 0s 166us/sample - loss: 16.9431 - val_loss: 15.9559\n",
      "Epoch 65/100\n",
      "379/379 [==============================] - 0s 162us/sample - loss: 16.8193 - val_loss: 15.8563\n",
      "Epoch 66/100\n",
      "379/379 [==============================] - 0s 165us/sample - loss: 16.6865 - val_loss: 15.7428\n",
      "Epoch 67/100\n",
      "379/379 [==============================] - 0s 163us/sample - loss: 16.5509 - val_loss: 15.6764\n",
      "Epoch 68/100\n",
      "379/379 [==============================] - 0s 164us/sample - loss: 16.4832 - val_loss: 15.5165\n",
      "Epoch 69/100\n",
      "379/379 [==============================] - 0s 163us/sample - loss: 16.3369 - val_loss: 15.4734\n",
      "Epoch 70/100\n",
      "379/379 [==============================] - 0s 162us/sample - loss: 16.2249 - val_loss: 15.3484\n",
      "Epoch 71/100\n",
      "379/379 [==============================] - 0s 161us/sample - loss: 16.1057 - val_loss: 15.2628\n",
      "Epoch 72/100\n",
      "379/379 [==============================] - 0s 163us/sample - loss: 16.0077 - val_loss: 15.0982\n",
      "Epoch 73/100\n",
      "379/379 [==============================] - 0s 166us/sample - loss: 15.9010 - val_loss: 15.0427\n",
      "Epoch 74/100\n",
      "379/379 [==============================] - 0s 166us/sample - loss: 15.8304 - val_loss: 15.0021\n",
      "Epoch 75/100\n",
      "379/379 [==============================] - 0s 167us/sample - loss: 15.7001 - val_loss: 14.8978\n",
      "Epoch 76/100\n",
      "379/379 [==============================] - 0s 165us/sample - loss: 15.5806 - val_loss: 14.7760\n",
      "Epoch 77/100\n",
      "379/379 [==============================] - 0s 166us/sample - loss: 15.5036 - val_loss: 14.6982\n",
      "Epoch 78/100\n",
      "379/379 [==============================] - 0s 166us/sample - loss: 15.3963 - val_loss: 14.6050\n",
      "Epoch 79/100\n",
      "379/379 [==============================] - 0s 163us/sample - loss: 15.2884 - val_loss: 14.5363\n",
      "Epoch 80/100\n",
      "379/379 [==============================] - 0s 167us/sample - loss: 15.2066 - val_loss: 14.4686\n",
      "Epoch 81/100\n",
      "379/379 [==============================] - 0s 162us/sample - loss: 15.0974 - val_loss: 14.3792\n",
      "Epoch 82/100\n",
      "379/379 [==============================] - 0s 142us/sample - loss: 15.0157 - val_loss: 14.3398\n",
      "Epoch 83/100\n",
      "379/379 [==============================] - 0s 152us/sample - loss: 14.9382 - val_loss: 14.2267\n",
      "Epoch 84/100\n",
      "379/379 [==============================] - 0s 160us/sample - loss: 14.8355 - val_loss: 14.1341\n",
      "Epoch 85/100\n",
      "379/379 [==============================] - 0s 155us/sample - loss: 14.7421 - val_loss: 14.0921\n",
      "Epoch 86/100\n",
      "379/379 [==============================] - 0s 155us/sample - loss: 14.6481 - val_loss: 13.9718\n",
      "Epoch 87/100\n",
      "379/379 [==============================] - 0s 150us/sample - loss: 14.5685 - val_loss: 13.8693\n",
      "Epoch 88/100\n",
      "379/379 [==============================] - 0s 157us/sample - loss: 14.4806 - val_loss: 13.8361\n",
      "Epoch 89/100\n",
      "379/379 [==============================] - 0s 150us/sample - loss: 14.3964 - val_loss: 13.7482\n",
      "Epoch 90/100\n",
      "379/379 [==============================] - 0s 156us/sample - loss: 14.3093 - val_loss: 13.6687\n",
      "Epoch 91/100\n",
      "379/379 [==============================] - 0s 166us/sample - loss: 14.2476 - val_loss: 13.5811\n",
      "Epoch 92/100\n",
      "379/379 [==============================] - 0s 168us/sample - loss: 14.1659 - val_loss: 13.5113\n",
      "Epoch 93/100\n",
      "379/379 [==============================] - 0s 162us/sample - loss: 14.1129 - val_loss: 13.4938\n",
      "Epoch 94/100\n",
      "379/379 [==============================] - 0s 163us/sample - loss: 14.0095 - val_loss: 13.3840\n",
      "Epoch 95/100\n",
      "379/379 [==============================] - 0s 161us/sample - loss: 13.9526 - val_loss: 13.2962\n",
      "Epoch 96/100\n",
      "379/379 [==============================] - 0s 164us/sample - loss: 13.8660 - val_loss: 13.2854\n",
      "Epoch 97/100\n",
      "379/379 [==============================] - 0s 164us/sample - loss: 13.8205 - val_loss: 13.2449\n",
      "Epoch 98/100\n",
      "379/379 [==============================] - 0s 163us/sample - loss: 13.7663 - val_loss: 13.1557\n",
      "Epoch 99/100\n",
      "379/379 [==============================] - 0s 168us/sample - loss: 13.6714 - val_loss: 13.0801\n",
      "Epoch 100/100\n",
      "379/379 [==============================] - 0s 163us/sample - loss: 13.6549 - val_loss: 13.0668\n",
      "\n",
      "Forming the grid-search model #5 using: optimizer=<tensorflow.python.keras.optimizer_v2.adam.Adam object at 0x7f9132bd52d0>, kernel=<tensorflow.python.ops.init_ops_v2.GlorotNormal object at 0x7f9132ed1050>, epochs=100, batch_size=64\n",
      "Train on 379 samples, validate on 127 samples\n",
      "Epoch 1/100\n",
      "379/379 [==============================] - 1s 3ms/sample - loss: 559.5838 - val_loss: 559.6758\n",
      "Epoch 2/100\n",
      "379/379 [==============================] - 0s 103us/sample - loss: 524.4660 - val_loss: 516.9431\n",
      "Epoch 3/100\n",
      "379/379 [==============================] - 0s 114us/sample - loss: 481.9391 - val_loss: 471.1659\n",
      "Epoch 4/100\n",
      "379/379 [==============================] - 0s 109us/sample - loss: 437.1101 - val_loss: 421.8376\n",
      "Epoch 5/100\n",
      "379/379 [==============================] - 0s 109us/sample - loss: 390.7151 - val_loss: 370.0259\n",
      "Epoch 6/100\n",
      "379/379 [==============================] - 0s 109us/sample - loss: 342.1151 - val_loss: 319.1875\n",
      "Epoch 7/100\n",
      "379/379 [==============================] - 0s 108us/sample - loss: 294.5983 - val_loss: 270.7928\n",
      "Epoch 8/100\n",
      "379/379 [==============================] - 0s 109us/sample - loss: 251.0753 - val_loss: 226.8671\n",
      "Epoch 9/100\n",
      "379/379 [==============================] - 0s 106us/sample - loss: 211.7726 - val_loss: 188.9902\n",
      "Epoch 10/100\n",
      "379/379 [==============================] - 0s 109us/sample - loss: 178.5542 - val_loss: 157.5031\n",
      "Epoch 11/100\n",
      "379/379 [==============================] - 0s 109us/sample - loss: 151.2160 - val_loss: 132.6380\n",
      "Epoch 12/100\n",
      "379/379 [==============================] - 0s 112us/sample - loss: 128.9591 - val_loss: 113.7215\n",
      "Epoch 13/100\n",
      "379/379 [==============================] - 0s 109us/sample - loss: 111.4087 - val_loss: 99.6875\n",
      "Epoch 14/100\n",
      "379/379 [==============================] - 0s 109us/sample - loss: 98.6662 - val_loss: 88.8083\n",
      "Epoch 15/100\n",
      "379/379 [==============================] - 0s 109us/sample - loss: 88.2815 - val_loss: 80.2867\n",
      "Epoch 16/100\n",
      "379/379 [==============================] - 0s 107us/sample - loss: 80.2155 - val_loss: 73.2800\n",
      "Epoch 17/100\n",
      "379/379 [==============================] - 0s 113us/sample - loss: 73.3078 - val_loss: 67.3556\n",
      "Epoch 18/100\n",
      "379/379 [==============================] - 0s 106us/sample - loss: 67.4947 - val_loss: 62.1918\n",
      "Epoch 19/100\n",
      "379/379 [==============================] - 0s 109us/sample - loss: 62.4915 - val_loss: 57.5862\n",
      "Epoch 20/100\n",
      "379/379 [==============================] - 0s 107us/sample - loss: 58.1770 - val_loss: 53.4736\n",
      "Epoch 21/100\n",
      "379/379 [==============================] - 0s 107us/sample - loss: 54.3426 - val_loss: 49.8199\n",
      "Epoch 22/100\n",
      "379/379 [==============================] - 0s 104us/sample - loss: 50.9352 - val_loss: 46.6184\n",
      "Epoch 23/100\n",
      "379/379 [==============================] - 0s 108us/sample - loss: 48.0519 - val_loss: 43.7562\n",
      "Epoch 24/100\n",
      "379/379 [==============================] - 0s 109us/sample - loss: 45.5686 - val_loss: 41.2063\n",
      "Epoch 25/100\n",
      "379/379 [==============================] - 0s 107us/sample - loss: 43.1510 - val_loss: 39.0254\n",
      "Epoch 26/100\n",
      "379/379 [==============================] - 0s 106us/sample - loss: 41.2614 - val_loss: 37.0572\n",
      "Epoch 27/100\n",
      "379/379 [==============================] - 0s 110us/sample - loss: 39.4573 - val_loss: 35.3606\n",
      "Epoch 28/100\n",
      "379/379 [==============================] - 0s 114us/sample - loss: 37.9332 - val_loss: 33.8322\n",
      "Epoch 29/100\n",
      "379/379 [==============================] - 0s 128us/sample - loss: 36.5368 - val_loss: 32.5045\n",
      "Epoch 30/100\n",
      "379/379 [==============================] - 0s 107us/sample - loss: 35.3496 - val_loss: 31.3371\n",
      "Epoch 31/100\n",
      "379/379 [==============================] - 0s 110us/sample - loss: 34.2087 - val_loss: 30.3484\n",
      "Epoch 32/100\n",
      "379/379 [==============================] - 0s 112us/sample - loss: 33.2998 - val_loss: 29.4413\n",
      "Epoch 33/100\n",
      "379/379 [==============================] - 0s 109us/sample - loss: 32.4548 - val_loss: 28.6430\n",
      "Epoch 34/100\n",
      "379/379 [==============================] - 0s 108us/sample - loss: 31.6834 - val_loss: 27.9575\n",
      "Epoch 35/100\n",
      "379/379 [==============================] - 0s 105us/sample - loss: 31.0095 - val_loss: 27.3412\n",
      "Epoch 36/100\n",
      "379/379 [==============================] - 0s 107us/sample - loss: 30.4243 - val_loss: 26.7943\n",
      "Epoch 37/100\n",
      "379/379 [==============================] - 0s 117us/sample - loss: 29.8506 - val_loss: 26.3257\n",
      "Epoch 38/100\n",
      "379/379 [==============================] - 0s 109us/sample - loss: 29.3831 - val_loss: 25.8940\n",
      "Epoch 39/100\n",
      "379/379 [==============================] - 0s 107us/sample - loss: 28.9338 - val_loss: 25.4960\n",
      "Epoch 40/100\n",
      "379/379 [==============================] - 0s 110us/sample - loss: 28.5125 - val_loss: 25.1429\n",
      "Epoch 41/100\n",
      "379/379 [==============================] - 0s 107us/sample - loss: 28.1319 - val_loss: 24.8214\n",
      "Epoch 42/100\n",
      "379/379 [==============================] - 0s 109us/sample - loss: 27.7750 - val_loss: 24.5165\n",
      "Epoch 43/100\n",
      "379/379 [==============================] - 0s 107us/sample - loss: 27.4478 - val_loss: 24.2642\n",
      "Epoch 44/100\n",
      "379/379 [==============================] - 0s 112us/sample - loss: 27.1230 - val_loss: 24.0151\n",
      "Epoch 45/100\n",
      "379/379 [==============================] - 0s 107us/sample - loss: 26.8292 - val_loss: 23.7934\n",
      "Epoch 46/100\n",
      "379/379 [==============================] - 0s 106us/sample - loss: 26.5458 - val_loss: 23.5851\n",
      "Epoch 47/100\n",
      "379/379 [==============================] - 0s 110us/sample - loss: 26.3018 - val_loss: 23.3498\n",
      "Epoch 48/100\n",
      "379/379 [==============================] - 0s 109us/sample - loss: 26.0507 - val_loss: 23.1693\n",
      "Epoch 49/100\n",
      "379/379 [==============================] - 0s 110us/sample - loss: 25.7819 - val_loss: 22.9902\n",
      "Epoch 50/100\n",
      "379/379 [==============================] - 0s 109us/sample - loss: 25.5538 - val_loss: 22.8092\n",
      "Epoch 51/100\n",
      "379/379 [==============================] - 0s 106us/sample - loss: 25.3200 - val_loss: 22.6290\n",
      "Epoch 52/100\n",
      "379/379 [==============================] - 0s 111us/sample - loss: 25.1117 - val_loss: 22.4470\n",
      "Epoch 53/100\n",
      "379/379 [==============================] - 0s 109us/sample - loss: 24.9027 - val_loss: 22.2629\n",
      "Epoch 54/100\n",
      "379/379 [==============================] - 0s 106us/sample - loss: 24.6788 - val_loss: 22.1182\n",
      "Epoch 55/100\n",
      "379/379 [==============================] - 0s 108us/sample - loss: 24.4828 - val_loss: 21.9692\n",
      "Epoch 56/100\n",
      "379/379 [==============================] - 0s 109us/sample - loss: 24.2910 - val_loss: 21.8390\n",
      "Epoch 57/100\n",
      "379/379 [==============================] - 0s 110us/sample - loss: 24.0974 - val_loss: 21.7166\n",
      "Epoch 58/100\n",
      "379/379 [==============================] - 0s 107us/sample - loss: 23.9091 - val_loss: 21.5308\n",
      "Epoch 59/100\n",
      "379/379 [==============================] - 0s 115us/sample - loss: 23.7185 - val_loss: 21.3800\n",
      "Epoch 60/100\n",
      "379/379 [==============================] - 0s 105us/sample - loss: 23.5392 - val_loss: 21.2434\n",
      "Epoch 61/100\n",
      "379/379 [==============================] - 0s 113us/sample - loss: 23.3544 - val_loss: 21.1017\n",
      "Epoch 62/100\n",
      "379/379 [==============================] - 0s 107us/sample - loss: 23.1791 - val_loss: 20.9832\n",
      "Epoch 63/100\n",
      "379/379 [==============================] - 0s 107us/sample - loss: 23.0087 - val_loss: 20.8458\n",
      "Epoch 64/100\n",
      "379/379 [==============================] - 0s 115us/sample - loss: 22.8425 - val_loss: 20.7116\n",
      "Epoch 65/100\n",
      "379/379 [==============================] - 0s 117us/sample - loss: 22.6713 - val_loss: 20.5806\n",
      "Epoch 66/100\n",
      "379/379 [==============================] - 0s 108us/sample - loss: 22.5126 - val_loss: 20.4484\n",
      "Epoch 67/100\n",
      "379/379 [==============================] - 0s 110us/sample - loss: 22.3428 - val_loss: 20.3346\n",
      "Epoch 68/100\n",
      "379/379 [==============================] - 0s 109us/sample - loss: 22.1872 - val_loss: 20.1869\n",
      "Epoch 69/100\n",
      "379/379 [==============================] - 0s 121us/sample - loss: 22.0215 - val_loss: 20.0734\n",
      "Epoch 70/100\n",
      "379/379 [==============================] - 0s 106us/sample - loss: 21.8661 - val_loss: 19.9490\n",
      "Epoch 71/100\n",
      "379/379 [==============================] - 0s 106us/sample - loss: 21.6965 - val_loss: 19.8302\n",
      "Epoch 72/100\n",
      "379/379 [==============================] - 0s 107us/sample - loss: 21.5365 - val_loss: 19.6770\n",
      "Epoch 73/100\n",
      "379/379 [==============================] - 0s 108us/sample - loss: 21.3934 - val_loss: 19.5541\n",
      "Epoch 74/100\n",
      "379/379 [==============================] - 0s 106us/sample - loss: 21.2665 - val_loss: 19.4421\n",
      "Epoch 75/100\n",
      "379/379 [==============================] - 0s 108us/sample - loss: 21.1028 - val_loss: 19.3206\n",
      "Epoch 76/100\n",
      "379/379 [==============================] - 0s 111us/sample - loss: 20.9558 - val_loss: 19.1955\n",
      "Epoch 77/100\n",
      "379/379 [==============================] - 0s 109us/sample - loss: 20.8386 - val_loss: 19.0802\n",
      "Epoch 78/100\n",
      "379/379 [==============================] - 0s 109us/sample - loss: 20.6920 - val_loss: 18.9439\n",
      "Epoch 79/100\n",
      "379/379 [==============================] - 0s 108us/sample - loss: 20.5532 - val_loss: 18.8218\n",
      "Epoch 80/100\n",
      "379/379 [==============================] - 0s 104us/sample - loss: 20.4295 - val_loss: 18.7090\n",
      "Epoch 81/100\n",
      "379/379 [==============================] - 0s 108us/sample - loss: 20.2969 - val_loss: 18.5924\n",
      "Epoch 82/100\n",
      "379/379 [==============================] - 0s 106us/sample - loss: 20.1776 - val_loss: 18.4961\n",
      "Epoch 83/100\n",
      "379/379 [==============================] - 0s 108us/sample - loss: 20.0555 - val_loss: 18.3880\n",
      "Epoch 84/100\n",
      "379/379 [==============================] - 0s 110us/sample - loss: 19.9272 - val_loss: 18.2763\n",
      "Epoch 85/100\n",
      "379/379 [==============================] - 0s 126us/sample - loss: 19.8018 - val_loss: 18.1821\n",
      "Epoch 86/100\n",
      "379/379 [==============================] - 0s 107us/sample - loss: 19.6748 - val_loss: 18.0626\n",
      "Epoch 87/100\n",
      "379/379 [==============================] - 0s 110us/sample - loss: 19.5565 - val_loss: 17.9310\n",
      "Epoch 88/100\n",
      "379/379 [==============================] - 0s 108us/sample - loss: 19.4400 - val_loss: 17.8307\n",
      "Epoch 89/100\n",
      "379/379 [==============================] - 0s 107us/sample - loss: 19.3169 - val_loss: 17.7239\n",
      "Epoch 90/100\n",
      "379/379 [==============================] - 0s 118us/sample - loss: 19.1969 - val_loss: 17.6232\n",
      "Epoch 91/100\n",
      "379/379 [==============================] - 0s 109us/sample - loss: 19.0817 - val_loss: 17.5162\n",
      "Epoch 92/100\n",
      "379/379 [==============================] - 0s 108us/sample - loss: 18.9677 - val_loss: 17.4153\n",
      "Epoch 93/100\n",
      "379/379 [==============================] - 0s 108us/sample - loss: 18.8513 - val_loss: 17.3367\n",
      "Epoch 94/100\n",
      "379/379 [==============================] - 0s 109us/sample - loss: 18.7310 - val_loss: 17.2373\n",
      "Epoch 95/100\n",
      "379/379 [==============================] - 0s 105us/sample - loss: 18.6202 - val_loss: 17.1306\n",
      "Epoch 96/100\n",
      "379/379 [==============================] - 0s 106us/sample - loss: 18.5064 - val_loss: 17.0556\n",
      "Epoch 97/100\n",
      "379/379 [==============================] - 0s 109us/sample - loss: 18.3971 - val_loss: 16.9822\n",
      "Epoch 98/100\n",
      "379/379 [==============================] - 0s 109us/sample - loss: 18.3107 - val_loss: 16.8922\n",
      "Epoch 99/100\n",
      "379/379 [==============================] - 0s 111us/sample - loss: 18.1849 - val_loss: 16.7919\n",
      "Epoch 100/100\n",
      "379/379 [==============================] - 0s 106us/sample - loss: 18.1109 - val_loss: 16.7166\n",
      "\n",
      "Forming the grid-search model #6 using: optimizer=<tensorflow.python.keras.optimizer_v2.adam.Adam object at 0x7f9132bd5690>, kernel=<tensorflow.python.ops.init_ops_v2.RandomNormal object at 0x7f9132bbbed0>, epochs=100, batch_size=32\n",
      "Train on 379 samples, validate on 127 samples\n",
      "Epoch 1/100\n",
      "379/379 [==============================] - 1s 2ms/sample - loss: 588.6573 - val_loss: 601.2248\n",
      "Epoch 2/100\n",
      "379/379 [==============================] - 0s 166us/sample - loss: 587.2218 - val_loss: 599.6575\n",
      "Epoch 3/100\n",
      "379/379 [==============================] - 0s 163us/sample - loss: 585.6890 - val_loss: 598.0013\n",
      "Epoch 4/100\n",
      "379/379 [==============================] - 0s 162us/sample - loss: 584.0329 - val_loss: 596.1258\n",
      "Epoch 5/100\n",
      "379/379 [==============================] - 0s 167us/sample - loss: 582.1669 - val_loss: 593.9425\n",
      "Epoch 6/100\n",
      "379/379 [==============================] - 0s 165us/sample - loss: 579.9682 - val_loss: 591.4517\n",
      "Epoch 7/100\n",
      "379/379 [==============================] - 0s 164us/sample - loss: 577.4377 - val_loss: 588.5162\n",
      "Epoch 8/100\n",
      "379/379 [==============================] - 0s 162us/sample - loss: 574.5449 - val_loss: 585.1576\n",
      "Epoch 9/100\n",
      "379/379 [==============================] - 0s 164us/sample - loss: 571.2075 - val_loss: 581.3824\n",
      "Epoch 10/100\n",
      "379/379 [==============================] - 0s 165us/sample - loss: 567.5148 - val_loss: 577.0931\n",
      "Epoch 11/100\n",
      "379/379 [==============================] - 0s 167us/sample - loss: 563.2907 - val_loss: 572.4078\n",
      "Epoch 12/100\n",
      "379/379 [==============================] - 0s 167us/sample - loss: 558.7241 - val_loss: 567.2072\n",
      "Epoch 13/100\n",
      "379/379 [==============================] - 0s 166us/sample - loss: 553.6464 - val_loss: 561.5709\n",
      "Epoch 14/100\n",
      "379/379 [==============================] - 0s 165us/sample - loss: 548.2279 - val_loss: 555.4138\n",
      "Epoch 15/100\n",
      "379/379 [==============================] - 0s 163us/sample - loss: 542.2187 - val_loss: 548.7883\n",
      "Epoch 16/100\n",
      "379/379 [==============================] - 0s 169us/sample - loss: 535.8591 - val_loss: 541.5653\n",
      "Epoch 17/100\n",
      "379/379 [==============================] - 0s 164us/sample - loss: 528.8913 - val_loss: 533.9060\n",
      "Epoch 18/100\n",
      "379/379 [==============================] - 0s 160us/sample - loss: 521.4687 - val_loss: 525.7418\n",
      "Epoch 19/100\n",
      "379/379 [==============================] - 0s 167us/sample - loss: 513.6869 - val_loss: 517.0450\n",
      "Epoch 20/100\n",
      "379/379 [==============================] - 0s 169us/sample - loss: 505.4589 - val_loss: 507.7846\n",
      "Epoch 21/100\n",
      "379/379 [==============================] - 0s 165us/sample - loss: 496.6313 - val_loss: 498.1786\n",
      "Epoch 22/100\n",
      "379/379 [==============================] - 0s 164us/sample - loss: 487.5744 - val_loss: 487.9263\n",
      "Epoch 23/100\n",
      "379/379 [==============================] - 0s 165us/sample - loss: 477.3977 - val_loss: 477.6177\n",
      "Epoch 24/100\n",
      "379/379 [==============================] - 0s 164us/sample - loss: 467.4471 - val_loss: 466.2466\n",
      "Epoch 25/100\n",
      "379/379 [==============================] - 0s 163us/sample - loss: 456.6865 - val_loss: 454.2811\n",
      "Epoch 26/100\n",
      "379/379 [==============================] - 0s 164us/sample - loss: 445.4432 - val_loss: 441.8162\n",
      "Epoch 27/100\n",
      "379/379 [==============================] - 0s 166us/sample - loss: 433.7652 - val_loss: 429.0459\n",
      "Epoch 28/100\n",
      "379/379 [==============================] - 0s 161us/sample - loss: 421.5967 - val_loss: 416.1636\n",
      "Epoch 29/100\n",
      "379/379 [==============================] - 0s 162us/sample - loss: 409.6120 - val_loss: 402.6938\n",
      "Epoch 30/100\n",
      "379/379 [==============================] - 0s 162us/sample - loss: 396.9924 - val_loss: 389.3199\n",
      "Epoch 31/100\n",
      "379/379 [==============================] - 0s 163us/sample - loss: 384.4225 - val_loss: 375.7399\n",
      "Epoch 32/100\n",
      "379/379 [==============================] - 0s 173us/sample - loss: 371.9156 - val_loss: 361.9998\n",
      "Epoch 33/100\n",
      "379/379 [==============================] - 0s 168us/sample - loss: 359.0103 - val_loss: 348.5316\n",
      "Epoch 34/100\n",
      "379/379 [==============================] - 0s 167us/sample - loss: 346.5651 - val_loss: 334.8324\n",
      "Epoch 35/100\n",
      "379/379 [==============================] - 0s 168us/sample - loss: 333.9551 - val_loss: 321.3402\n",
      "Epoch 36/100\n",
      "379/379 [==============================] - 0s 166us/sample - loss: 321.3170 - val_loss: 308.0889\n",
      "Epoch 37/100\n",
      "379/379 [==============================] - 0s 169us/sample - loss: 309.0817 - val_loss: 294.8307\n",
      "Epoch 38/100\n",
      "379/379 [==============================] - 0s 165us/sample - loss: 296.8605 - val_loss: 281.8613\n",
      "Epoch 39/100\n",
      "379/379 [==============================] - 0s 164us/sample - loss: 284.8698 - val_loss: 269.1231\n",
      "Epoch 40/100\n",
      "379/379 [==============================] - 0s 162us/sample - loss: 272.9711 - val_loss: 256.9674\n",
      "Epoch 41/100\n",
      "379/379 [==============================] - 0s 164us/sample - loss: 261.7979 - val_loss: 244.8689\n",
      "Epoch 42/100\n",
      "379/379 [==============================] - 0s 168us/sample - loss: 250.4932 - val_loss: 233.4757\n",
      "Epoch 43/100\n",
      "379/379 [==============================] - 0s 169us/sample - loss: 239.7080 - val_loss: 222.5496\n",
      "Epoch 44/100\n",
      "379/379 [==============================] - 0s 165us/sample - loss: 229.5730 - val_loss: 211.7890\n",
      "Epoch 45/100\n",
      "379/379 [==============================] - 0s 170us/sample - loss: 219.5351 - val_loss: 201.5626\n",
      "Epoch 46/100\n",
      "379/379 [==============================] - 0s 165us/sample - loss: 209.8527 - val_loss: 191.9356\n",
      "Epoch 47/100\n",
      "379/379 [==============================] - 0s 162us/sample - loss: 200.9993 - val_loss: 182.3850\n",
      "Epoch 48/100\n",
      "379/379 [==============================] - 0s 164us/sample - loss: 191.8793 - val_loss: 173.8551\n",
      "Epoch 49/100\n",
      "379/379 [==============================] - 0s 166us/sample - loss: 183.7396 - val_loss: 165.5729\n",
      "Epoch 50/100\n",
      "379/379 [==============================] - 0s 170us/sample - loss: 175.8059 - val_loss: 157.8469\n",
      "Epoch 51/100\n",
      "379/379 [==============================] - 0s 164us/sample - loss: 168.3858 - val_loss: 150.4590\n",
      "Epoch 52/100\n",
      "379/379 [==============================] - 0s 165us/sample - loss: 161.3295 - val_loss: 143.4270\n",
      "Epoch 53/100\n",
      "379/379 [==============================] - 0s 165us/sample - loss: 154.5926 - val_loss: 136.8710\n",
      "Epoch 54/100\n",
      "379/379 [==============================] - 0s 166us/sample - loss: 147.9843 - val_loss: 130.9227\n",
      "Epoch 55/100\n",
      "379/379 [==============================] - 0s 168us/sample - loss: 142.1659 - val_loss: 125.0262\n",
      "Epoch 56/100\n",
      "379/379 [==============================] - 0s 174us/sample - loss: 136.2916 - val_loss: 119.6902\n",
      "Epoch 57/100\n",
      "379/379 [==============================] - 0s 170us/sample - loss: 130.7547 - val_loss: 114.7326\n",
      "Epoch 58/100\n",
      "379/379 [==============================] - 0s 170us/sample - loss: 125.8631 - val_loss: 109.8039\n",
      "Epoch 59/100\n",
      "379/379 [==============================] - 0s 166us/sample - loss: 120.9316 - val_loss: 105.2809\n",
      "Epoch 60/100\n",
      "379/379 [==============================] - 0s 161us/sample - loss: 116.2798 - val_loss: 101.0914\n",
      "Epoch 61/100\n",
      "379/379 [==============================] - 0s 187us/sample - loss: 111.8561 - val_loss: 97.1973\n",
      "Epoch 62/100\n",
      "379/379 [==============================] - 0s 169us/sample - loss: 107.7706 - val_loss: 93.4908\n",
      "Epoch 63/100\n",
      "379/379 [==============================] - 0s 164us/sample - loss: 103.8437 - val_loss: 90.0066\n",
      "Epoch 64/100\n",
      "379/379 [==============================] - 0s 161us/sample - loss: 100.2018 - val_loss: 86.6393\n",
      "Epoch 65/100\n",
      "379/379 [==============================] - 0s 164us/sample - loss: 96.6587 - val_loss: 83.4513\n",
      "Epoch 66/100\n",
      "379/379 [==============================] - 0s 168us/sample - loss: 93.2585 - val_loss: 80.4617\n",
      "Epoch 67/100\n",
      "379/379 [==============================] - 0s 166us/sample - loss: 90.0766 - val_loss: 77.6204\n",
      "Epoch 68/100\n",
      "379/379 [==============================] - 0s 168us/sample - loss: 87.0011 - val_loss: 74.9749\n",
      "Epoch 69/100\n",
      "379/379 [==============================] - 0s 160us/sample - loss: 84.1171 - val_loss: 72.4442\n",
      "Epoch 70/100\n",
      "379/379 [==============================] - 0s 165us/sample - loss: 81.3871 - val_loss: 70.0374\n",
      "Epoch 71/100\n",
      "379/379 [==============================] - 0s 168us/sample - loss: 78.7341 - val_loss: 67.7872\n",
      "Epoch 72/100\n",
      "379/379 [==============================] - 0s 162us/sample - loss: 76.3277 - val_loss: 65.5746\n",
      "Epoch 73/100\n",
      "379/379 [==============================] - 0s 152us/sample - loss: 73.8694 - val_loss: 63.5557\n",
      "Epoch 74/100\n",
      "379/379 [==============================] - 0s 148us/sample - loss: 71.6839 - val_loss: 61.5743\n",
      "Epoch 75/100\n",
      "379/379 [==============================] - 0s 151us/sample - loss: 69.5317 - val_loss: 59.6938\n",
      "Epoch 76/100\n",
      "379/379 [==============================] - 0s 180us/sample - loss: 67.4453 - val_loss: 57.9726\n",
      "Epoch 77/100\n",
      "379/379 [==============================] - 0s 165us/sample - loss: 65.6187 - val_loss: 56.2447\n",
      "Epoch 78/100\n",
      "379/379 [==============================] - 0s 168us/sample - loss: 63.7174 - val_loss: 54.6498\n",
      "Epoch 79/100\n",
      "379/379 [==============================] - 0s 164us/sample - loss: 61.9643 - val_loss: 53.1321\n",
      "Epoch 80/100\n",
      "379/379 [==============================] - 0s 157us/sample - loss: 60.2874 - val_loss: 51.6923\n",
      "Epoch 81/100\n",
      "379/379 [==============================] - 0s 157us/sample - loss: 58.6972 - val_loss: 50.3199\n",
      "Epoch 82/100\n",
      "379/379 [==============================] - 0s 154us/sample - loss: 57.2080 - val_loss: 48.9869\n",
      "Epoch 83/100\n",
      "379/379 [==============================] - 0s 155us/sample - loss: 55.7556 - val_loss: 47.7366\n",
      "Epoch 84/100\n",
      "379/379 [==============================] - 0s 158us/sample - loss: 54.3802 - val_loss: 46.5325\n",
      "Epoch 85/100\n",
      "379/379 [==============================] - 0s 159us/sample - loss: 53.0854 - val_loss: 45.3674\n",
      "Epoch 86/100\n",
      "379/379 [==============================] - 0s 154us/sample - loss: 51.8073 - val_loss: 44.3072\n",
      "Epoch 87/100\n",
      "379/379 [==============================] - 0s 153us/sample - loss: 50.6854 - val_loss: 43.2428\n",
      "Epoch 88/100\n",
      "379/379 [==============================] - 0s 153us/sample - loss: 49.5007 - val_loss: 42.2919\n",
      "Epoch 89/100\n",
      "379/379 [==============================] - 0s 153us/sample - loss: 48.4803 - val_loss: 41.3471\n",
      "Epoch 90/100\n",
      "379/379 [==============================] - 0s 172us/sample - loss: 47.4446 - val_loss: 40.4789\n",
      "Epoch 91/100\n",
      "379/379 [==============================] - 0s 152us/sample - loss: 46.5040 - val_loss: 39.6372\n",
      "Epoch 92/100\n",
      "379/379 [==============================] - 0s 157us/sample - loss: 45.5925 - val_loss: 38.8449\n",
      "Epoch 93/100\n",
      "379/379 [==============================] - 0s 160us/sample - loss: 44.7087 - val_loss: 38.1069\n",
      "Epoch 94/100\n",
      "379/379 [==============================] - 0s 161us/sample - loss: 43.9243 - val_loss: 37.3761\n",
      "Epoch 95/100\n",
      "379/379 [==============================] - 0s 162us/sample - loss: 43.1350 - val_loss: 36.7063\n",
      "Epoch 96/100\n",
      "379/379 [==============================] - 0s 164us/sample - loss: 42.3947 - val_loss: 36.0549\n",
      "Epoch 97/100\n",
      "379/379 [==============================] - 0s 163us/sample - loss: 41.7050 - val_loss: 35.4525\n",
      "Epoch 98/100\n",
      "379/379 [==============================] - 0s 167us/sample - loss: 41.0430 - val_loss: 34.8703\n",
      "Epoch 99/100\n",
      "379/379 [==============================] - 0s 164us/sample - loss: 40.4250 - val_loss: 34.3013\n",
      "Epoch 100/100\n",
      "379/379 [==============================] - 0s 166us/sample - loss: 39.8270 - val_loss: 33.7837\n",
      "\n",
      "Forming the grid-search model #7 using: optimizer=<tensorflow.python.keras.optimizer_v2.adam.Adam object at 0x7f9132bd5690>, kernel=<tensorflow.python.ops.init_ops_v2.RandomNormal object at 0x7f9132bbbed0>, epochs=100, batch_size=64\n",
      "Train on 379 samples, validate on 127 samples\n",
      "Epoch 1/100\n",
      "379/379 [==============================] - 1s 2ms/sample - loss: 588.2050 - val_loss: 599.4551\n",
      "Epoch 2/100\n",
      "379/379 [==============================] - 0s 108us/sample - loss: 584.5172 - val_loss: 594.7513\n",
      "Epoch 3/100\n",
      "379/379 [==============================] - 0s 108us/sample - loss: 579.5499 - val_loss: 588.8992\n",
      "Epoch 4/100\n",
      "379/379 [==============================] - 0s 107us/sample - loss: 573.4605 - val_loss: 581.6340\n",
      "Epoch 5/100\n",
      "379/379 [==============================] - 0s 108us/sample - loss: 566.1190 - val_loss: 572.8212\n",
      "Epoch 6/100\n",
      "379/379 [==============================] - 0s 108us/sample - loss: 557.2685 - val_loss: 562.6642\n",
      "Epoch 7/100\n",
      "379/379 [==============================] - 0s 108us/sample - loss: 547.0916 - val_loss: 551.1127\n",
      "Epoch 8/100\n",
      "379/379 [==============================] - 0s 108us/sample - loss: 535.9306 - val_loss: 538.3150\n",
      "Epoch 9/100\n",
      "379/379 [==============================] - 0s 109us/sample - loss: 523.4477 - val_loss: 524.4390\n",
      "Epoch 10/100\n",
      "379/379 [==============================] - 0s 106us/sample - loss: 510.2313 - val_loss: 509.5089\n",
      "Epoch 11/100\n",
      "379/379 [==============================] - 0s 112us/sample - loss: 495.7683 - val_loss: 493.9653\n",
      "Epoch 12/100\n",
      "379/379 [==============================] - 0s 110us/sample - loss: 481.1292 - val_loss: 477.7480\n",
      "Epoch 13/100\n",
      "379/379 [==============================] - 0s 107us/sample - loss: 465.6202 - val_loss: 461.2912\n",
      "Epoch 14/100\n",
      "379/379 [==============================] - 0s 108us/sample - loss: 450.2696 - val_loss: 444.5424\n",
      "Epoch 15/100\n",
      "379/379 [==============================] - 0s 109us/sample - loss: 434.3283 - val_loss: 427.7980\n",
      "Epoch 16/100\n",
      "379/379 [==============================] - 0s 113us/sample - loss: 418.7953 - val_loss: 410.9389\n",
      "Epoch 17/100\n",
      "379/379 [==============================] - 0s 111us/sample - loss: 402.8984 - val_loss: 394.3230\n",
      "Epoch 18/100\n",
      "379/379 [==============================] - 0s 111us/sample - loss: 387.2226 - val_loss: 377.8930\n",
      "Epoch 19/100\n",
      "379/379 [==============================] - 0s 109us/sample - loss: 372.1106 - val_loss: 361.6236\n",
      "Epoch 20/100\n",
      "379/379 [==============================] - 0s 111us/sample - loss: 357.2299 - val_loss: 345.6369\n",
      "Epoch 21/100\n",
      "379/379 [==============================] - 0s 112us/sample - loss: 342.3704 - val_loss: 330.2230\n",
      "Epoch 22/100\n",
      "379/379 [==============================] - 0s 110us/sample - loss: 328.4586 - val_loss: 315.1688\n",
      "Epoch 23/100\n",
      "379/379 [==============================] - 0s 150us/sample - loss: 313.7270 - val_loss: 301.1269\n",
      "Epoch 24/100\n",
      "379/379 [==============================] - 0s 108us/sample - loss: 300.9456 - val_loss: 287.2769\n",
      "Epoch 25/100\n",
      "379/379 [==============================] - 0s 107us/sample - loss: 288.1886 - val_loss: 273.9195\n",
      "Epoch 26/100\n",
      "379/379 [==============================] - 0s 110us/sample - loss: 275.7456 - val_loss: 261.1768\n",
      "Epoch 27/100\n",
      "379/379 [==============================] - 0s 107us/sample - loss: 264.0268 - val_loss: 248.9797\n",
      "Epoch 28/100\n",
      "379/379 [==============================] - 0s 106us/sample - loss: 252.5721 - val_loss: 237.4570\n",
      "Epoch 29/100\n",
      "379/379 [==============================] - 0s 108us/sample - loss: 242.0612 - val_loss: 226.3272\n",
      "Epoch 30/100\n",
      "379/379 [==============================] - 0s 109us/sample - loss: 231.6213 - val_loss: 215.8432\n",
      "Epoch 31/100\n",
      "379/379 [==============================] - 0s 106us/sample - loss: 221.6179 - val_loss: 205.9400\n",
      "Epoch 32/100\n",
      "379/379 [==============================] - 0s 108us/sample - loss: 212.6070 - val_loss: 196.3742\n",
      "Epoch 33/100\n",
      "379/379 [==============================] - 0s 105us/sample - loss: 203.3416 - val_loss: 187.5271\n",
      "Epoch 34/100\n",
      "379/379 [==============================] - 0s 109us/sample - loss: 195.0527 - val_loss: 179.0309\n",
      "Epoch 35/100\n",
      "379/379 [==============================] - 0s 106us/sample - loss: 187.1264 - val_loss: 170.9582\n",
      "Epoch 36/100\n",
      "379/379 [==============================] - 0s 108us/sample - loss: 179.2163 - val_loss: 163.4486\n",
      "Epoch 37/100\n",
      "379/379 [==============================] - 0s 106us/sample - loss: 172.0977 - val_loss: 156.2501\n",
      "Epoch 38/100\n",
      "379/379 [==============================] - 0s 108us/sample - loss: 165.2550 - val_loss: 149.4622\n",
      "Epoch 39/100\n",
      "379/379 [==============================] - 0s 110us/sample - loss: 158.6441 - val_loss: 143.0778\n",
      "Epoch 40/100\n",
      "379/379 [==============================] - 0s 110us/sample - loss: 152.3560 - val_loss: 137.1031\n",
      "Epoch 41/100\n",
      "379/379 [==============================] - 0s 112us/sample - loss: 146.5759 - val_loss: 131.4055\n",
      "Epoch 42/100\n",
      "379/379 [==============================] - 0s 106us/sample - loss: 140.9900 - val_loss: 126.0156\n",
      "Epoch 43/100\n",
      "379/379 [==============================] - 0s 105us/sample - loss: 135.4434 - val_loss: 121.0613\n",
      "Epoch 44/100\n",
      "379/379 [==============================] - 0s 109us/sample - loss: 130.6907 - val_loss: 116.2379\n",
      "Epoch 45/100\n",
      "379/379 [==============================] - 0s 108us/sample - loss: 125.8884 - val_loss: 111.7074\n",
      "Epoch 46/100\n",
      "379/379 [==============================] - 0s 110us/sample - loss: 121.2973 - val_loss: 107.4633\n",
      "Epoch 47/100\n",
      "379/379 [==============================] - 0s 110us/sample - loss: 117.1799 - val_loss: 103.3585\n",
      "Epoch 48/100\n",
      "379/379 [==============================] - 0s 109us/sample - loss: 112.8467 - val_loss: 99.6192\n",
      "Epoch 49/100\n",
      "379/379 [==============================] - 0s 110us/sample - loss: 109.0707 - val_loss: 96.0262\n",
      "Epoch 50/100\n",
      "379/379 [==============================] - 0s 113us/sample - loss: 105.3539 - val_loss: 92.6401\n",
      "Epoch 51/100\n",
      "379/379 [==============================] - 0s 112us/sample - loss: 101.9086 - val_loss: 89.3930\n",
      "Epoch 52/100\n",
      "379/379 [==============================] - 0s 108us/sample - loss: 98.6065 - val_loss: 86.2977\n",
      "Epoch 53/100\n",
      "379/379 [==============================] - 0s 112us/sample - loss: 95.4636 - val_loss: 83.3404\n",
      "Epoch 54/100\n",
      "379/379 [==============================] - 0s 125us/sample - loss: 92.2202 - val_loss: 80.6303\n",
      "Epoch 55/100\n",
      "379/379 [==============================] - 0s 127us/sample - loss: 89.4757 - val_loss: 77.9682\n",
      "Epoch 56/100\n",
      "379/379 [==============================] - 0s 109us/sample - loss: 86.6765 - val_loss: 75.4618\n",
      "Epoch 57/100\n",
      "379/379 [==============================] - 0s 107us/sample - loss: 83.8470 - val_loss: 73.1484\n",
      "Epoch 58/100\n",
      "379/379 [==============================] - 0s 110us/sample - loss: 81.5193 - val_loss: 70.8402\n",
      "Epoch 59/100\n",
      "379/379 [==============================] - 0s 107us/sample - loss: 79.0537 - val_loss: 68.6831\n",
      "Epoch 60/100\n",
      "379/379 [==============================] - 0s 129us/sample - loss: 76.7854 - val_loss: 66.6171\n",
      "Epoch 61/100\n",
      "379/379 [==============================] - 0s 118us/sample - loss: 74.5002 - val_loss: 64.6904\n",
      "Epoch 62/100\n",
      "379/379 [==============================] - 0s 111us/sample - loss: 72.4535 - val_loss: 62.8333\n",
      "Epoch 63/100\n",
      "379/379 [==============================] - 0s 113us/sample - loss: 70.4266 - val_loss: 61.0850\n",
      "Epoch 64/100\n",
      "379/379 [==============================] - 0s 111us/sample - loss: 68.5786 - val_loss: 59.3940\n",
      "Epoch 65/100\n",
      "379/379 [==============================] - 0s 112us/sample - loss: 66.7838 - val_loss: 57.7762\n",
      "Epoch 66/100\n",
      "379/379 [==============================] - 0s 109us/sample - loss: 65.0520 - val_loss: 56.2312\n",
      "Epoch 67/100\n",
      "379/379 [==============================] - 0s 113us/sample - loss: 63.3990 - val_loss: 54.7618\n",
      "Epoch 68/100\n",
      "379/379 [==============================] - 0s 112us/sample - loss: 61.7983 - val_loss: 53.3869\n",
      "Epoch 69/100\n",
      "379/379 [==============================] - 0s 109us/sample - loss: 60.3419 - val_loss: 52.0545\n",
      "Epoch 70/100\n",
      "379/379 [==============================] - 0s 107us/sample - loss: 58.9022 - val_loss: 50.8048\n",
      "Epoch 71/100\n",
      "379/379 [==============================] - 0s 106us/sample - loss: 57.5456 - val_loss: 49.6118\n",
      "Epoch 72/100\n",
      "379/379 [==============================] - 0s 106us/sample - loss: 56.2949 - val_loss: 48.4521\n",
      "Epoch 73/100\n",
      "379/379 [==============================] - 0s 106us/sample - loss: 55.0004 - val_loss: 47.3696\n",
      "Epoch 74/100\n",
      "379/379 [==============================] - 0s 106us/sample - loss: 53.8511 - val_loss: 46.3319\n",
      "Epoch 75/100\n",
      "379/379 [==============================] - 0s 106us/sample - loss: 52.7131 - val_loss: 45.3358\n",
      "Epoch 76/100\n",
      "379/379 [==============================] - 0s 107us/sample - loss: 51.5989 - val_loss: 44.3982\n",
      "Epoch 77/100\n",
      "379/379 [==============================] - 0s 110us/sample - loss: 50.6082 - val_loss: 43.4825\n",
      "Epoch 78/100\n",
      "379/379 [==============================] - 0s 108us/sample - loss: 49.6150 - val_loss: 42.6040\n",
      "Epoch 79/100\n",
      "379/379 [==============================] - 0s 105us/sample - loss: 48.6322 - val_loss: 41.7812\n",
      "Epoch 80/100\n",
      "379/379 [==============================] - 0s 105us/sample - loss: 47.7296 - val_loss: 40.9922\n",
      "Epoch 81/100\n",
      "379/379 [==============================] - 0s 98us/sample - loss: 46.8644 - val_loss: 40.2377\n",
      "Epoch 82/100\n",
      "379/379 [==============================] - 0s 103us/sample - loss: 46.0557 - val_loss: 39.5133\n",
      "Epoch 83/100\n",
      "379/379 [==============================] - 0s 104us/sample - loss: 45.2439 - val_loss: 38.8343\n",
      "Epoch 84/100\n",
      "379/379 [==============================] - 0s 99us/sample - loss: 44.5049 - val_loss: 38.1771\n",
      "Epoch 85/100\n",
      "379/379 [==============================] - 0s 109us/sample - loss: 43.7871 - val_loss: 37.5409\n",
      "Epoch 86/100\n",
      "379/379 [==============================] - 0s 110us/sample - loss: 43.0799 - val_loss: 36.9444\n",
      "Epoch 87/100\n",
      "379/379 [==============================] - 0s 113us/sample - loss: 42.4592 - val_loss: 36.3547\n",
      "Epoch 88/100\n",
      "379/379 [==============================] - 0s 106us/sample - loss: 41.7814 - val_loss: 35.8076\n",
      "Epoch 89/100\n",
      "379/379 [==============================] - 0s 113us/sample - loss: 41.2006 - val_loss: 35.2732\n",
      "Epoch 90/100\n",
      "379/379 [==============================] - 0s 106us/sample - loss: 40.5986 - val_loss: 34.7714\n",
      "Epoch 91/100\n",
      "379/379 [==============================] - 0s 111us/sample - loss: 40.0625 - val_loss: 34.2794\n",
      "Epoch 92/100\n",
      "379/379 [==============================] - 0s 111us/sample - loss: 39.5292 - val_loss: 33.8098\n",
      "Epoch 93/100\n",
      "379/379 [==============================] - 0s 109us/sample - loss: 38.9860 - val_loss: 33.3701\n",
      "Epoch 94/100\n",
      "379/379 [==============================] - 0s 117us/sample - loss: 38.5228 - val_loss: 32.9367\n",
      "Epoch 95/100\n",
      "379/379 [==============================] - 0s 113us/sample - loss: 38.0497 - val_loss: 32.5240\n",
      "Epoch 96/100\n",
      "379/379 [==============================] - 0s 107us/sample - loss: 37.5917 - val_loss: 32.1290\n",
      "Epoch 97/100\n",
      "379/379 [==============================] - 0s 110us/sample - loss: 37.1576 - val_loss: 31.7532\n",
      "Epoch 98/100\n",
      "379/379 [==============================] - 0s 107us/sample - loss: 36.7416 - val_loss: 31.3971\n",
      "Epoch 99/100\n",
      "379/379 [==============================] - 0s 106us/sample - loss: 36.3434 - val_loss: 31.0464\n",
      "Epoch 100/100\n",
      "379/379 [==============================] - 0s 114us/sample - loss: 35.9696 - val_loss: 30.7114\n",
      "\n",
      "Forming the grid-search model #8 using: optimizer=<tensorflow.python.keras.optimizer_v2.adam.Adam object at 0x7f9132bd5690>, kernel=<tensorflow.python.ops.init_ops_v2.Orthogonal object at 0x7f9132bbb090>, epochs=100, batch_size=32\n",
      "Train on 379 samples, validate on 127 samples\n",
      "Epoch 1/100\n",
      "379/379 [==============================] - 1s 3ms/sample - loss: 577.6073 - val_loss: 572.3081\n",
      "Epoch 2/100\n",
      "379/379 [==============================] - 0s 165us/sample - loss: 550.0159 - val_loss: 540.6539\n",
      "Epoch 3/100\n",
      "379/379 [==============================] - 0s 166us/sample - loss: 520.1548 - val_loss: 508.5447\n",
      "Epoch 4/100\n",
      "379/379 [==============================] - 0s 162us/sample - loss: 490.6952 - val_loss: 475.6160\n",
      "Epoch 5/100\n",
      "379/379 [==============================] - 0s 168us/sample - loss: 461.2190 - val_loss: 442.2414\n",
      "Epoch 6/100\n",
      "379/379 [==============================] - 0s 178us/sample - loss: 431.1404 - val_loss: 410.2262\n",
      "Epoch 7/100\n",
      "379/379 [==============================] - 0s 171us/sample - loss: 402.1207 - val_loss: 378.7174\n",
      "Epoch 8/100\n",
      "379/379 [==============================] - 0s 166us/sample - loss: 374.5927 - val_loss: 348.6527\n",
      "Epoch 9/100\n",
      "379/379 [==============================] - 0s 177us/sample - loss: 348.2841 - val_loss: 320.7587\n",
      "Epoch 10/100\n",
      "379/379 [==============================] - 0s 160us/sample - loss: 324.1538 - val_loss: 294.4688\n",
      "Epoch 11/100\n",
      "379/379 [==============================] - 0s 168us/sample - loss: 301.1889 - val_loss: 270.8584\n",
      "Epoch 12/100\n",
      "379/379 [==============================] - 0s 162us/sample - loss: 280.2075 - val_loss: 249.3217\n",
      "Epoch 13/100\n",
      "379/379 [==============================] - 0s 165us/sample - loss: 260.7841 - val_loss: 230.1119\n",
      "Epoch 14/100\n",
      "379/379 [==============================] - 0s 165us/sample - loss: 243.5596 - val_loss: 212.5576\n",
      "Epoch 15/100\n",
      "379/379 [==============================] - 0s 164us/sample - loss: 227.3281 - val_loss: 196.8521\n",
      "Epoch 16/100\n",
      "379/379 [==============================] - 0s 164us/sample - loss: 212.8228 - val_loss: 182.5006\n",
      "Epoch 17/100\n",
      "379/379 [==============================] - 0s 164us/sample - loss: 199.0220 - val_loss: 169.9848\n",
      "Epoch 18/100\n",
      "379/379 [==============================] - 0s 166us/sample - loss: 186.8061 - val_loss: 158.3795\n",
      "Epoch 19/100\n",
      "379/379 [==============================] - 0s 167us/sample - loss: 175.4204 - val_loss: 147.8646\n",
      "Epoch 20/100\n",
      "379/379 [==============================] - 0s 168us/sample - loss: 165.0676 - val_loss: 138.1411\n",
      "Epoch 21/100\n",
      "379/379 [==============================] - 0s 161us/sample - loss: 155.0199 - val_loss: 129.6196\n",
      "Epoch 22/100\n",
      "379/379 [==============================] - 0s 164us/sample - loss: 146.2688 - val_loss: 121.5346\n",
      "Epoch 23/100\n",
      "379/379 [==============================] - 0s 166us/sample - loss: 137.5223 - val_loss: 114.3263\n",
      "Epoch 24/100\n",
      "379/379 [==============================] - 0s 165us/sample - loss: 129.9626 - val_loss: 107.3829\n",
      "Epoch 25/100\n",
      "379/379 [==============================] - 0s 164us/sample - loss: 122.3167 - val_loss: 101.2217\n",
      "Epoch 26/100\n",
      "379/379 [==============================] - 0s 168us/sample - loss: 115.4044 - val_loss: 95.5074\n",
      "Epoch 27/100\n",
      "379/379 [==============================] - 0s 169us/sample - loss: 109.0527 - val_loss: 90.0578\n",
      "Epoch 28/100\n",
      "379/379 [==============================] - 0s 176us/sample - loss: 103.1060 - val_loss: 84.9033\n",
      "Epoch 29/100\n",
      "379/379 [==============================] - 0s 163us/sample - loss: 97.4717 - val_loss: 80.1239\n",
      "Epoch 30/100\n",
      "379/379 [==============================] - 0s 165us/sample - loss: 92.1212 - val_loss: 75.7265\n",
      "Epoch 31/100\n",
      "379/379 [==============================] - 0s 166us/sample - loss: 87.0191 - val_loss: 71.7898\n",
      "Epoch 32/100\n",
      "379/379 [==============================] - 0s 169us/sample - loss: 82.6417 - val_loss: 67.8920\n",
      "Epoch 33/100\n",
      "379/379 [==============================] - 0s 163us/sample - loss: 78.2348 - val_loss: 64.4064\n",
      "Epoch 34/100\n",
      "379/379 [==============================] - 0s 165us/sample - loss: 74.2591 - val_loss: 61.1420\n",
      "Epoch 35/100\n",
      "379/379 [==============================] - 0s 164us/sample - loss: 70.6708 - val_loss: 58.0129\n",
      "Epoch 36/100\n",
      "379/379 [==============================] - 0s 163us/sample - loss: 67.1415 - val_loss: 55.2009\n",
      "Epoch 37/100\n",
      "379/379 [==============================] - 0s 163us/sample - loss: 63.8707 - val_loss: 52.6680\n",
      "Epoch 38/100\n",
      "379/379 [==============================] - 0s 162us/sample - loss: 61.0752 - val_loss: 50.1567\n",
      "Epoch 39/100\n",
      "379/379 [==============================] - 0s 162us/sample - loss: 58.2994 - val_loss: 47.8719\n",
      "Epoch 40/100\n",
      "379/379 [==============================] - 0s 163us/sample - loss: 55.7962 - val_loss: 45.8036\n",
      "Epoch 41/100\n",
      "379/379 [==============================] - 0s 171us/sample - loss: 53.5026 - val_loss: 43.8568\n",
      "Epoch 42/100\n",
      "379/379 [==============================] - 0s 168us/sample - loss: 51.3621 - val_loss: 42.0899\n",
      "Epoch 43/100\n",
      "379/379 [==============================] - 0s 163us/sample - loss: 49.3630 - val_loss: 40.5265\n",
      "Epoch 44/100\n",
      "379/379 [==============================] - 0s 160us/sample - loss: 47.6187 - val_loss: 39.0153\n",
      "Epoch 45/100\n",
      "379/379 [==============================] - 0s 166us/sample - loss: 45.9392 - val_loss: 37.6471\n",
      "Epoch 46/100\n",
      "379/379 [==============================] - 0s 169us/sample - loss: 44.4368 - val_loss: 36.3760\n",
      "Epoch 47/100\n",
      "379/379 [==============================] - 0s 164us/sample - loss: 43.0559 - val_loss: 35.2185\n",
      "Epoch 48/100\n",
      "379/379 [==============================] - 0s 163us/sample - loss: 41.7397 - val_loss: 34.1803\n",
      "Epoch 49/100\n",
      "379/379 [==============================] - 0s 169us/sample - loss: 40.5731 - val_loss: 33.2228\n",
      "Epoch 50/100\n",
      "379/379 [==============================] - 0s 165us/sample - loss: 39.4910 - val_loss: 32.3483\n",
      "Epoch 51/100\n",
      "379/379 [==============================] - 0s 165us/sample - loss: 38.4936 - val_loss: 31.5389\n",
      "Epoch 52/100\n",
      "379/379 [==============================] - 0s 163us/sample - loss: 37.6224 - val_loss: 30.7690\n",
      "Epoch 53/100\n",
      "379/379 [==============================] - 0s 160us/sample - loss: 36.7732 - val_loss: 30.0809\n",
      "Epoch 54/100\n",
      "379/379 [==============================] - 0s 166us/sample - loss: 35.9680 - val_loss: 29.5087\n",
      "Epoch 55/100\n",
      "379/379 [==============================] - 0s 168us/sample - loss: 35.2781 - val_loss: 28.9426\n",
      "Epoch 56/100\n",
      "379/379 [==============================] - 0s 166us/sample - loss: 34.6546 - val_loss: 28.4095\n",
      "Epoch 57/100\n",
      "379/379 [==============================] - 0s 166us/sample - loss: 33.9913 - val_loss: 28.0052\n",
      "Epoch 58/100\n",
      "379/379 [==============================] - 0s 161us/sample - loss: 33.4755 - val_loss: 27.5661\n",
      "Epoch 59/100\n",
      "379/379 [==============================] - 0s 166us/sample - loss: 32.9458 - val_loss: 27.1807\n",
      "Epoch 60/100\n",
      "379/379 [==============================] - 0s 163us/sample - loss: 32.5051 - val_loss: 26.7923\n",
      "Epoch 61/100\n",
      "379/379 [==============================] - 0s 166us/sample - loss: 32.0142 - val_loss: 26.4954\n",
      "Epoch 62/100\n",
      "379/379 [==============================] - 0s 168us/sample - loss: 31.6336 - val_loss: 26.1826\n",
      "Epoch 63/100\n",
      "379/379 [==============================] - 0s 163us/sample - loss: 31.2343 - val_loss: 25.9336\n",
      "Epoch 64/100\n",
      "379/379 [==============================] - 0s 166us/sample - loss: 30.8965 - val_loss: 25.6423\n",
      "Epoch 65/100\n",
      "379/379 [==============================] - 0s 168us/sample - loss: 30.5412 - val_loss: 25.4175\n",
      "Epoch 66/100\n",
      "379/379 [==============================] - 0s 166us/sample - loss: 30.2221 - val_loss: 25.1948\n",
      "Epoch 67/100\n",
      "379/379 [==============================] - 0s 165us/sample - loss: 29.9333 - val_loss: 24.9751\n",
      "Epoch 68/100\n",
      "379/379 [==============================] - 0s 170us/sample - loss: 29.6566 - val_loss: 24.7765\n",
      "Epoch 69/100\n",
      "379/379 [==============================] - 0s 168us/sample - loss: 29.3838 - val_loss: 24.5872\n",
      "Epoch 70/100\n",
      "379/379 [==============================] - 0s 166us/sample - loss: 29.1273 - val_loss: 24.4261\n",
      "Epoch 71/100\n",
      "379/379 [==============================] - 0s 168us/sample - loss: 28.8935 - val_loss: 24.2808\n",
      "Epoch 72/100\n",
      "379/379 [==============================] - 0s 164us/sample - loss: 28.6609 - val_loss: 24.1342\n",
      "Epoch 73/100\n",
      "379/379 [==============================] - 0s 165us/sample - loss: 28.4375 - val_loss: 24.0084\n",
      "Epoch 74/100\n",
      "379/379 [==============================] - 0s 169us/sample - loss: 28.2648 - val_loss: 23.8554\n",
      "Epoch 75/100\n",
      "379/379 [==============================] - 0s 167us/sample - loss: 28.0277 - val_loss: 23.7570\n",
      "Epoch 76/100\n",
      "379/379 [==============================] - 0s 171us/sample - loss: 27.8271 - val_loss: 23.6348\n",
      "Epoch 77/100\n",
      "379/379 [==============================] - 0s 168us/sample - loss: 27.6474 - val_loss: 23.5081\n",
      "Epoch 78/100\n",
      "379/379 [==============================] - 0s 161us/sample - loss: 27.4560 - val_loss: 23.4040\n",
      "Epoch 79/100\n",
      "379/379 [==============================] - 0s 169us/sample - loss: 27.2577 - val_loss: 23.3023\n",
      "Epoch 80/100\n",
      "379/379 [==============================] - 0s 163us/sample - loss: 27.0953 - val_loss: 23.2062\n",
      "Epoch 81/100\n",
      "379/379 [==============================] - 0s 171us/sample - loss: 26.9164 - val_loss: 23.0925\n",
      "Epoch 82/100\n",
      "379/379 [==============================] - 0s 171us/sample - loss: 26.7530 - val_loss: 22.9828\n",
      "Epoch 83/100\n",
      "379/379 [==============================] - 0s 165us/sample - loss: 26.5937 - val_loss: 22.8886\n",
      "Epoch 84/100\n",
      "379/379 [==============================] - 0s 168us/sample - loss: 26.4388 - val_loss: 22.7913\n",
      "Epoch 85/100\n",
      "379/379 [==============================] - 0s 170us/sample - loss: 26.2804 - val_loss: 22.7132\n",
      "Epoch 86/100\n",
      "379/379 [==============================] - 0s 172us/sample - loss: 26.1265 - val_loss: 22.6308\n",
      "Epoch 87/100\n",
      "379/379 [==============================] - 0s 167us/sample - loss: 25.9794 - val_loss: 22.5325\n",
      "Epoch 88/100\n",
      "379/379 [==============================] - 0s 172us/sample - loss: 25.8335 - val_loss: 22.4461\n",
      "Epoch 89/100\n",
      "379/379 [==============================] - 0s 171us/sample - loss: 25.6849 - val_loss: 22.3645\n",
      "Epoch 90/100\n",
      "379/379 [==============================] - 0s 166us/sample - loss: 25.5469 - val_loss: 22.2901\n",
      "Epoch 91/100\n",
      "379/379 [==============================] - 0s 166us/sample - loss: 25.4211 - val_loss: 22.2028\n",
      "Epoch 92/100\n",
      "379/379 [==============================] - 0s 163us/sample - loss: 25.2840 - val_loss: 22.1345\n",
      "Epoch 93/100\n",
      "379/379 [==============================] - 0s 165us/sample - loss: 25.1547 - val_loss: 22.0618\n",
      "Epoch 94/100\n",
      "379/379 [==============================] - 0s 164us/sample - loss: 25.0125 - val_loss: 21.9747\n",
      "Epoch 95/100\n",
      "379/379 [==============================] - 0s 163us/sample - loss: 24.8849 - val_loss: 21.8962\n",
      "Epoch 96/100\n",
      "379/379 [==============================] - 0s 168us/sample - loss: 24.7524 - val_loss: 21.8205\n",
      "Epoch 97/100\n",
      "379/379 [==============================] - 0s 166us/sample - loss: 24.6387 - val_loss: 21.7483\n",
      "Epoch 98/100\n",
      "379/379 [==============================] - 0s 170us/sample - loss: 24.5148 - val_loss: 21.6528\n",
      "Epoch 99/100\n",
      "379/379 [==============================] - 0s 171us/sample - loss: 24.3667 - val_loss: 21.5874\n",
      "Epoch 100/100\n",
      "379/379 [==============================] - 0s 166us/sample - loss: 24.2840 - val_loss: 21.5363\n",
      "\n",
      "Forming the grid-search model #9 using: optimizer=<tensorflow.python.keras.optimizer_v2.adam.Adam object at 0x7f9132bd5690>, kernel=<tensorflow.python.ops.init_ops_v2.Orthogonal object at 0x7f9132bbb090>, epochs=100, batch_size=64\n",
      "Train on 379 samples, validate on 127 samples\n",
      "Epoch 1/100\n",
      "379/379 [==============================] - 1s 2ms/sample - loss: 583.3711 - val_loss: 586.0685\n",
      "Epoch 2/100\n",
      "379/379 [==============================] - 0s 109us/sample - loss: 569.4891 - val_loss: 569.4472\n",
      "Epoch 3/100\n",
      "379/379 [==============================] - 0s 111us/sample - loss: 553.4791 - val_loss: 552.1646\n",
      "Epoch 4/100\n",
      "379/379 [==============================] - 0s 106us/sample - loss: 537.2972 - val_loss: 534.3410\n",
      "Epoch 5/100\n",
      "379/379 [==============================] - 0s 108us/sample - loss: 520.9199 - val_loss: 515.8139\n",
      "Epoch 6/100\n",
      "379/379 [==============================] - 0s 108us/sample - loss: 503.8432 - val_loss: 497.2187\n",
      "Epoch 7/100\n",
      "379/379 [==============================] - 0s 107us/sample - loss: 486.6951 - val_loss: 478.4343\n",
      "Epoch 8/100\n",
      "379/379 [==============================] - 0s 106us/sample - loss: 469.8084 - val_loss: 459.4716\n",
      "Epoch 9/100\n",
      "379/379 [==============================] - 0s 106us/sample - loss: 452.6146 - val_loss: 440.5282\n",
      "Epoch 10/100\n",
      "379/379 [==============================] - 0s 106us/sample - loss: 435.8522 - val_loss: 421.5575\n",
      "Epoch 11/100\n",
      "379/379 [==============================] - 0s 121us/sample - loss: 418.7707 - val_loss: 402.9831\n",
      "Epoch 12/100\n",
      "379/379 [==============================] - 0s 109us/sample - loss: 402.3105 - val_loss: 384.6163\n",
      "Epoch 13/100\n",
      "379/379 [==============================] - 0s 109us/sample - loss: 385.8197 - val_loss: 366.9423\n",
      "Epoch 14/100\n",
      "379/379 [==============================] - 0s 109us/sample - loss: 370.3971 - val_loss: 349.6999\n",
      "Epoch 15/100\n",
      "379/379 [==============================] - 0s 109us/sample - loss: 355.0261 - val_loss: 333.1162\n",
      "Epoch 16/100\n",
      "379/379 [==============================] - 0s 109us/sample - loss: 340.5745 - val_loss: 317.1238\n",
      "Epoch 17/100\n",
      "379/379 [==============================] - 0s 112us/sample - loss: 326.0850 - val_loss: 302.1067\n",
      "Epoch 18/100\n",
      "379/379 [==============================] - 0s 108us/sample - loss: 312.6869 - val_loss: 287.7379\n",
      "Epoch 19/100\n",
      "379/379 [==============================] - 0s 108us/sample - loss: 299.9577 - val_loss: 274.0461\n",
      "Epoch 20/100\n",
      "379/379 [==============================] - 0s 112us/sample - loss: 287.9603 - val_loss: 260.9883\n",
      "Epoch 21/100\n",
      "379/379 [==============================] - 0s 107us/sample - loss: 276.0678 - val_loss: 248.8549\n",
      "Epoch 22/100\n",
      "379/379 [==============================] - 0s 105us/sample - loss: 265.2887 - val_loss: 237.3365\n",
      "Epoch 23/100\n",
      "379/379 [==============================] - 0s 107us/sample - loss: 254.1323 - val_loss: 226.8767\n",
      "Epoch 24/100\n",
      "379/379 [==============================] - 0s 108us/sample - loss: 244.6037 - val_loss: 216.8144\n",
      "Epoch 25/100\n",
      "379/379 [==============================] - 0s 106us/sample - loss: 235.1901 - val_loss: 207.3508\n",
      "Epoch 26/100\n",
      "379/379 [==============================] - 0s 106us/sample - loss: 226.1817 - val_loss: 198.5208\n",
      "Epoch 27/100\n",
      "379/379 [==============================] - 0s 112us/sample - loss: 217.8908 - val_loss: 190.1818\n",
      "Epoch 28/100\n",
      "379/379 [==============================] - 0s 106us/sample - loss: 209.7853 - val_loss: 182.4363\n",
      "Epoch 29/100\n",
      "379/379 [==============================] - 0s 108us/sample - loss: 202.4227 - val_loss: 175.0642\n",
      "Epoch 30/100\n",
      "379/379 [==============================] - 0s 105us/sample - loss: 195.0605 - val_loss: 168.2112\n",
      "Epoch 31/100\n",
      "379/379 [==============================] - 0s 107us/sample - loss: 187.9917 - val_loss: 161.8182\n",
      "Epoch 32/100\n",
      "379/379 [==============================] - 0s 106us/sample - loss: 181.7944 - val_loss: 155.5825\n",
      "Epoch 33/100\n",
      "379/379 [==============================] - 0s 110us/sample - loss: 175.2554 - val_loss: 149.8448\n",
      "Epoch 34/100\n",
      "379/379 [==============================] - 0s 112us/sample - loss: 169.3822 - val_loss: 144.3247\n",
      "Epoch 35/100\n",
      "379/379 [==============================] - 0s 106us/sample - loss: 163.7467 - val_loss: 139.0279\n",
      "Epoch 36/100\n",
      "379/379 [==============================] - 0s 108us/sample - loss: 158.0170 - val_loss: 134.0814\n",
      "Epoch 37/100\n",
      "379/379 [==============================] - 0s 108us/sample - loss: 152.6775 - val_loss: 129.3351\n",
      "Epoch 38/100\n",
      "379/379 [==============================] - 0s 109us/sample - loss: 147.6771 - val_loss: 124.7530\n",
      "Epoch 39/100\n",
      "379/379 [==============================] - 0s 107us/sample - loss: 142.6253 - val_loss: 120.4100\n",
      "Epoch 40/100\n",
      "379/379 [==============================] - 0s 111us/sample - loss: 137.8615 - val_loss: 116.2492\n",
      "Epoch 41/100\n",
      "379/379 [==============================] - 0s 109us/sample - loss: 133.2971 - val_loss: 112.2324\n",
      "Epoch 42/100\n",
      "379/379 [==============================] - 0s 106us/sample - loss: 128.8357 - val_loss: 108.3795\n",
      "Epoch 43/100\n",
      "379/379 [==============================] - 0s 109us/sample - loss: 124.3785 - val_loss: 104.7734\n",
      "Epoch 44/100\n",
      "379/379 [==============================] - 0s 107us/sample - loss: 120.4107 - val_loss: 101.2133\n",
      "Epoch 45/100\n",
      "379/379 [==============================] - 0s 109us/sample - loss: 116.3977 - val_loss: 97.8060\n",
      "Epoch 46/100\n",
      "379/379 [==============================] - 0s 113us/sample - loss: 112.5301 - val_loss: 94.5402\n",
      "Epoch 47/100\n",
      "379/379 [==============================] - 0s 107us/sample - loss: 108.8800 - val_loss: 91.3756\n",
      "Epoch 48/100\n",
      "379/379 [==============================] - 0s 105us/sample - loss: 105.1948 - val_loss: 88.4031\n",
      "Epoch 49/100\n",
      "379/379 [==============================] - 0s 111us/sample - loss: 101.8218 - val_loss: 85.5257\n",
      "Epoch 50/100\n",
      "379/379 [==============================] - 0s 105us/sample - loss: 98.5044 - val_loss: 82.7635\n",
      "Epoch 51/100\n",
      "379/379 [==============================] - 0s 105us/sample - loss: 95.3541 - val_loss: 80.1005\n",
      "Epoch 52/100\n",
      "379/379 [==============================] - 0s 108us/sample - loss: 92.3532 - val_loss: 77.5383\n",
      "Epoch 53/100\n",
      "379/379 [==============================] - 0s 106us/sample - loss: 89.5074 - val_loss: 75.0613\n",
      "Epoch 54/100\n",
      "379/379 [==============================] - 0s 108us/sample - loss: 86.5540 - val_loss: 72.7641\n",
      "Epoch 55/100\n",
      "379/379 [==============================] - 0s 103us/sample - loss: 83.9510 - val_loss: 70.5266\n",
      "Epoch 56/100\n",
      "379/379 [==============================] - 0s 111us/sample - loss: 81.3976 - val_loss: 68.3852\n",
      "Epoch 57/100\n",
      "379/379 [==============================] - 0s 105us/sample - loss: 78.7972 - val_loss: 66.3906\n",
      "Epoch 58/100\n",
      "379/379 [==============================] - 0s 112us/sample - loss: 76.5363 - val_loss: 64.4309\n",
      "Epoch 59/100\n",
      "379/379 [==============================] - 0s 108us/sample - loss: 74.3019 - val_loss: 62.5548\n",
      "Epoch 60/100\n",
      "379/379 [==============================] - 0s 105us/sample - loss: 72.1834 - val_loss: 60.7420\n",
      "Epoch 61/100\n",
      "379/379 [==============================] - 0s 115us/sample - loss: 70.0068 - val_loss: 59.0624\n",
      "Epoch 62/100\n",
      "379/379 [==============================] - 0s 119us/sample - loss: 68.1087 - val_loss: 57.4302\n",
      "Epoch 63/100\n",
      "379/379 [==============================] - 0s 105us/sample - loss: 66.1648 - val_loss: 55.9044\n",
      "Epoch 64/100\n",
      "379/379 [==============================] - 0s 108us/sample - loss: 64.4785 - val_loss: 54.3922\n",
      "Epoch 65/100\n",
      "379/379 [==============================] - 0s 107us/sample - loss: 62.7286 - val_loss: 52.9657\n",
      "Epoch 66/100\n",
      "379/379 [==============================] - 0s 106us/sample - loss: 61.1154 - val_loss: 51.5904\n",
      "Epoch 67/100\n",
      "379/379 [==============================] - 0s 115us/sample - loss: 59.5483 - val_loss: 50.2843\n",
      "Epoch 68/100\n",
      "379/379 [==============================] - 0s 105us/sample - loss: 58.0634 - val_loss: 49.0455\n",
      "Epoch 69/100\n",
      "379/379 [==============================] - 0s 105us/sample - loss: 56.6422 - val_loss: 47.8575\n",
      "Epoch 70/100\n",
      "379/379 [==============================] - 0s 105us/sample - loss: 55.2907 - val_loss: 46.7286\n",
      "Epoch 71/100\n",
      "379/379 [==============================] - 0s 107us/sample - loss: 53.9795 - val_loss: 45.6616\n",
      "Epoch 72/100\n",
      "379/379 [==============================] - 0s 107us/sample - loss: 52.7798 - val_loss: 44.6304\n",
      "Epoch 73/100\n",
      "379/379 [==============================] - 0s 108us/sample - loss: 51.5662 - val_loss: 43.6587\n",
      "Epoch 74/100\n",
      "379/379 [==============================] - 0s 109us/sample - loss: 50.4997 - val_loss: 42.7081\n",
      "Epoch 75/100\n",
      "379/379 [==============================] - 0s 109us/sample - loss: 49.3761 - val_loss: 41.8242\n",
      "Epoch 76/100\n",
      "379/379 [==============================] - 0s 110us/sample - loss: 48.3443 - val_loss: 40.9869\n",
      "Epoch 77/100\n",
      "379/379 [==============================] - 0s 113us/sample - loss: 47.4414 - val_loss: 40.1601\n",
      "Epoch 78/100\n",
      "379/379 [==============================] - 0s 107us/sample - loss: 46.4833 - val_loss: 39.3850\n",
      "Epoch 79/100\n",
      "379/379 [==============================] - 0s 106us/sample - loss: 45.5884 - val_loss: 38.6500\n",
      "Epoch 80/100\n",
      "379/379 [==============================] - 0s 109us/sample - loss: 44.7402 - val_loss: 37.9514\n",
      "Epoch 81/100\n",
      "379/379 [==============================] - 0s 109us/sample - loss: 43.9484 - val_loss: 37.2798\n",
      "Epoch 82/100\n",
      "379/379 [==============================] - 0s 100us/sample - loss: 43.2015 - val_loss: 36.6317\n",
      "Epoch 83/100\n",
      "379/379 [==============================] - 0s 103us/sample - loss: 42.4714 - val_loss: 36.0155\n",
      "Epoch 84/100\n",
      "379/379 [==============================] - 0s 103us/sample - loss: 41.7898 - val_loss: 35.4242\n",
      "Epoch 85/100\n",
      "379/379 [==============================] - 0s 109us/sample - loss: 41.1169 - val_loss: 34.8697\n",
      "Epoch 86/100\n",
      "379/379 [==============================] - 0s 104us/sample - loss: 40.4872 - val_loss: 34.3444\n",
      "Epoch 87/100\n",
      "379/379 [==============================] - 0s 103us/sample - loss: 39.9243 - val_loss: 33.8241\n",
      "Epoch 88/100\n",
      "379/379 [==============================] - 0s 107us/sample - loss: 39.3133 - val_loss: 33.3464\n",
      "Epoch 89/100\n",
      "379/379 [==============================] - 0s 107us/sample - loss: 38.7899 - val_loss: 32.8832\n",
      "Epoch 90/100\n",
      "379/379 [==============================] - 0s 102us/sample - loss: 38.2575 - val_loss: 32.4489\n",
      "Epoch 91/100\n",
      "379/379 [==============================] - 0s 104us/sample - loss: 37.7686 - val_loss: 32.0288\n",
      "Epoch 92/100\n",
      "379/379 [==============================] - 0s 108us/sample - loss: 37.3007 - val_loss: 31.6277\n",
      "Epoch 93/100\n",
      "379/379 [==============================] - 0s 106us/sample - loss: 36.8181 - val_loss: 31.2521\n",
      "Epoch 94/100\n",
      "379/379 [==============================] - 0s 106us/sample - loss: 36.4101 - val_loss: 30.8818\n",
      "Epoch 95/100\n",
      "379/379 [==============================] - 0s 107us/sample - loss: 35.9930 - val_loss: 30.5324\n",
      "Epoch 96/100\n",
      "379/379 [==============================] - 0s 107us/sample - loss: 35.5929 - val_loss: 30.1963\n",
      "Epoch 97/100\n",
      "379/379 [==============================] - 0s 109us/sample - loss: 35.2073 - val_loss: 29.8831\n",
      "Epoch 98/100\n",
      "379/379 [==============================] - 0s 105us/sample - loss: 34.8568 - val_loss: 29.5755\n",
      "Epoch 99/100\n",
      "379/379 [==============================] - 0s 113us/sample - loss: 34.4970 - val_loss: 29.2861\n",
      "Epoch 100/100\n",
      "379/379 [==============================] - 0s 107us/sample - loss: 34.1623 - val_loss: 29.0174\n",
      "\n",
      "Forming the grid-search model #10 using: optimizer=<tensorflow.python.keras.optimizer_v2.adam.Adam object at 0x7f9132bd5690>, kernel=<tensorflow.python.ops.init_ops_v2.GlorotNormal object at 0x7f9132ed1050>, epochs=100, batch_size=32\n",
      "Train on 379 samples, validate on 127 samples\n",
      "Epoch 1/100\n",
      "379/379 [==============================] - 1s 2ms/sample - loss: 557.1844 - val_loss: 557.2977\n",
      "Epoch 2/100\n",
      "379/379 [==============================] - 0s 165us/sample - loss: 521.0592 - val_loss: 517.0619\n",
      "Epoch 3/100\n",
      "379/379 [==============================] - 0s 167us/sample - loss: 483.1331 - val_loss: 478.8113\n",
      "Epoch 4/100\n",
      "379/379 [==============================] - 0s 167us/sample - loss: 446.6851 - val_loss: 439.9314\n",
      "Epoch 5/100\n",
      "379/379 [==============================] - 0s 169us/sample - loss: 410.9440 - val_loss: 400.8113\n",
      "Epoch 6/100\n",
      "379/379 [==============================] - 0s 171us/sample - loss: 375.0364 - val_loss: 364.0922\n",
      "Epoch 7/100\n",
      "379/379 [==============================] - 0s 169us/sample - loss: 340.9552 - val_loss: 328.7744\n",
      "Epoch 8/100\n",
      "379/379 [==============================] - 0s 169us/sample - loss: 309.1061 - val_loss: 295.7519\n",
      "Epoch 9/100\n",
      "379/379 [==============================] - 0s 172us/sample - loss: 279.3504 - val_loss: 265.4384\n",
      "Epoch 10/100\n",
      "379/379 [==============================] - 0s 166us/sample - loss: 252.3329 - val_loss: 237.4094\n",
      "Epoch 11/100\n",
      "379/379 [==============================] - 0s 169us/sample - loss: 227.3933 - val_loss: 212.5636\n",
      "Epoch 12/100\n",
      "379/379 [==============================] - 0s 168us/sample - loss: 204.9953 - val_loss: 190.5998\n",
      "Epoch 13/100\n",
      "379/379 [==============================] - 0s 163us/sample - loss: 185.0043 - val_loss: 171.5881\n",
      "Epoch 14/100\n",
      "379/379 [==============================] - 0s 164us/sample - loss: 167.9752 - val_loss: 154.7804\n",
      "Epoch 15/100\n",
      "379/379 [==============================] - 0s 163us/sample - loss: 152.5528 - val_loss: 140.4349\n",
      "Epoch 16/100\n",
      "379/379 [==============================] - 0s 164us/sample - loss: 139.5368 - val_loss: 127.8548\n",
      "Epoch 17/100\n",
      "379/379 [==============================] - 0s 165us/sample - loss: 127.7704 - val_loss: 117.3976\n",
      "Epoch 18/100\n",
      "379/379 [==============================] - 0s 164us/sample - loss: 117.9481 - val_loss: 108.1292\n",
      "Epoch 19/100\n",
      "379/379 [==============================] - 0s 165us/sample - loss: 109.1601 - val_loss: 100.2064\n",
      "Epoch 20/100\n",
      "379/379 [==============================] - 0s 163us/sample - loss: 101.7259 - val_loss: 93.1253\n",
      "Epoch 21/100\n",
      "379/379 [==============================] - 0s 165us/sample - loss: 94.8091 - val_loss: 87.1446\n",
      "Epoch 22/100\n",
      "379/379 [==============================] - 0s 161us/sample - loss: 89.0185 - val_loss: 81.7118\n",
      "Epoch 23/100\n",
      "379/379 [==============================] - 0s 164us/sample - loss: 83.5609 - val_loss: 76.9495\n",
      "Epoch 24/100\n",
      "379/379 [==============================] - 0s 164us/sample - loss: 79.0091 - val_loss: 72.4869\n",
      "Epoch 25/100\n",
      "379/379 [==============================] - 0s 168us/sample - loss: 74.4694 - val_loss: 68.6685\n",
      "Epoch 26/100\n",
      "379/379 [==============================] - 0s 167us/sample - loss: 70.6281 - val_loss: 65.1312\n",
      "Epoch 27/100\n",
      "379/379 [==============================] - 0s 168us/sample - loss: 67.0813 - val_loss: 61.8863\n",
      "Epoch 28/100\n",
      "379/379 [==============================] - 0s 165us/sample - loss: 63.9147 - val_loss: 58.8456\n",
      "Epoch 29/100\n",
      "379/379 [==============================] - 0s 165us/sample - loss: 60.9243 - val_loss: 56.0854\n",
      "Epoch 30/100\n",
      "379/379 [==============================] - 0s 168us/sample - loss: 58.2193 - val_loss: 53.5414\n",
      "Epoch 31/100\n",
      "379/379 [==============================] - 0s 163us/sample - loss: 55.6510 - val_loss: 51.2706\n",
      "Epoch 32/100\n",
      "379/379 [==============================] - 0s 166us/sample - loss: 53.4547 - val_loss: 49.0907\n",
      "Epoch 33/100\n",
      "379/379 [==============================] - 0s 173us/sample - loss: 51.3168 - val_loss: 47.1277\n",
      "Epoch 34/100\n",
      "379/379 [==============================] - 0s 174us/sample - loss: 49.3936 - val_loss: 45.3204\n",
      "Epoch 35/100\n",
      "379/379 [==============================] - 0s 169us/sample - loss: 47.6625 - val_loss: 43.6239\n",
      "Epoch 36/100\n",
      "379/379 [==============================] - 0s 164us/sample - loss: 46.0475 - val_loss: 42.0704\n",
      "Epoch 37/100\n",
      "379/379 [==============================] - 0s 175us/sample - loss: 44.5285 - val_loss: 40.6933\n",
      "Epoch 38/100\n",
      "379/379 [==============================] - 0s 164us/sample - loss: 43.2372 - val_loss: 39.3528\n",
      "Epoch 39/100\n",
      "379/379 [==============================] - 0s 168us/sample - loss: 41.9711 - val_loss: 38.1344\n",
      "Epoch 40/100\n",
      "379/379 [==============================] - 0s 169us/sample - loss: 40.8294 - val_loss: 37.0229\n",
      "Epoch 41/100\n",
      "379/379 [==============================] - 0s 167us/sample - loss: 39.7641 - val_loss: 35.9949\n",
      "Epoch 42/100\n",
      "379/379 [==============================] - 0s 163us/sample - loss: 38.7935 - val_loss: 35.0364\n",
      "Epoch 43/100\n",
      "379/379 [==============================] - 0s 161us/sample - loss: 37.8951 - val_loss: 34.1722\n",
      "Epoch 44/100\n",
      "379/379 [==============================] - 0s 165us/sample - loss: 37.0407 - val_loss: 33.3632\n",
      "Epoch 45/100\n",
      "379/379 [==============================] - 0s 163us/sample - loss: 36.2616 - val_loss: 32.6197\n",
      "Epoch 46/100\n",
      "379/379 [==============================] - 0s 171us/sample - loss: 35.5526 - val_loss: 31.9167\n",
      "Epoch 47/100\n",
      "379/379 [==============================] - 0s 165us/sample - loss: 34.9096 - val_loss: 31.2465\n",
      "Epoch 48/100\n",
      "379/379 [==============================] - 0s 165us/sample - loss: 34.2655 - val_loss: 30.6642\n",
      "Epoch 49/100\n",
      "379/379 [==============================] - 0s 167us/sample - loss: 33.6844 - val_loss: 30.1140\n",
      "Epoch 50/100\n",
      "379/379 [==============================] - 0s 168us/sample - loss: 33.1539 - val_loss: 29.6054\n",
      "Epoch 51/100\n",
      "379/379 [==============================] - 0s 164us/sample - loss: 32.6454 - val_loss: 29.1188\n",
      "Epoch 52/100\n",
      "379/379 [==============================] - 0s 165us/sample - loss: 32.1923 - val_loss: 28.6542\n",
      "Epoch 53/100\n",
      "379/379 [==============================] - 0s 169us/sample - loss: 31.7418 - val_loss: 28.2328\n",
      "Epoch 54/100\n",
      "379/379 [==============================] - 0s 166us/sample - loss: 31.3238 - val_loss: 27.8599\n",
      "Epoch 55/100\n",
      "379/379 [==============================] - 0s 167us/sample - loss: 30.9277 - val_loss: 27.5077\n",
      "Epoch 56/100\n",
      "379/379 [==============================] - 0s 166us/sample - loss: 30.5809 - val_loss: 27.1681\n",
      "Epoch 57/100\n",
      "379/379 [==============================] - 0s 169us/sample - loss: 30.2081 - val_loss: 26.8907\n",
      "Epoch 58/100\n",
      "379/379 [==============================] - 0s 163us/sample - loss: 29.8893 - val_loss: 26.5638\n",
      "Epoch 59/100\n",
      "379/379 [==============================] - 0s 161us/sample - loss: 29.5652 - val_loss: 26.2909\n",
      "Epoch 60/100\n",
      "379/379 [==============================] - 0s 165us/sample - loss: 29.2847 - val_loss: 26.0146\n",
      "Epoch 61/100\n",
      "379/379 [==============================] - 0s 167us/sample - loss: 28.9849 - val_loss: 25.7735\n",
      "Epoch 62/100\n",
      "379/379 [==============================] - 0s 166us/sample - loss: 28.7244 - val_loss: 25.5456\n",
      "Epoch 63/100\n",
      "379/379 [==============================] - 0s 162us/sample - loss: 28.4624 - val_loss: 25.3392\n",
      "Epoch 64/100\n",
      "379/379 [==============================] - 0s 178us/sample - loss: 28.2263 - val_loss: 25.1261\n",
      "Epoch 65/100\n",
      "379/379 [==============================] - 0s 170us/sample - loss: 27.9839 - val_loss: 24.9331\n",
      "Epoch 66/100\n",
      "379/379 [==============================] - 0s 168us/sample - loss: 27.7580 - val_loss: 24.7405\n",
      "Epoch 67/100\n",
      "379/379 [==============================] - 0s 163us/sample - loss: 27.5434 - val_loss: 24.5646\n",
      "Epoch 68/100\n",
      "379/379 [==============================] - 0s 163us/sample - loss: 27.3351 - val_loss: 24.3846\n",
      "Epoch 69/100\n",
      "379/379 [==============================] - 0s 165us/sample - loss: 27.1365 - val_loss: 24.2165\n",
      "Epoch 70/100\n",
      "379/379 [==============================] - 0s 165us/sample - loss: 26.9264 - val_loss: 24.0652\n",
      "Epoch 71/100\n",
      "379/379 [==============================] - 0s 167us/sample - loss: 26.7411 - val_loss: 23.9157\n",
      "Epoch 72/100\n",
      "379/379 [==============================] - 0s 167us/sample - loss: 26.5565 - val_loss: 23.7551\n",
      "Epoch 73/100\n",
      "379/379 [==============================] - 0s 162us/sample - loss: 26.3826 - val_loss: 23.6156\n",
      "Epoch 74/100\n",
      "379/379 [==============================] - 0s 175us/sample - loss: 26.2280 - val_loss: 23.4797\n",
      "Epoch 75/100\n",
      "379/379 [==============================] - 0s 163us/sample - loss: 26.0340 - val_loss: 23.3605\n",
      "Epoch 76/100\n",
      "379/379 [==============================] - 0s 163us/sample - loss: 25.8687 - val_loss: 23.2222\n",
      "Epoch 77/100\n",
      "379/379 [==============================] - 0s 168us/sample - loss: 25.7137 - val_loss: 23.0946\n",
      "Epoch 78/100\n",
      "379/379 [==============================] - 0s 164us/sample - loss: 25.5493 - val_loss: 22.9641\n",
      "Epoch 79/100\n",
      "379/379 [==============================] - 0s 171us/sample - loss: 25.3833 - val_loss: 22.8434\n",
      "Epoch 80/100\n",
      "379/379 [==============================] - 0s 166us/sample - loss: 25.2366 - val_loss: 22.7272\n",
      "Epoch 81/100\n",
      "379/379 [==============================] - 0s 169us/sample - loss: 25.0817 - val_loss: 22.6023\n",
      "Epoch 82/100\n",
      "379/379 [==============================] - 0s 162us/sample - loss: 24.9336 - val_loss: 22.4923\n",
      "Epoch 83/100\n",
      "379/379 [==============================] - 0s 164us/sample - loss: 24.7918 - val_loss: 22.3826\n",
      "Epoch 84/100\n",
      "379/379 [==============================] - 0s 163us/sample - loss: 24.6508 - val_loss: 22.2693\n",
      "Epoch 85/100\n",
      "379/379 [==============================] - 0s 163us/sample - loss: 24.5066 - val_loss: 22.1793\n",
      "Epoch 86/100\n",
      "379/379 [==============================] - 0s 168us/sample - loss: 24.3658 - val_loss: 22.0676\n",
      "Epoch 87/100\n",
      "379/379 [==============================] - 0s 164us/sample - loss: 24.2295 - val_loss: 21.9493\n",
      "Epoch 88/100\n",
      "379/379 [==============================] - 0s 165us/sample - loss: 24.0985 - val_loss: 21.8499\n",
      "Epoch 89/100\n",
      "379/379 [==============================] - 0s 165us/sample - loss: 23.9605 - val_loss: 21.7477\n",
      "Epoch 90/100\n",
      "379/379 [==============================] - 0s 164us/sample - loss: 23.8316 - val_loss: 21.6466\n",
      "Epoch 91/100\n",
      "379/379 [==============================] - 0s 150us/sample - loss: 23.7131 - val_loss: 21.5358\n",
      "Epoch 92/100\n",
      "379/379 [==============================] - 0s 156us/sample - loss: 23.5847 - val_loss: 21.4433\n",
      "Epoch 93/100\n",
      "379/379 [==============================] - 0s 186us/sample - loss: 23.4646 - val_loss: 21.3621\n",
      "Epoch 94/100\n",
      "379/379 [==============================] - 0s 155us/sample - loss: 23.3314 - val_loss: 21.2565\n",
      "Epoch 95/100\n",
      "379/379 [==============================] - 0s 164us/sample - loss: 23.2143 - val_loss: 21.1554\n",
      "Epoch 96/100\n",
      "379/379 [==============================] - 0s 155us/sample - loss: 23.0898 - val_loss: 21.0752\n",
      "Epoch 97/100\n",
      "379/379 [==============================] - 0s 155us/sample - loss: 22.9834 - val_loss: 20.9893\n",
      "Epoch 98/100\n",
      "379/379 [==============================] - 0s 164us/sample - loss: 22.8651 - val_loss: 20.8839\n",
      "Epoch 99/100\n",
      "379/379 [==============================] - 0s 164us/sample - loss: 22.7278 - val_loss: 20.7884\n",
      "Epoch 100/100\n",
      "379/379 [==============================] - 0s 165us/sample - loss: 22.6423 - val_loss: 20.7158\n",
      "\n",
      "Forming the grid-search model #11 using: optimizer=<tensorflow.python.keras.optimizer_v2.adam.Adam object at 0x7f9132bd5690>, kernel=<tensorflow.python.ops.init_ops_v2.GlorotNormal object at 0x7f9132ed1050>, epochs=100, batch_size=64\n",
      "Train on 379 samples, validate on 127 samples\n",
      "Epoch 1/100\n",
      "379/379 [==============================] - 1s 3ms/sample - loss: 565.0120 - val_loss: 575.6899\n",
      "Epoch 2/100\n",
      "379/379 [==============================] - 0s 110us/sample - loss: 547.5663 - val_loss: 554.8060\n",
      "Epoch 3/100\n",
      "379/379 [==============================] - 0s 112us/sample - loss: 527.1404 - val_loss: 533.7649\n",
      "Epoch 4/100\n",
      "379/379 [==============================] - 0s 109us/sample - loss: 506.9911 - val_loss: 512.6410\n",
      "Epoch 5/100\n",
      "379/379 [==============================] - 0s 110us/sample - loss: 487.5529 - val_loss: 491.2814\n",
      "Epoch 6/100\n",
      "379/379 [==============================] - 0s 105us/sample - loss: 467.4635 - val_loss: 470.2618\n",
      "Epoch 7/100\n",
      "379/379 [==============================] - 0s 108us/sample - loss: 447.4192 - val_loss: 449.2129\n",
      "Epoch 8/100\n",
      "379/379 [==============================] - 0s 108us/sample - loss: 427.8850 - val_loss: 428.2044\n",
      "Epoch 9/100\n",
      "379/379 [==============================] - 0s 108us/sample - loss: 408.2926 - val_loss: 407.4513\n",
      "Epoch 10/100\n",
      "379/379 [==============================] - 0s 107us/sample - loss: 389.2522 - val_loss: 386.8753\n",
      "Epoch 11/100\n",
      "379/379 [==============================] - 0s 109us/sample - loss: 370.3018 - val_loss: 366.9368\n",
      "Epoch 12/100\n",
      "379/379 [==============================] - 0s 109us/sample - loss: 351.9706 - val_loss: 347.4946\n",
      "Epoch 13/100\n",
      "379/379 [==============================] - 0s 109us/sample - loss: 333.7556 - val_loss: 328.8773\n",
      "Epoch 14/100\n",
      "379/379 [==============================] - 0s 114us/sample - loss: 316.8721 - val_loss: 310.7440\n",
      "Epoch 15/100\n",
      "379/379 [==============================] - 0s 106us/sample - loss: 299.9323 - val_loss: 293.5032\n",
      "Epoch 16/100\n",
      "379/379 [==============================] - 0s 110us/sample - loss: 284.2700 - val_loss: 276.9043\n",
      "Epoch 17/100\n",
      "379/379 [==============================] - 0s 113us/sample - loss: 268.7109 - val_loss: 261.3499\n",
      "Epoch 18/100\n",
      "379/379 [==============================] - 0s 110us/sample - loss: 254.2729 - val_loss: 246.5274\n",
      "Epoch 19/100\n",
      "379/379 [==============================] - 0s 110us/sample - loss: 240.6604 - val_loss: 232.4354\n",
      "Epoch 20/100\n",
      "379/379 [==============================] - 0s 108us/sample - loss: 227.8779 - val_loss: 219.0755\n",
      "Epoch 21/100\n",
      "379/379 [==============================] - 0s 110us/sample - loss: 215.3900 - val_loss: 206.6848\n",
      "Epoch 22/100\n",
      "379/379 [==============================] - 0s 108us/sample - loss: 204.1764 - val_loss: 194.9927\n",
      "Epoch 23/100\n",
      "379/379 [==============================] - 0s 109us/sample - loss: 192.6669 - val_loss: 184.4845\n",
      "Epoch 24/100\n",
      "379/379 [==============================] - 0s 108us/sample - loss: 183.0468 - val_loss: 174.4563\n",
      "Epoch 25/100\n",
      "379/379 [==============================] - 0s 106us/sample - loss: 173.6206 - val_loss: 165.1543\n",
      "Epoch 26/100\n",
      "379/379 [==============================] - 0s 110us/sample - loss: 164.7599 - val_loss: 156.6068\n",
      "Epoch 27/100\n",
      "379/379 [==============================] - 0s 110us/sample - loss: 156.7538 - val_loss: 148.6546\n",
      "Epoch 28/100\n",
      "379/379 [==============================] - 0s 112us/sample - loss: 149.1829 - val_loss: 141.3476\n",
      "Epoch 29/100\n",
      "379/379 [==============================] - 0s 99us/sample - loss: 142.3530 - val_loss: 134.5432\n",
      "Epoch 30/100\n",
      "379/379 [==============================] - 0s 110us/sample - loss: 135.7502 - val_loss: 128.3242\n",
      "Epoch 31/100\n",
      "379/379 [==============================] - 0s 110us/sample - loss: 129.5887 - val_loss: 122.6397\n",
      "Epoch 32/100\n",
      "379/379 [==============================] - 0s 108us/sample - loss: 124.2980 - val_loss: 117.2290\n",
      "Epoch 33/100\n",
      "379/379 [==============================] - 0s 110us/sample - loss: 118.8473 - val_loss: 112.3655\n",
      "Epoch 34/100\n",
      "379/379 [==============================] - 0s 105us/sample - loss: 114.1121 - val_loss: 107.8010\n",
      "Epoch 35/100\n",
      "379/379 [==============================] - 0s 107us/sample - loss: 109.7210 - val_loss: 103.5068\n",
      "Epoch 36/100\n",
      "379/379 [==============================] - 0s 108us/sample - loss: 105.3433 - val_loss: 99.5974\n",
      "Epoch 37/100\n",
      "379/379 [==============================] - 0s 108us/sample - loss: 101.4608 - val_loss: 95.9139\n",
      "Epoch 38/100\n",
      "379/379 [==============================] - 0s 108us/sample - loss: 97.8495 - val_loss: 92.4563\n",
      "Epoch 39/100\n",
      "379/379 [==============================] - 0s 111us/sample - loss: 94.3419 - val_loss: 89.2363\n",
      "Epoch 40/100\n",
      "379/379 [==============================] - 0s 108us/sample - loss: 91.0663 - val_loss: 86.2380\n",
      "Epoch 41/100\n",
      "379/379 [==============================] - 0s 107us/sample - loss: 88.0507 - val_loss: 83.3893\n",
      "Epoch 42/100\n",
      "379/379 [==============================] - 0s 117us/sample - loss: 85.2169 - val_loss: 80.6859\n",
      "Epoch 43/100\n",
      "379/379 [==============================] - 0s 108us/sample - loss: 82.3550 - val_loss: 78.2156\n",
      "Epoch 44/100\n",
      "379/379 [==============================] - 0s 109us/sample - loss: 79.9143 - val_loss: 75.8135\n",
      "Epoch 45/100\n",
      "379/379 [==============================] - 0s 108us/sample - loss: 77.4788 - val_loss: 73.5444\n",
      "Epoch 46/100\n",
      "379/379 [==============================] - 0s 113us/sample - loss: 75.1882 - val_loss: 71.3876\n",
      "Epoch 47/100\n",
      "379/379 [==============================] - 0s 135us/sample - loss: 73.0618 - val_loss: 69.3102\n",
      "Epoch 48/100\n",
      "379/379 [==============================] - 0s 108us/sample - loss: 70.9043 - val_loss: 67.3863\n",
      "Epoch 49/100\n",
      "379/379 [==============================] - 0s 112us/sample - loss: 68.9852 - val_loss: 65.5275\n",
      "Epoch 50/100\n",
      "379/379 [==============================] - 0s 106us/sample - loss: 67.0921 - val_loss: 63.7635\n",
      "Epoch 51/100\n",
      "379/379 [==============================] - 0s 115us/sample - loss: 65.3276 - val_loss: 62.0612\n",
      "Epoch 52/100\n",
      "379/379 [==============================] - 0s 109us/sample - loss: 63.6781 - val_loss: 60.4176\n",
      "Epoch 53/100\n",
      "379/379 [==============================] - 0s 107us/sample - loss: 62.0682 - val_loss: 58.8465\n",
      "Epoch 54/100\n",
      "379/379 [==============================] - 0s 114us/sample - loss: 60.4389 - val_loss: 57.3913\n",
      "Epoch 55/100\n",
      "379/379 [==============================] - 0s 104us/sample - loss: 59.0253 - val_loss: 55.9642\n",
      "Epoch 56/100\n",
      "379/379 [==============================] - 0s 111us/sample - loss: 57.6439 - val_loss: 54.5869\n",
      "Epoch 57/100\n",
      "379/379 [==============================] - 0s 106us/sample - loss: 56.1872 - val_loss: 53.3267\n",
      "Epoch 58/100\n",
      "379/379 [==============================] - 0s 110us/sample - loss: 54.9973 - val_loss: 52.0621\n",
      "Epoch 59/100\n",
      "379/379 [==============================] - 0s 106us/sample - loss: 53.7607 - val_loss: 50.8668\n",
      "Epoch 60/100\n",
      "379/379 [==============================] - 0s 111us/sample - loss: 52.6333 - val_loss: 49.6926\n",
      "Epoch 61/100\n",
      "379/379 [==============================] - 0s 107us/sample - loss: 51.4559 - val_loss: 48.5977\n",
      "Epoch 62/100\n",
      "379/379 [==============================] - 0s 111us/sample - loss: 50.4242 - val_loss: 47.5290\n",
      "Epoch 63/100\n",
      "379/379 [==============================] - 0s 109us/sample - loss: 49.3635 - val_loss: 46.5259\n",
      "Epoch 64/100\n",
      "379/379 [==============================] - 0s 105us/sample - loss: 48.4329 - val_loss: 45.5380\n",
      "Epoch 65/100\n",
      "379/379 [==============================] - 0s 109us/sample - loss: 47.4864 - val_loss: 44.5935\n",
      "Epoch 66/100\n",
      "379/379 [==============================] - 0s 117us/sample - loss: 46.6080 - val_loss: 43.6813\n",
      "Epoch 67/100\n",
      "379/379 [==============================] - 0s 109us/sample - loss: 45.7528 - val_loss: 42.8100\n",
      "Epoch 68/100\n",
      "379/379 [==============================] - 0s 109us/sample - loss: 44.9142 - val_loss: 41.9901\n",
      "Epoch 69/100\n",
      "379/379 [==============================] - 0s 116us/sample - loss: 44.1658 - val_loss: 41.1831\n",
      "Epoch 70/100\n",
      "379/379 [==============================] - 0s 104us/sample - loss: 43.3904 - val_loss: 40.4301\n",
      "Epoch 71/100\n",
      "379/379 [==============================] - 0s 109us/sample - loss: 42.6744 - val_loss: 39.7076\n",
      "Epoch 72/100\n",
      "379/379 [==============================] - 0s 106us/sample - loss: 42.0041 - val_loss: 39.0013\n",
      "Epoch 73/100\n",
      "379/379 [==============================] - 0s 107us/sample - loss: 41.3343 - val_loss: 38.3335\n",
      "Epoch 74/100\n",
      "379/379 [==============================] - 0s 108us/sample - loss: 40.7385 - val_loss: 37.6820\n",
      "Epoch 75/100\n",
      "379/379 [==============================] - 0s 108us/sample - loss: 40.1019 - val_loss: 37.0650\n",
      "Epoch 76/100\n",
      "379/379 [==============================] - 0s 109us/sample - loss: 39.5105 - val_loss: 36.4773\n",
      "Epoch 77/100\n",
      "379/379 [==============================] - 0s 108us/sample - loss: 38.9681 - val_loss: 35.9047\n",
      "Epoch 78/100\n",
      "379/379 [==============================] - 0s 108us/sample - loss: 38.4300 - val_loss: 35.3485\n",
      "Epoch 79/100\n",
      "379/379 [==============================] - 0s 106us/sample - loss: 37.8957 - val_loss: 34.8266\n",
      "Epoch 80/100\n",
      "379/379 [==============================] - 0s 133us/sample - loss: 37.4008 - val_loss: 34.3244\n",
      "Epoch 81/100\n",
      "379/379 [==============================] - 0s 110us/sample - loss: 36.9282 - val_loss: 33.8415\n",
      "Epoch 82/100\n",
      "379/379 [==============================] - 0s 107us/sample - loss: 36.4700 - val_loss: 33.3804\n",
      "Epoch 83/100\n",
      "379/379 [==============================] - 0s 107us/sample - loss: 36.0282 - val_loss: 32.9422\n",
      "Epoch 84/100\n",
      "379/379 [==============================] - 0s 110us/sample - loss: 35.6153 - val_loss: 32.5152\n",
      "Epoch 85/100\n",
      "379/379 [==============================] - 0s 98us/sample - loss: 35.2096 - val_loss: 32.1075\n",
      "Epoch 86/100\n",
      "379/379 [==============================] - 0s 103us/sample - loss: 34.8077 - val_loss: 31.7246\n",
      "Epoch 87/100\n",
      "379/379 [==============================] - 0s 99us/sample - loss: 34.4534 - val_loss: 31.3409\n",
      "Epoch 88/100\n",
      "379/379 [==============================] - 0s 104us/sample - loss: 34.0748 - val_loss: 30.9845\n",
      "Epoch 89/100\n",
      "379/379 [==============================] - 0s 107us/sample - loss: 33.7410 - val_loss: 30.6382\n",
      "Epoch 90/100\n",
      "379/379 [==============================] - 0s 107us/sample - loss: 33.3923 - val_loss: 30.3163\n",
      "Epoch 91/100\n",
      "379/379 [==============================] - 0s 108us/sample - loss: 33.0919 - val_loss: 29.9970\n",
      "Epoch 92/100\n",
      "379/379 [==============================] - 0s 109us/sample - loss: 32.7901 - val_loss: 29.6958\n",
      "Epoch 93/100\n",
      "379/379 [==============================] - 0s 108us/sample - loss: 32.4839 - val_loss: 29.4141\n",
      "Epoch 94/100\n",
      "379/379 [==============================] - 0s 107us/sample - loss: 32.2152 - val_loss: 29.1329\n",
      "Epoch 95/100\n",
      "379/379 [==============================] - 0s 106us/sample - loss: 31.9378 - val_loss: 28.8680\n",
      "Epoch 96/100\n",
      "379/379 [==============================] - 0s 109us/sample - loss: 31.6831 - val_loss: 28.6145\n",
      "Epoch 97/100\n",
      "379/379 [==============================] - 0s 107us/sample - loss: 31.4350 - val_loss: 28.3722\n",
      "Epoch 98/100\n",
      "379/379 [==============================] - 0s 106us/sample - loss: 31.1881 - val_loss: 28.1423\n",
      "Epoch 99/100\n",
      "379/379 [==============================] - 0s 107us/sample - loss: 30.9521 - val_loss: 27.9165\n",
      "Epoch 100/100\n",
      "379/379 [==============================] - 0s 109us/sample - loss: 30.7347 - val_loss: 27.7057\n",
      "\n",
      "Forming the grid-search model #12 using: optimizer=<tensorflow.python.keras.optimizer_v2.adam.Adam object at 0x7f9132bd5290>, kernel=<tensorflow.python.ops.init_ops_v2.RandomNormal object at 0x7f9132bbbed0>, epochs=100, batch_size=32\n",
      "Train on 379 samples, validate on 127 samples\n",
      "Epoch 1/100\n",
      "379/379 [==============================] - 1s 2ms/sample - loss: 589.1770 - val_loss: 602.4497\n",
      "Epoch 2/100\n",
      "379/379 [==============================] - 0s 168us/sample - loss: 588.8906 - val_loss: 602.1421\n",
      "Epoch 3/100\n",
      "379/379 [==============================] - 0s 164us/sample - loss: 588.5992 - val_loss: 601.8417\n",
      "Epoch 4/100\n",
      "379/379 [==============================] - 0s 166us/sample - loss: 588.3117 - val_loss: 601.5362\n",
      "Epoch 5/100\n",
      "379/379 [==============================] - 0s 165us/sample - loss: 588.0243 - val_loss: 601.2250\n",
      "Epoch 6/100\n",
      "379/379 [==============================] - 0s 165us/sample - loss: 587.7256 - val_loss: 600.9184\n",
      "Epoch 7/100\n",
      "379/379 [==============================] - 0s 191us/sample - loss: 587.4268 - val_loss: 600.5994\n",
      "Epoch 8/100\n",
      "379/379 [==============================] - 0s 191us/sample - loss: 587.1226 - val_loss: 600.2716\n",
      "Epoch 9/100\n",
      "379/379 [==============================] - 0s 174us/sample - loss: 586.8096 - val_loss: 599.9396\n",
      "Epoch 10/100\n",
      "379/379 [==============================] - 0s 164us/sample - loss: 586.4956 - val_loss: 599.5916\n",
      "Epoch 11/100\n",
      "379/379 [==============================] - 0s 168us/sample - loss: 586.1653 - val_loss: 599.2402\n",
      "Epoch 12/100\n",
      "379/379 [==============================] - 0s 166us/sample - loss: 585.8323 - val_loss: 598.8736\n",
      "Epoch 13/100\n",
      "379/379 [==============================] - 0s 171us/sample - loss: 585.4827 - val_loss: 598.5001\n",
      "Epoch 14/100\n",
      "379/379 [==============================] - 0s 170us/sample - loss: 585.1289 - val_loss: 598.1109\n",
      "Epoch 15/100\n",
      "379/379 [==============================] - 0s 169us/sample - loss: 584.7557 - val_loss: 597.7131\n",
      "Epoch 16/100\n",
      "379/379 [==============================] - 0s 193us/sample - loss: 584.3779 - val_loss: 597.2968\n",
      "Epoch 17/100\n",
      "379/379 [==============================] - 0s 164us/sample - loss: 583.9810 - val_loss: 596.8694\n",
      "Epoch 18/100\n",
      "379/379 [==============================] - 0s 164us/sample - loss: 583.5691 - val_loss: 596.4292\n",
      "Epoch 19/100\n",
      "379/379 [==============================] - 0s 179us/sample - loss: 583.1516 - val_loss: 595.9683\n",
      "Epoch 20/100\n",
      "379/379 [==============================] - 0s 174us/sample - loss: 582.7177 - val_loss: 595.4878\n",
      "Epoch 21/100\n",
      "379/379 [==============================] - 0s 164us/sample - loss: 582.2641 - val_loss: 594.9979\n",
      "Epoch 22/100\n",
      "379/379 [==============================] - 0s 169us/sample - loss: 581.8064 - val_loss: 594.4861\n",
      "Epoch 23/100\n",
      "379/379 [==============================] - 0s 165us/sample - loss: 581.3055 - val_loss: 593.9866\n",
      "Epoch 24/100\n",
      "379/379 [==============================] - 0s 163us/sample - loss: 580.8258 - val_loss: 593.4458\n",
      "Epoch 25/100\n",
      "379/379 [==============================] - 0s 168us/sample - loss: 580.3175 - val_loss: 592.8867\n",
      "Epoch 26/100\n",
      "379/379 [==============================] - 0s 163us/sample - loss: 579.7947 - val_loss: 592.3112\n",
      "Epoch 27/100\n",
      "379/379 [==============================] - 0s 165us/sample - loss: 579.2545 - val_loss: 591.7270\n",
      "Epoch 28/100\n",
      "379/379 [==============================] - 0s 161us/sample - loss: 578.6941 - val_loss: 591.1355\n",
      "Epoch 29/100\n",
      "379/379 [==============================] - 0s 164us/sample - loss: 578.1388 - val_loss: 590.5081\n",
      "Epoch 30/100\n",
      "379/379 [==============================] - 0s 164us/sample - loss: 577.5465 - val_loss: 589.8757\n",
      "Epoch 31/100\n",
      "379/379 [==============================] - 0s 177us/sample - loss: 576.9489 - val_loss: 589.2202\n",
      "Epoch 32/100\n",
      "379/379 [==============================] - 0s 165us/sample - loss: 576.3328 - val_loss: 588.5506\n",
      "Epoch 33/100\n",
      "379/379 [==============================] - 0s 168us/sample - loss: 575.6970 - val_loss: 587.8686\n",
      "Epoch 34/100\n",
      "379/379 [==============================] - 0s 164us/sample - loss: 575.0548 - val_loss: 587.1611\n",
      "Epoch 35/100\n",
      "379/379 [==============================] - 0s 168us/sample - loss: 574.3900 - val_loss: 586.4428\n",
      "Epoch 36/100\n",
      "379/379 [==============================] - 0s 169us/sample - loss: 573.7073 - val_loss: 585.7083\n",
      "Epoch 37/100\n",
      "379/379 [==============================] - 0s 165us/sample - loss: 573.0176 - val_loss: 584.9493\n",
      "Epoch 38/100\n",
      "379/379 [==============================] - 0s 162us/sample - loss: 572.3036 - val_loss: 584.1781\n",
      "Epoch 39/100\n",
      "379/379 [==============================] - 0s 164us/sample - loss: 571.5768 - val_loss: 583.3851\n",
      "Epoch 40/100\n",
      "379/379 [==============================] - 0s 165us/sample - loss: 570.8263 - val_loss: 582.5868\n",
      "Epoch 41/100\n",
      "379/379 [==============================] - 0s 165us/sample - loss: 570.0766 - val_loss: 581.7599\n",
      "Epoch 42/100\n",
      "379/379 [==============================] - 0s 169us/sample - loss: 569.2879 - val_loss: 580.9378\n",
      "Epoch 43/100\n",
      "379/379 [==============================] - 0s 163us/sample - loss: 568.5060 - val_loss: 580.0839\n",
      "Epoch 44/100\n",
      "379/379 [==============================] - 0s 163us/sample - loss: 567.6998 - val_loss: 579.2122\n",
      "Epoch 45/100\n",
      "379/379 [==============================] - 0s 163us/sample - loss: 566.8786 - val_loss: 578.3193\n",
      "Epoch 46/100\n",
      "379/379 [==============================] - 0s 161us/sample - loss: 566.0255 - val_loss: 577.4240\n",
      "Epoch 47/100\n",
      "379/379 [==============================] - 0s 163us/sample - loss: 565.1906 - val_loss: 576.4779\n",
      "Epoch 48/100\n",
      "379/379 [==============================] - 0s 162us/sample - loss: 564.2927 - val_loss: 575.5552\n",
      "Epoch 49/100\n",
      "379/379 [==============================] - 0s 165us/sample - loss: 563.4138 - val_loss: 574.6033\n",
      "Epoch 50/100\n",
      "379/379 [==============================] - 0s 164us/sample - loss: 562.5122 - val_loss: 573.6346\n",
      "Epoch 51/100\n",
      "379/379 [==============================] - 0s 165us/sample - loss: 561.5963 - val_loss: 572.6438\n",
      "Epoch 52/100\n",
      "379/379 [==============================] - 0s 167us/sample - loss: 560.6682 - val_loss: 571.6276\n",
      "Epoch 53/100\n",
      "379/379 [==============================] - 0s 161us/sample - loss: 559.7075 - val_loss: 570.6131\n",
      "Epoch 54/100\n",
      "379/379 [==============================] - 0s 170us/sample - loss: 558.7406 - val_loss: 569.5811\n",
      "Epoch 55/100\n",
      "379/379 [==============================] - 0s 167us/sample - loss: 557.7724 - val_loss: 568.5144\n",
      "Epoch 56/100\n",
      "379/379 [==============================] - 0s 167us/sample - loss: 556.7649 - val_loss: 567.4517\n",
      "Epoch 57/100\n",
      "379/379 [==============================] - 0s 161us/sample - loss: 555.7622 - val_loss: 566.3618\n",
      "Epoch 58/100\n",
      "379/379 [==============================] - 0s 164us/sample - loss: 554.7471 - val_loss: 565.2502\n",
      "Epoch 59/100\n",
      "379/379 [==============================] - 0s 162us/sample - loss: 553.6973 - val_loss: 564.1435\n",
      "Epoch 60/100\n",
      "379/379 [==============================] - 0s 165us/sample - loss: 552.6433 - val_loss: 563.0215\n",
      "Epoch 61/100\n",
      "379/379 [==============================] - 0s 164us/sample - loss: 551.5846 - val_loss: 561.8725\n",
      "Epoch 62/100\n",
      "379/379 [==============================] - 0s 170us/sample - loss: 550.4857 - val_loss: 560.7307\n",
      "Epoch 63/100\n",
      "379/379 [==============================] - 0s 165us/sample - loss: 549.4036 - val_loss: 559.5529\n",
      "Epoch 64/100\n",
      "379/379 [==============================] - 0s 165us/sample - loss: 548.3041 - val_loss: 558.3484\n",
      "Epoch 65/100\n",
      "379/379 [==============================] - 0s 169us/sample - loss: 547.1588 - val_loss: 557.1580\n",
      "Epoch 66/100\n",
      "379/379 [==============================] - 0s 164us/sample - loss: 546.0208 - val_loss: 555.9475\n",
      "Epoch 67/100\n",
      "379/379 [==============================] - 0s 165us/sample - loss: 544.8835 - val_loss: 554.6973\n",
      "Epoch 68/100\n",
      "379/379 [==============================] - 0s 163us/sample - loss: 543.7129 - val_loss: 553.4389\n",
      "Epoch 69/100\n",
      "379/379 [==============================] - 0s 162us/sample - loss: 542.5196 - val_loss: 552.1759\n",
      "Epoch 70/100\n",
      "379/379 [==============================] - 0s 170us/sample - loss: 541.3340 - val_loss: 550.8849\n",
      "Epoch 71/100\n",
      "379/379 [==============================] - 0s 166us/sample - loss: 540.0939 - val_loss: 549.6145\n",
      "Epoch 72/100\n",
      "379/379 [==============================] - 0s 166us/sample - loss: 538.9022 - val_loss: 548.2686\n",
      "Epoch 73/100\n",
      "379/379 [==============================] - 0s 169us/sample - loss: 537.6395 - val_loss: 546.9238\n",
      "Epoch 74/100\n",
      "379/379 [==============================] - 0s 164us/sample - loss: 536.3624 - val_loss: 545.5805\n",
      "Epoch 75/100\n",
      "379/379 [==============================] - 0s 162us/sample - loss: 535.0752 - val_loss: 544.2158\n",
      "Epoch 76/100\n",
      "379/379 [==============================] - 0s 167us/sample - loss: 533.7891 - val_loss: 542.8063\n",
      "Epoch 77/100\n",
      "379/379 [==============================] - 0s 164us/sample - loss: 532.4885 - val_loss: 541.3674\n",
      "Epoch 78/100\n",
      "379/379 [==============================] - 0s 164us/sample - loss: 531.1224 - val_loss: 539.9412\n",
      "Epoch 79/100\n",
      "379/379 [==============================] - 0s 165us/sample - loss: 529.7598 - val_loss: 538.5041\n",
      "Epoch 80/100\n",
      "379/379 [==============================] - 0s 165us/sample - loss: 528.4030 - val_loss: 537.0138\n",
      "Epoch 81/100\n",
      "379/379 [==============================] - 0s 162us/sample - loss: 527.0101 - val_loss: 535.5052\n",
      "Epoch 82/100\n",
      "379/379 [==============================] - 0s 165us/sample - loss: 525.5995 - val_loss: 533.9836\n",
      "Epoch 83/100\n",
      "379/379 [==============================] - 0s 163us/sample - loss: 524.1735 - val_loss: 532.4454\n",
      "Epoch 84/100\n",
      "379/379 [==============================] - 0s 164us/sample - loss: 522.7228 - val_loss: 530.8992\n",
      "Epoch 85/100\n",
      "379/379 [==============================] - 0s 170us/sample - loss: 521.2711 - val_loss: 529.3129\n",
      "Epoch 86/100\n",
      "379/379 [==============================] - 0s 167us/sample - loss: 519.7862 - val_loss: 527.7253\n",
      "Epoch 87/100\n",
      "379/379 [==============================] - 0s 171us/sample - loss: 518.2928 - val_loss: 526.1077\n",
      "Epoch 88/100\n",
      "379/379 [==============================] - 0s 156us/sample - loss: 516.7599 - val_loss: 524.4920\n",
      "Epoch 89/100\n",
      "379/379 [==============================] - 0s 165us/sample - loss: 515.2481 - val_loss: 522.8191\n",
      "Epoch 90/100\n",
      "379/379 [==============================] - 0s 164us/sample - loss: 513.6728 - val_loss: 521.1622\n",
      "Epoch 91/100\n",
      "379/379 [==============================] - 0s 166us/sample - loss: 512.1106 - val_loss: 519.4698\n",
      "Epoch 92/100\n",
      "379/379 [==============================] - 0s 168us/sample - loss: 510.5184 - val_loss: 517.7585\n",
      "Epoch 93/100\n",
      "379/379 [==============================] - 0s 167us/sample - loss: 508.9074 - val_loss: 516.0236\n",
      "Epoch 94/100\n",
      "379/379 [==============================] - 0s 168us/sample - loss: 507.2597 - val_loss: 514.2989\n",
      "Epoch 95/100\n",
      "379/379 [==============================] - 0s 165us/sample - loss: 505.6692 - val_loss: 512.4707\n",
      "Epoch 96/100\n",
      "379/379 [==============================] - 0s 168us/sample - loss: 503.9524 - val_loss: 510.6860\n",
      "Epoch 97/100\n",
      "379/379 [==============================] - 0s 167us/sample - loss: 502.2759 - val_loss: 508.8585\n",
      "Epoch 98/100\n",
      "379/379 [==============================] - 0s 168us/sample - loss: 500.5933 - val_loss: 506.9775\n",
      "Epoch 99/100\n",
      "379/379 [==============================] - 0s 166us/sample - loss: 498.8434 - val_loss: 505.0964\n",
      "Epoch 100/100\n",
      "379/379 [==============================] - 0s 165us/sample - loss: 497.0634 - val_loss: 503.2177\n",
      "\n",
      "Forming the grid-search model #13 using: optimizer=<tensorflow.python.keras.optimizer_v2.adam.Adam object at 0x7f9132bd5290>, kernel=<tensorflow.python.ops.init_ops_v2.RandomNormal object at 0x7f9132bbbed0>, epochs=100, batch_size=64\n",
      "Train on 379 samples, validate on 127 samples\n",
      "Epoch 1/100\n",
      "379/379 [==============================] - 1s 2ms/sample - loss: 589.0859 - val_loss: 602.0956\n",
      "Epoch 2/100\n",
      "379/379 [==============================] - 0s 111us/sample - loss: 588.3667 - val_loss: 601.2382\n",
      "Epoch 3/100\n",
      "379/379 [==============================] - 0s 113us/sample - loss: 587.5362 - val_loss: 600.3810\n",
      "Epoch 4/100\n",
      "379/379 [==============================] - 0s 109us/sample - loss: 586.7223 - val_loss: 599.5311\n",
      "Epoch 5/100\n",
      "379/379 [==============================] - 0s 110us/sample - loss: 585.9406 - val_loss: 598.6947\n",
      "Epoch 6/100\n",
      "379/379 [==============================] - 0s 112us/sample - loss: 585.1542 - val_loss: 597.8809\n",
      "Epoch 7/100\n",
      "379/379 [==============================] - 0s 123us/sample - loss: 584.3833 - val_loss: 597.0736\n",
      "Epoch 8/100\n",
      "379/379 [==============================] - 0s 109us/sample - loss: 583.6305 - val_loss: 596.2747\n",
      "Epoch 9/100\n",
      "379/379 [==============================] - 0s 109us/sample - loss: 582.8723 - val_loss: 595.4827\n",
      "Epoch 10/100\n",
      "379/379 [==============================] - 0s 112us/sample - loss: 582.1298 - val_loss: 594.6822\n",
      "Epoch 11/100\n",
      "379/379 [==============================] - 0s 108us/sample - loss: 581.3644 - val_loss: 593.8855\n",
      "Epoch 12/100\n",
      "379/379 [==============================] - 0s 110us/sample - loss: 580.6192 - val_loss: 593.0750\n",
      "Epoch 13/100\n",
      "379/379 [==============================] - 0s 109us/sample - loss: 579.8471 - val_loss: 592.2633\n",
      "Epoch 14/100\n",
      "379/379 [==============================] - 0s 109us/sample - loss: 579.0895 - val_loss: 591.4370\n",
      "Epoch 15/100\n",
      "379/379 [==============================] - 0s 111us/sample - loss: 578.3030 - val_loss: 590.6054\n",
      "Epoch 16/100\n",
      "379/379 [==============================] - 0s 110us/sample - loss: 577.5306 - val_loss: 589.7599\n",
      "Epoch 17/100\n",
      "379/379 [==============================] - 0s 105us/sample - loss: 576.7338 - val_loss: 588.9136\n",
      "Epoch 18/100\n",
      "379/379 [==============================] - 0s 110us/sample - loss: 575.9302 - val_loss: 588.0607\n",
      "Epoch 19/100\n",
      "379/379 [==============================] - 0s 108us/sample - loss: 575.1410 - val_loss: 587.1905\n",
      "Epoch 20/100\n",
      "379/379 [==============================] - 0s 105us/sample - loss: 574.3347 - val_loss: 586.3114\n",
      "Epoch 21/100\n",
      "379/379 [==============================] - 0s 110us/sample - loss: 573.5172 - val_loss: 585.4311\n",
      "Epoch 22/100\n",
      "379/379 [==============================] - 0s 107us/sample - loss: 572.7100 - val_loss: 584.5422\n",
      "Epoch 23/100\n",
      "379/379 [==============================] - 0s 109us/sample - loss: 571.8358 - val_loss: 583.6764\n",
      "Epoch 24/100\n",
      "379/379 [==============================] - 0s 108us/sample - loss: 571.0272 - val_loss: 582.7867\n",
      "Epoch 25/100\n",
      "379/379 [==============================] - 0s 108us/sample - loss: 570.1956 - val_loss: 581.8850\n",
      "Epoch 26/100\n",
      "379/379 [==============================] - 0s 110us/sample - loss: 569.3539 - val_loss: 580.9768\n",
      "Epoch 27/100\n",
      "379/379 [==============================] - 0s 108us/sample - loss: 568.5059 - val_loss: 580.0684\n",
      "Epoch 28/100\n",
      "379/379 [==============================] - 0s 113us/sample - loss: 567.6345 - val_loss: 579.1685\n",
      "Epoch 29/100\n",
      "379/379 [==============================] - 0s 111us/sample - loss: 566.8045 - val_loss: 578.2484\n",
      "Epoch 30/100\n",
      "379/379 [==============================] - 0s 107us/sample - loss: 565.9365 - val_loss: 577.3315\n",
      "Epoch 31/100\n",
      "379/379 [==============================] - 0s 113us/sample - loss: 565.0704 - val_loss: 576.4097\n",
      "Epoch 32/100\n",
      "379/379 [==============================] - 0s 108us/sample - loss: 564.2051 - val_loss: 575.4847\n",
      "Epoch 33/100\n",
      "379/379 [==============================] - 0s 110us/sample - loss: 563.3275 - val_loss: 574.5601\n",
      "Epoch 34/100\n",
      "379/379 [==============================] - 0s 104us/sample - loss: 562.4614 - val_loss: 573.6249\n",
      "Epoch 35/100\n",
      "379/379 [==============================] - 0s 105us/sample - loss: 561.5846 - val_loss: 572.6861\n",
      "Epoch 36/100\n",
      "379/379 [==============================] - 0s 111us/sample - loss: 560.6942 - val_loss: 571.7453\n",
      "Epoch 37/100\n",
      "379/379 [==============================] - 0s 112us/sample - loss: 559.8190 - val_loss: 570.7904\n",
      "Epoch 38/100\n",
      "379/379 [==============================] - 0s 111us/sample - loss: 558.9253 - val_loss: 569.8355\n",
      "Epoch 39/100\n",
      "379/379 [==============================] - 0s 108us/sample - loss: 558.0296 - val_loss: 568.8740\n",
      "Epoch 40/100\n",
      "379/379 [==============================] - 0s 108us/sample - loss: 557.1238 - val_loss: 567.9134\n",
      "Epoch 41/100\n",
      "379/379 [==============================] - 0s 108us/sample - loss: 556.2293 - val_loss: 566.9400\n",
      "Epoch 42/100\n",
      "379/379 [==============================] - 0s 107us/sample - loss: 555.2999 - val_loss: 565.9704\n",
      "Epoch 43/100\n",
      "379/379 [==============================] - 0s 111us/sample - loss: 554.3931 - val_loss: 564.9871\n",
      "Epoch 44/100\n",
      "379/379 [==============================] - 0s 108us/sample - loss: 553.4767 - val_loss: 563.9966\n",
      "Epoch 45/100\n",
      "379/379 [==============================] - 0s 106us/sample - loss: 552.5535 - val_loss: 562.9992\n",
      "Epoch 46/100\n",
      "379/379 [==============================] - 0s 111us/sample - loss: 551.6068 - val_loss: 562.0033\n",
      "Epoch 47/100\n",
      "379/379 [==============================] - 0s 108us/sample - loss: 550.6989 - val_loss: 560.9819\n",
      "Epoch 48/100\n",
      "379/379 [==============================] - 0s 105us/sample - loss: 549.7249 - val_loss: 559.9785\n",
      "Epoch 49/100\n",
      "379/379 [==============================] - 0s 108us/sample - loss: 548.7857 - val_loss: 558.9648\n",
      "Epoch 50/100\n",
      "379/379 [==============================] - 0s 113us/sample - loss: 547.8358 - val_loss: 557.9451\n",
      "Epoch 51/100\n",
      "379/379 [==============================] - 0s 108us/sample - loss: 546.8885 - val_loss: 556.9138\n",
      "Epoch 52/100\n",
      "379/379 [==============================] - 0s 104us/sample - loss: 545.9352 - val_loss: 555.8729\n",
      "Epoch 53/100\n",
      "379/379 [==============================] - 0s 109us/sample - loss: 544.9532 - val_loss: 554.8403\n",
      "Epoch 54/100\n",
      "379/379 [==============================] - 0s 109us/sample - loss: 543.9861 - val_loss: 553.7977\n",
      "Epoch 55/100\n",
      "379/379 [==============================] - 0s 106us/sample - loss: 543.0213 - val_loss: 552.7429\n",
      "Epoch 56/100\n",
      "379/379 [==============================] - 0s 108us/sample - loss: 542.0269 - val_loss: 551.6928\n",
      "Epoch 57/100\n",
      "379/379 [==============================] - 0s 107us/sample - loss: 541.0517 - val_loss: 550.6293\n",
      "Epoch 58/100\n",
      "379/379 [==============================] - 0s 107us/sample - loss: 540.0686 - val_loss: 549.5580\n",
      "Epoch 59/100\n",
      "379/379 [==============================] - 0s 106us/sample - loss: 539.0632 - val_loss: 548.4946\n",
      "Epoch 60/100\n",
      "379/379 [==============================] - 0s 106us/sample - loss: 538.0660 - val_loss: 547.4262\n",
      "Epoch 61/100\n",
      "379/379 [==============================] - 0s 105us/sample - loss: 537.0694 - val_loss: 546.3491\n",
      "Epoch 62/100\n",
      "379/379 [==============================] - 0s 106us/sample - loss: 536.0366 - val_loss: 545.2853\n",
      "Epoch 63/100\n",
      "379/379 [==============================] - 0s 106us/sample - loss: 535.0448 - val_loss: 544.2014\n",
      "Epoch 64/100\n",
      "379/379 [==============================] - 0s 108us/sample - loss: 534.0453 - val_loss: 543.1025\n",
      "Epoch 65/100\n",
      "379/379 [==============================] - 0s 113us/sample - loss: 532.9993 - val_loss: 542.0167\n",
      "Epoch 66/100\n",
      "379/379 [==============================] - 0s 107us/sample - loss: 531.9735 - val_loss: 540.9243\n",
      "Epoch 67/100\n",
      "379/379 [==============================] - 0s 107us/sample - loss: 530.9591 - val_loss: 539.8154\n",
      "Epoch 68/100\n",
      "379/379 [==============================] - 0s 105us/sample - loss: 529.9247 - val_loss: 538.7022\n",
      "Epoch 69/100\n",
      "379/379 [==============================] - 0s 110us/sample - loss: 528.8700 - val_loss: 537.5928\n",
      "Epoch 70/100\n",
      "379/379 [==============================] - 0s 106us/sample - loss: 527.8382 - val_loss: 536.4714\n",
      "Epoch 71/100\n",
      "379/379 [==============================] - 0s 104us/sample - loss: 526.7582 - val_loss: 535.3634\n",
      "Epoch 72/100\n",
      "379/379 [==============================] - 0s 112us/sample - loss: 525.7408 - val_loss: 534.2189\n",
      "Epoch 73/100\n",
      "379/379 [==============================] - 0s 104us/sample - loss: 524.6624 - val_loss: 533.0727\n",
      "Epoch 74/100\n",
      "379/379 [==============================] - 0s 109us/sample - loss: 523.5731 - val_loss: 531.9304\n",
      "Epoch 75/100\n",
      "379/379 [==============================] - 0s 107us/sample - loss: 522.4820 - val_loss: 530.7814\n",
      "Epoch 76/100\n",
      "379/379 [==============================] - 0s 108us/sample - loss: 521.4071 - val_loss: 529.6109\n",
      "Epoch 77/100\n",
      "379/379 [==============================] - 0s 107us/sample - loss: 520.3341 - val_loss: 528.4195\n",
      "Epoch 78/100\n",
      "379/379 [==============================] - 0s 107us/sample - loss: 519.2013 - val_loss: 527.2348\n",
      "Epoch 79/100\n",
      "379/379 [==============================] - 0s 106us/sample - loss: 518.0735 - val_loss: 526.0507\n",
      "Epoch 80/100\n",
      "379/379 [==============================] - 0s 107us/sample - loss: 516.9756 - val_loss: 524.8403\n",
      "Epoch 81/100\n",
      "379/379 [==============================] - 0s 105us/sample - loss: 515.8486 - val_loss: 523.6201\n",
      "Epoch 82/100\n",
      "379/379 [==============================] - 0s 106us/sample - loss: 514.7141 - val_loss: 522.3963\n",
      "Epoch 83/100\n",
      "379/379 [==============================] - 0s 109us/sample - loss: 513.5773 - val_loss: 521.1643\n",
      "Epoch 84/100\n",
      "379/379 [==============================] - 0s 104us/sample - loss: 512.4227 - val_loss: 519.9319\n",
      "Epoch 85/100\n",
      "379/379 [==============================] - 0s 106us/sample - loss: 511.2746 - val_loss: 518.6849\n",
      "Epoch 86/100\n",
      "379/379 [==============================] - 0s 108us/sample - loss: 510.1147 - val_loss: 517.4350\n",
      "Epoch 87/100\n",
      "379/379 [==============================] - 0s 109us/sample - loss: 508.9508 - val_loss: 516.1774\n",
      "Epoch 88/100\n",
      "379/379 [==============================] - 0s 109us/sample - loss: 507.7639 - val_loss: 514.9215\n",
      "Epoch 89/100\n",
      "379/379 [==============================] - 0s 107us/sample - loss: 506.6017 - val_loss: 513.6458\n",
      "Epoch 90/100\n",
      "379/379 [==============================] - 0s 108us/sample - loss: 505.4001 - val_loss: 512.3739\n",
      "Epoch 91/100\n",
      "379/379 [==============================] - 0s 109us/sample - loss: 504.2113 - val_loss: 511.0893\n",
      "Epoch 92/100\n",
      "379/379 [==============================] - 0s 113us/sample - loss: 503.0110 - val_loss: 509.7990\n",
      "Epoch 93/100\n",
      "379/379 [==============================] - 0s 106us/sample - loss: 501.8017 - val_loss: 508.5027\n",
      "Epoch 94/100\n",
      "379/379 [==============================] - 0s 109us/sample - loss: 500.5681 - val_loss: 507.2201\n",
      "Epoch 95/100\n",
      "379/379 [==============================] - 0s 107us/sample - loss: 499.4089 - val_loss: 505.8949\n",
      "Epoch 96/100\n",
      "379/379 [==============================] - 0s 112us/sample - loss: 498.1518 - val_loss: 504.5900\n",
      "Epoch 97/100\n",
      "379/379 [==============================] - 0s 109us/sample - loss: 496.9334 - val_loss: 503.2785\n",
      "Epoch 98/100\n",
      "379/379 [==============================] - 0s 109us/sample - loss: 495.7431 - val_loss: 501.9431\n",
      "Epoch 99/100\n",
      "379/379 [==============================] - 0s 113us/sample - loss: 494.5021 - val_loss: 500.6169\n",
      "Epoch 100/100\n",
      "379/379 [==============================] - 0s 106us/sample - loss: 493.2478 - val_loss: 499.3014\n",
      "\n",
      "Forming the grid-search model #14 using: optimizer=<tensorflow.python.keras.optimizer_v2.adam.Adam object at 0x7f9132bd5290>, kernel=<tensorflow.python.ops.init_ops_v2.Orthogonal object at 0x7f9132bbb090>, epochs=100, batch_size=32\n",
      "Train on 379 samples, validate on 127 samples\n",
      "Epoch 1/100\n",
      "379/379 [==============================] - 1s 2ms/sample - loss: 585.6189 - val_loss: 593.5231\n",
      "Epoch 2/100\n",
      "379/379 [==============================] - 0s 180us/sample - loss: 580.1583 - val_loss: 587.6427\n",
      "Epoch 3/100\n",
      "379/379 [==============================] - 0s 174us/sample - loss: 574.8742 - val_loss: 582.5543\n",
      "Epoch 4/100\n",
      "379/379 [==============================] - 0s 168us/sample - loss: 570.3150 - val_loss: 577.9682\n",
      "Epoch 5/100\n",
      "379/379 [==============================] - 0s 165us/sample - loss: 566.2808 - val_loss: 573.7386\n",
      "Epoch 6/100\n",
      "379/379 [==============================] - 0s 164us/sample - loss: 562.4845 - val_loss: 569.9080\n",
      "Epoch 7/100\n",
      "379/379 [==============================] - 0s 166us/sample - loss: 558.9844 - val_loss: 566.2191\n",
      "Epoch 8/100\n",
      "379/379 [==============================] - 0s 165us/sample - loss: 555.6718 - val_loss: 562.6760\n",
      "Epoch 9/100\n",
      "379/379 [==============================] - 0s 167us/sample - loss: 552.4507 - val_loss: 559.2734\n",
      "Epoch 10/100\n",
      "379/379 [==============================] - 0s 166us/sample - loss: 549.3757 - val_loss: 555.9090\n",
      "Epoch 11/100\n",
      "379/379 [==============================] - 0s 163us/sample - loss: 546.3259 - val_loss: 552.6791\n",
      "Epoch 12/100\n",
      "379/379 [==============================] - 0s 168us/sample - loss: 543.3653 - val_loss: 549.4921\n",
      "Epoch 13/100\n",
      "379/379 [==============================] - 0s 177us/sample - loss: 540.4390 - val_loss: 546.3824\n",
      "Epoch 14/100\n",
      "379/379 [==============================] - 0s 166us/sample - loss: 537.6261 - val_loss: 543.2930\n",
      "Epoch 15/100\n",
      "379/379 [==============================] - 0s 166us/sample - loss: 534.8036 - val_loss: 540.2784\n",
      "Epoch 16/100\n",
      "379/379 [==============================] - 0s 166us/sample - loss: 532.0828 - val_loss: 537.2907\n",
      "Epoch 17/100\n",
      "379/379 [==============================] - 0s 165us/sample - loss: 529.3621 - val_loss: 534.3732\n",
      "Epoch 18/100\n",
      "379/379 [==============================] - 0s 162us/sample - loss: 526.6993 - val_loss: 531.4869\n",
      "Epoch 19/100\n",
      "379/379 [==============================] - 0s 168us/sample - loss: 524.0918 - val_loss: 528.6192\n",
      "Epoch 20/100\n",
      "379/379 [==============================] - 0s 166us/sample - loss: 521.5346 - val_loss: 525.7320\n",
      "Epoch 21/100\n",
      "379/379 [==============================] - 0s 164us/sample - loss: 518.9535 - val_loss: 522.9166\n",
      "Epoch 22/100\n",
      "379/379 [==============================] - 0s 166us/sample - loss: 516.4494 - val_loss: 520.1161\n",
      "Epoch 23/100\n",
      "379/379 [==============================] - 0s 163us/sample - loss: 513.8261 - val_loss: 517.4891\n",
      "Epoch 24/100\n",
      "379/379 [==============================] - 0s 164us/sample - loss: 511.4208 - val_loss: 514.7482\n",
      "Epoch 25/100\n",
      "379/379 [==============================] - 0s 187us/sample - loss: 508.9573 - val_loss: 512.0016\n",
      "Epoch 26/100\n",
      "379/379 [==============================] - 0s 161us/sample - loss: 506.5057 - val_loss: 509.2588\n",
      "Epoch 27/100\n",
      "379/379 [==============================] - 0s 170us/sample - loss: 504.0549 - val_loss: 506.5704\n",
      "Epoch 28/100\n",
      "379/379 [==============================] - 0s 169us/sample - loss: 501.5870 - val_loss: 503.9582\n",
      "Epoch 29/100\n",
      "379/379 [==============================] - 0s 163us/sample - loss: 499.2405 - val_loss: 501.2820\n",
      "Epoch 30/100\n",
      "379/379 [==============================] - 0s 166us/sample - loss: 496.8127 - val_loss: 498.6513\n",
      "Epoch 31/100\n",
      "379/379 [==============================] - 0s 172us/sample - loss: 494.4410 - val_loss: 495.9844\n",
      "Epoch 32/100\n",
      "379/379 [==============================] - 0s 167us/sample - loss: 492.0651 - val_loss: 493.3390\n",
      "Epoch 33/100\n",
      "379/379 [==============================] - 0s 165us/sample - loss: 489.6622 - val_loss: 490.7440\n",
      "Epoch 34/100\n",
      "379/379 [==============================] - 0s 167us/sample - loss: 487.3364 - val_loss: 488.1085\n",
      "Epoch 35/100\n",
      "379/379 [==============================] - 0s 166us/sample - loss: 484.9644 - val_loss: 485.5100\n",
      "Epoch 36/100\n",
      "379/379 [==============================] - 0s 172us/sample - loss: 482.6007 - val_loss: 482.9244\n",
      "Epoch 37/100\n",
      "379/379 [==============================] - 0s 162us/sample - loss: 480.2907 - val_loss: 480.3000\n",
      "Epoch 38/100\n",
      "379/379 [==============================] - 0s 167us/sample - loss: 477.9252 - val_loss: 477.7250\n",
      "Epoch 39/100\n",
      "379/379 [==============================] - 0s 164us/sample - loss: 475.6007 - val_loss: 475.1264\n",
      "Epoch 40/100\n",
      "379/379 [==============================] - 0s 163us/sample - loss: 473.2406 - val_loss: 472.5856\n",
      "Epoch 41/100\n",
      "379/379 [==============================] - 0s 166us/sample - loss: 470.9628 - val_loss: 470.0070\n",
      "Epoch 42/100\n",
      "379/379 [==============================] - 0s 167us/sample - loss: 468.6168 - val_loss: 467.4967\n",
      "Epoch 43/100\n",
      "379/379 [==============================] - 0s 174us/sample - loss: 466.3354 - val_loss: 464.9490\n",
      "Epoch 44/100\n",
      "379/379 [==============================] - 0s 165us/sample - loss: 464.0517 - val_loss: 462.3945\n",
      "Epoch 45/100\n",
      "379/379 [==============================] - 0s 169us/sample - loss: 461.7670 - val_loss: 459.8347\n",
      "Epoch 46/100\n",
      "379/379 [==============================] - 0s 166us/sample - loss: 459.4547 - val_loss: 457.3329\n",
      "Epoch 47/100\n",
      "379/379 [==============================] - 0s 167us/sample - loss: 457.2633 - val_loss: 454.7141\n",
      "Epoch 48/100\n",
      "379/379 [==============================] - 0s 163us/sample - loss: 454.9025 - val_loss: 452.2578\n",
      "Epoch 49/100\n",
      "379/379 [==============================] - 0s 166us/sample - loss: 452.6806 - val_loss: 449.7666\n",
      "Epoch 50/100\n",
      "379/379 [==============================] - 0s 166us/sample - loss: 450.4373 - val_loss: 447.2882\n",
      "Epoch 51/100\n",
      "379/379 [==============================] - 0s 165us/sample - loss: 448.2212 - val_loss: 444.7828\n",
      "Epoch 52/100\n",
      "379/379 [==============================] - 0s 165us/sample - loss: 446.0042 - val_loss: 442.2667\n",
      "Epoch 53/100\n",
      "379/379 [==============================] - 0s 167us/sample - loss: 443.7440 - val_loss: 439.8230\n",
      "Epoch 54/100\n",
      "379/379 [==============================] - 0s 165us/sample - loss: 441.5228 - val_loss: 437.3882\n",
      "Epoch 55/100\n",
      "379/379 [==============================] - 0s 166us/sample - loss: 439.3678 - val_loss: 434.8887\n",
      "Epoch 56/100\n",
      "379/379 [==============================] - 0s 170us/sample - loss: 437.1246 - val_loss: 432.4699\n",
      "Epoch 57/100\n",
      "379/379 [==============================] - 0s 165us/sample - loss: 434.9644 - val_loss: 430.0212\n",
      "Epoch 58/100\n",
      "379/379 [==============================] - 0s 168us/sample - loss: 432.8168 - val_loss: 427.5561\n",
      "Epoch 59/100\n",
      "379/379 [==============================] - 0s 167us/sample - loss: 430.6008 - val_loss: 425.1675\n",
      "Epoch 60/100\n",
      "379/379 [==============================] - 0s 167us/sample - loss: 428.4346 - val_loss: 422.7828\n",
      "Epoch 61/100\n",
      "379/379 [==============================] - 0s 171us/sample - loss: 426.3043 - val_loss: 420.3578\n",
      "Epoch 62/100\n",
      "379/379 [==============================] - 0s 171us/sample - loss: 424.0961 - val_loss: 418.0168\n",
      "Epoch 63/100\n",
      "379/379 [==============================] - 0s 170us/sample - loss: 421.9801 - val_loss: 415.6268\n",
      "Epoch 64/100\n",
      "379/379 [==============================] - 0s 167us/sample - loss: 419.8627 - val_loss: 413.2179\n",
      "Epoch 65/100\n",
      "379/379 [==============================] - 0s 164us/sample - loss: 417.6796 - val_loss: 410.8807\n",
      "Epoch 66/100\n",
      "379/379 [==============================] - 0s 167us/sample - loss: 415.5552 - val_loss: 408.5396\n",
      "Epoch 67/100\n",
      "379/379 [==============================] - 0s 162us/sample - loss: 413.4738 - val_loss: 406.1457\n",
      "Epoch 68/100\n",
      "379/379 [==============================] - 0s 165us/sample - loss: 411.3276 - val_loss: 403.7910\n",
      "Epoch 69/100\n",
      "379/379 [==============================] - 0s 166us/sample - loss: 409.1977 - val_loss: 401.4595\n",
      "Epoch 70/100\n",
      "379/379 [==============================] - 0s 164us/sample - loss: 407.1137 - val_loss: 399.1035\n",
      "Epoch 71/100\n",
      "379/379 [==============================] - 0s 165us/sample - loss: 404.9540 - val_loss: 396.8470\n",
      "Epoch 72/100\n",
      "379/379 [==============================] - 0s 165us/sample - loss: 402.9456 - val_loss: 394.4703\n",
      "Epoch 73/100\n",
      "379/379 [==============================] - 0s 167us/sample - loss: 400.8232 - val_loss: 392.1556\n",
      "Epoch 74/100\n",
      "379/379 [==============================] - 0s 163us/sample - loss: 398.7112 - val_loss: 389.8978\n",
      "Epoch 75/100\n",
      "379/379 [==============================] - 0s 209us/sample - loss: 396.6534 - val_loss: 387.6182\n",
      "Epoch 76/100\n",
      "379/379 [==============================] - 0s 168us/sample - loss: 394.6048 - val_loss: 385.3205\n",
      "Epoch 77/100\n",
      "379/379 [==============================] - 0s 164us/sample - loss: 392.5779 - val_loss: 383.0099\n",
      "Epoch 78/100\n",
      "379/379 [==============================] - 0s 163us/sample - loss: 390.4842 - val_loss: 380.7560\n",
      "Epoch 79/100\n",
      "379/379 [==============================] - 0s 167us/sample - loss: 388.4198 - val_loss: 378.5309\n",
      "Epoch 80/100\n",
      "379/379 [==============================] - 0s 163us/sample - loss: 386.4092 - val_loss: 376.2533\n",
      "Epoch 81/100\n",
      "379/379 [==============================] - 0s 163us/sample - loss: 384.3744 - val_loss: 373.9836\n",
      "Epoch 82/100\n",
      "379/379 [==============================] - 0s 167us/sample - loss: 382.3423 - val_loss: 371.7341\n",
      "Epoch 83/100\n",
      "379/379 [==============================] - 0s 163us/sample - loss: 380.3209 - val_loss: 369.4965\n",
      "Epoch 84/100\n",
      "379/379 [==============================] - 0s 163us/sample - loss: 378.2959 - val_loss: 367.2847\n",
      "Epoch 85/100\n",
      "379/379 [==============================] - 0s 161us/sample - loss: 376.3132 - val_loss: 365.0393\n",
      "Epoch 86/100\n",
      "379/379 [==============================] - 0s 164us/sample - loss: 374.2878 - val_loss: 362.8446\n",
      "Epoch 87/100\n",
      "379/379 [==============================] - 0s 165us/sample - loss: 372.3168 - val_loss: 360.6263\n",
      "Epoch 88/100\n",
      "379/379 [==============================] - 0s 165us/sample - loss: 370.2938 - val_loss: 358.4605\n",
      "Epoch 89/100\n",
      "379/379 [==============================] - 0s 163us/sample - loss: 368.3438 - val_loss: 356.2437\n",
      "Epoch 90/100\n",
      "379/379 [==============================] - 0s 164us/sample - loss: 366.3365 - val_loss: 354.0830\n",
      "Epoch 91/100\n",
      "379/379 [==============================] - 0s 161us/sample - loss: 364.3797 - val_loss: 351.9058\n",
      "Epoch 92/100\n",
      "379/379 [==============================] - 0s 167us/sample - loss: 362.4079 - val_loss: 349.7425\n",
      "Epoch 93/100\n",
      "379/379 [==============================] - 0s 171us/sample - loss: 360.4416 - val_loss: 347.5942\n",
      "Epoch 94/100\n",
      "379/379 [==============================] - 0s 165us/sample - loss: 358.4718 - val_loss: 345.4832\n",
      "Epoch 95/100\n",
      "379/379 [==============================] - 0s 166us/sample - loss: 356.6033 - val_loss: 343.2733\n",
      "Epoch 96/100\n",
      "379/379 [==============================] - 0s 166us/sample - loss: 354.5852 - val_loss: 341.1743\n",
      "Epoch 97/100\n",
      "379/379 [==============================] - 0s 166us/sample - loss: 352.6861 - val_loss: 339.0360\n",
      "Epoch 98/100\n",
      "379/379 [==============================] - 0s 167us/sample - loss: 350.7859 - val_loss: 336.8821\n",
      "Epoch 99/100\n",
      "379/379 [==============================] - 0s 165us/sample - loss: 348.8622 - val_loss: 334.7621\n",
      "Epoch 100/100\n",
      "379/379 [==============================] - 0s 174us/sample - loss: 346.9199 - val_loss: 332.6947\n",
      "\n",
      "Forming the grid-search model #15 using: optimizer=<tensorflow.python.keras.optimizer_v2.adam.Adam object at 0x7f9132bd5290>, kernel=<tensorflow.python.ops.init_ops_v2.Orthogonal object at 0x7f9132bbb090>, epochs=100, batch_size=64\n",
      "Train on 379 samples, validate on 127 samples\n",
      "Epoch 1/100\n",
      "379/379 [==============================] - 1s 2ms/sample - loss: 586.7806 - val_loss: 596.2581\n",
      "Epoch 2/100\n",
      "379/379 [==============================] - 0s 107us/sample - loss: 583.9802 - val_loss: 592.9242\n",
      "Epoch 3/100\n",
      "379/379 [==============================] - 0s 106us/sample - loss: 580.7899 - val_loss: 589.6243\n",
      "Epoch 4/100\n",
      "379/379 [==============================] - 0s 111us/sample - loss: 577.7302 - val_loss: 586.4852\n",
      "Epoch 5/100\n",
      "379/379 [==============================] - 0s 107us/sample - loss: 574.8819 - val_loss: 583.5191\n",
      "Epoch 6/100\n",
      "379/379 [==============================] - 0s 133us/sample - loss: 572.1325 - val_loss: 580.7651\n",
      "Epoch 7/100\n",
      "379/379 [==============================] - 0s 112us/sample - loss: 569.5864 - val_loss: 578.1430\n",
      "Epoch 8/100\n",
      "379/379 [==============================] - 0s 111us/sample - loss: 567.2228 - val_loss: 575.6517\n",
      "Epoch 9/100\n",
      "379/379 [==============================] - 0s 107us/sample - loss: 564.9448 - val_loss: 573.2861\n",
      "Epoch 10/100\n",
      "379/379 [==============================] - 0s 105us/sample - loss: 562.7989 - val_loss: 570.9784\n",
      "Epoch 11/100\n",
      "379/379 [==============================] - 0s 112us/sample - loss: 560.6993 - val_loss: 568.7658\n",
      "Epoch 12/100\n",
      "379/379 [==============================] - 0s 105us/sample - loss: 558.6880 - val_loss: 566.6094\n",
      "Epoch 13/100\n",
      "379/379 [==============================] - 0s 112us/sample - loss: 556.6996 - val_loss: 564.5251\n",
      "Epoch 14/100\n",
      "379/379 [==============================] - 0s 107us/sample - loss: 554.8088 - val_loss: 562.4673\n",
      "Epoch 15/100\n",
      "379/379 [==============================] - 0s 108us/sample - loss: 552.9069 - val_loss: 560.4574\n",
      "Epoch 16/100\n",
      "379/379 [==============================] - 0s 107us/sample - loss: 551.0823 - val_loss: 558.4714\n",
      "Epoch 17/100\n",
      "379/379 [==============================] - 0s 104us/sample - loss: 549.2511 - val_loss: 556.5342\n",
      "Epoch 18/100\n",
      "379/379 [==============================] - 0s 110us/sample - loss: 547.4541 - val_loss: 554.6283\n",
      "Epoch 19/100\n",
      "379/379 [==============================] - 0s 107us/sample - loss: 545.7130 - val_loss: 552.7329\n",
      "Epoch 20/100\n",
      "379/379 [==============================] - 0s 106us/sample - loss: 543.9945 - val_loss: 550.8437\n",
      "Epoch 21/100\n",
      "379/379 [==============================] - 0s 110us/sample - loss: 542.2753 - val_loss: 548.9839\n",
      "Epoch 22/100\n",
      "379/379 [==============================] - 0s 106us/sample - loss: 540.5977 - val_loss: 547.1407\n",
      "Epoch 23/100\n",
      "379/379 [==============================] - 0s 111us/sample - loss: 538.8319 - val_loss: 545.3782\n",
      "Epoch 24/100\n",
      "379/379 [==============================] - 0s 108us/sample - loss: 537.2322 - val_loss: 543.5978\n",
      "Epoch 25/100\n",
      "379/379 [==============================] - 0s 106us/sample - loss: 535.6250 - val_loss: 541.8201\n",
      "Epoch 26/100\n",
      "379/379 [==============================] - 0s 110us/sample - loss: 534.0229 - val_loss: 540.0570\n",
      "Epoch 27/100\n",
      "379/379 [==============================] - 0s 108us/sample - loss: 532.4270 - val_loss: 538.3189\n",
      "Epoch 28/100\n",
      "379/379 [==============================] - 0s 111us/sample - loss: 530.8115 - val_loss: 536.6238\n",
      "Epoch 29/100\n",
      "379/379 [==============================] - 0s 105us/sample - loss: 529.2952 - val_loss: 534.9101\n",
      "Epoch 30/100\n",
      "379/379 [==============================] - 0s 107us/sample - loss: 527.7334 - val_loss: 533.2216\n",
      "Epoch 31/100\n",
      "379/379 [==============================] - 0s 106us/sample - loss: 526.2013 - val_loss: 531.5403\n",
      "Epoch 32/100\n",
      "379/379 [==============================] - 0s 110us/sample - loss: 524.6845 - val_loss: 529.8764\n",
      "Epoch 33/100\n",
      "379/379 [==============================] - 0s 98us/sample - loss: 523.1527 - val_loss: 528.2377\n",
      "Epoch 34/100\n",
      "379/379 [==============================] - 0s 105us/sample - loss: 521.6813 - val_loss: 526.5954\n",
      "Epoch 35/100\n",
      "379/379 [==============================] - 0s 102us/sample - loss: 520.1890 - val_loss: 524.9687\n",
      "Epoch 36/100\n",
      "379/379 [==============================] - 0s 104us/sample - loss: 518.7011 - val_loss: 523.3535\n",
      "Epoch 37/100\n",
      "379/379 [==============================] - 0s 99us/sample - loss: 517.2611 - val_loss: 521.7270\n",
      "Epoch 38/100\n",
      "379/379 [==============================] - 0s 108us/sample - loss: 515.7835 - val_loss: 520.1275\n",
      "Epoch 39/100\n",
      "379/379 [==============================] - 0s 106us/sample - loss: 514.3347 - val_loss: 518.5271\n",
      "Epoch 40/100\n",
      "379/379 [==============================] - 0s 107us/sample - loss: 512.8698 - val_loss: 516.9487\n",
      "Epoch 41/100\n",
      "379/379 [==============================] - 0s 106us/sample - loss: 511.4468 - val_loss: 515.3644\n",
      "Epoch 42/100\n",
      "379/379 [==============================] - 0s 107us/sample - loss: 509.9884 - val_loss: 513.8026\n",
      "Epoch 43/100\n",
      "379/379 [==============================] - 0s 105us/sample - loss: 508.5663 - val_loss: 512.2357\n",
      "Epoch 44/100\n",
      "379/379 [==============================] - 0s 108us/sample - loss: 507.1548 - val_loss: 510.6677\n",
      "Epoch 45/100\n",
      "379/379 [==============================] - 0s 113us/sample - loss: 505.7415 - val_loss: 509.1071\n",
      "Epoch 46/100\n",
      "379/379 [==============================] - 0s 104us/sample - loss: 504.3114 - val_loss: 507.5653\n",
      "Epoch 47/100\n",
      "379/379 [==============================] - 0s 107us/sample - loss: 502.9634 - val_loss: 505.9909\n",
      "Epoch 48/100\n",
      "379/379 [==============================] - 0s 107us/sample - loss: 501.5018 - val_loss: 504.4667\n",
      "Epoch 49/100\n",
      "379/379 [==============================] - 0s 108us/sample - loss: 500.1223 - val_loss: 502.9326\n",
      "Epoch 50/100\n",
      "379/379 [==============================] - 0s 108us/sample - loss: 498.7345 - val_loss: 501.3985\n",
      "Epoch 51/100\n",
      "379/379 [==============================] - 0s 115us/sample - loss: 497.3725 - val_loss: 499.8523\n",
      "Epoch 52/100\n",
      "379/379 [==============================] - 0s 106us/sample - loss: 495.9986 - val_loss: 498.3073\n",
      "Epoch 53/100\n",
      "379/379 [==============================] - 0s 111us/sample - loss: 494.5936 - val_loss: 496.7921\n",
      "Epoch 54/100\n",
      "379/379 [==============================] - 0s 107us/sample - loss: 493.2240 - val_loss: 495.2744\n",
      "Epoch 55/100\n",
      "379/379 [==============================] - 0s 127us/sample - loss: 491.8851 - val_loss: 493.7433\n",
      "Epoch 56/100\n",
      "379/379 [==============================] - 0s 108us/sample - loss: 490.4912 - val_loss: 492.2361\n",
      "Epoch 57/100\n",
      "379/379 [==============================] - 0s 110us/sample - loss: 489.1466 - val_loss: 490.7149\n",
      "Epoch 58/100\n",
      "379/379 [==============================] - 0s 110us/sample - loss: 487.8004 - val_loss: 489.1862\n",
      "Epoch 59/100\n",
      "379/379 [==============================] - 0s 106us/sample - loss: 486.4108 - val_loss: 487.6872\n",
      "Epoch 60/100\n",
      "379/379 [==============================] - 0s 108us/sample - loss: 485.0604 - val_loss: 486.1891\n",
      "Epoch 61/100\n",
      "379/379 [==============================] - 0s 105us/sample - loss: 483.7261 - val_loss: 484.6825\n",
      "Epoch 62/100\n",
      "379/379 [==============================] - 0s 109us/sample - loss: 482.3344 - val_loss: 483.2152\n",
      "Epoch 63/100\n",
      "379/379 [==============================] - 0s 108us/sample - loss: 481.0180 - val_loss: 481.7287\n",
      "Epoch 64/100\n",
      "379/379 [==============================] - 0s 104us/sample - loss: 479.7036 - val_loss: 480.2298\n",
      "Epoch 65/100\n",
      "379/379 [==============================] - 0s 103us/sample - loss: 478.3294 - val_loss: 478.7609\n",
      "Epoch 66/100\n",
      "379/379 [==============================] - 0s 106us/sample - loss: 477.0007 - val_loss: 477.2921\n",
      "Epoch 67/100\n",
      "379/379 [==============================] - 0s 107us/sample - loss: 475.7053 - val_loss: 475.8062\n",
      "Epoch 68/100\n",
      "379/379 [==============================] - 0s 108us/sample - loss: 474.3651 - val_loss: 474.3384\n",
      "Epoch 69/100\n",
      "379/379 [==============================] - 0s 107us/sample - loss: 473.0371 - val_loss: 472.8821\n",
      "Epoch 70/100\n",
      "379/379 [==============================] - 0s 107us/sample - loss: 471.7372 - val_loss: 471.4156\n",
      "Epoch 71/100\n",
      "379/379 [==============================] - 0s 105us/sample - loss: 470.3824 - val_loss: 469.9799\n",
      "Epoch 72/100\n",
      "379/379 [==============================] - 0s 107us/sample - loss: 469.1365 - val_loss: 468.5008\n",
      "Epoch 73/100\n",
      "379/379 [==============================] - 0s 108us/sample - loss: 467.8131 - val_loss: 467.0357\n",
      "Epoch 74/100\n",
      "379/379 [==============================] - 0s 109us/sample - loss: 466.4797 - val_loss: 465.6001\n",
      "Epoch 75/100\n",
      "379/379 [==============================] - 0s 105us/sample - loss: 465.1837 - val_loss: 464.1631\n",
      "Epoch 76/100\n",
      "379/379 [==============================] - 0s 104us/sample - loss: 463.9062 - val_loss: 462.7181\n",
      "Epoch 77/100\n",
      "379/379 [==============================] - 0s 107us/sample - loss: 462.6429 - val_loss: 461.2620\n",
      "Epoch 78/100\n",
      "379/379 [==============================] - 0s 109us/sample - loss: 461.3273 - val_loss: 459.8278\n",
      "Epoch 79/100\n",
      "379/379 [==============================] - 0s 109us/sample - loss: 460.0223 - val_loss: 458.4126\n",
      "Epoch 80/100\n",
      "379/379 [==============================] - 0s 108us/sample - loss: 458.7718 - val_loss: 456.9778\n",
      "Epoch 81/100\n",
      "379/379 [==============================] - 0s 114us/sample - loss: 457.4974 - val_loss: 455.5437\n",
      "Epoch 82/100\n",
      "379/379 [==============================] - 0s 107us/sample - loss: 456.2190 - val_loss: 454.1206\n",
      "Epoch 83/100\n",
      "379/379 [==============================] - 0s 110us/sample - loss: 454.9507 - val_loss: 452.7011\n",
      "Epoch 84/100\n",
      "379/379 [==============================] - 0s 109us/sample - loss: 453.6725 - val_loss: 451.2945\n",
      "Epoch 85/100\n",
      "379/379 [==============================] - 0s 110us/sample - loss: 452.4247 - val_loss: 449.8761\n",
      "Epoch 86/100\n",
      "379/379 [==============================] - 0s 107us/sample - loss: 451.1524 - val_loss: 448.4727\n",
      "Epoch 87/100\n",
      "379/379 [==============================] - 0s 106us/sample - loss: 449.9065 - val_loss: 447.0651\n",
      "Epoch 88/100\n",
      "379/379 [==============================] - 0s 105us/sample - loss: 448.6302 - val_loss: 445.6765\n",
      "Epoch 89/100\n",
      "379/379 [==============================] - 0s 108us/sample - loss: 447.3998 - val_loss: 444.2754\n",
      "Epoch 90/100\n",
      "379/379 [==============================] - 0s 107us/sample - loss: 446.1331 - val_loss: 442.8922\n",
      "Epoch 91/100\n",
      "379/379 [==============================] - 0s 109us/sample - loss: 444.8997 - val_loss: 441.5027\n",
      "Epoch 92/100\n",
      "379/379 [==============================] - 0s 105us/sample - loss: 443.6542 - val_loss: 440.1182\n",
      "Epoch 93/100\n",
      "379/379 [==============================] - 0s 110us/sample - loss: 442.4095 - val_loss: 438.7378\n",
      "Epoch 94/100\n",
      "379/379 [==============================] - 0s 101us/sample - loss: 441.1440 - val_loss: 437.3846\n",
      "Epoch 95/100\n",
      "379/379 [==============================] - 0s 109us/sample - loss: 439.9747 - val_loss: 435.9888\n",
      "Epoch 96/100\n",
      "379/379 [==============================] - 0s 103us/sample - loss: 438.6958 - val_loss: 434.6283\n",
      "Epoch 97/100\n",
      "379/379 [==============================] - 0s 110us/sample - loss: 437.4804 - val_loss: 433.2638\n",
      "Epoch 98/100\n",
      "379/379 [==============================] - 0s 105us/sample - loss: 436.2882 - val_loss: 431.8813\n",
      "Epoch 99/100\n",
      "379/379 [==============================] - 0s 107us/sample - loss: 435.0549 - val_loss: 430.5137\n",
      "Epoch 100/100\n",
      "379/379 [==============================] - 0s 125us/sample - loss: 433.8087 - val_loss: 429.1671\n",
      "\n",
      "Forming the grid-search model #16 using: optimizer=<tensorflow.python.keras.optimizer_v2.adam.Adam object at 0x7f9132bd5290>, kernel=<tensorflow.python.ops.init_ops_v2.GlorotNormal object at 0x7f9132ed1050>, epochs=100, batch_size=32\n",
      "Train on 379 samples, validate on 127 samples\n",
      "Epoch 1/100\n",
      "379/379 [==============================] - 1s 2ms/sample - loss: 567.7871 - val_loss: 584.7594\n",
      "Epoch 2/100\n",
      "379/379 [==============================] - 0s 169us/sample - loss: 560.6365 - val_loss: 577.0701\n",
      "Epoch 3/100\n",
      "379/379 [==============================] - 0s 166us/sample - loss: 553.6305 - val_loss: 570.5641\n",
      "Epoch 4/100\n",
      "379/379 [==============================] - 0s 163us/sample - loss: 547.6560 - val_loss: 564.7229\n",
      "Epoch 5/100\n",
      "379/379 [==============================] - 0s 167us/sample - loss: 542.4240 - val_loss: 559.3326\n",
      "Epoch 6/100\n",
      "379/379 [==============================] - 0s 166us/sample - loss: 537.5196 - val_loss: 554.5206\n",
      "Epoch 7/100\n",
      "379/379 [==============================] - 0s 166us/sample - loss: 532.9885 - val_loss: 549.9482\n",
      "Epoch 8/100\n",
      "379/379 [==============================] - 0s 175us/sample - loss: 528.7615 - val_loss: 545.5643\n",
      "Epoch 9/100\n",
      "379/379 [==============================] - 0s 168us/sample - loss: 524.6916 - val_loss: 541.3858\n",
      "Epoch 10/100\n",
      "379/379 [==============================] - 0s 173us/sample - loss: 520.8335 - val_loss: 537.2872\n",
      "Epoch 11/100\n",
      "379/379 [==============================] - 0s 165us/sample - loss: 517.0463 - val_loss: 533.3632\n",
      "Epoch 12/100\n",
      "379/379 [==============================] - 0s 166us/sample - loss: 513.3995 - val_loss: 529.5262\n",
      "Epoch 13/100\n",
      "379/379 [==============================] - 0s 162us/sample - loss: 509.7977 - val_loss: 525.8352\n",
      "Epoch 14/100\n",
      "379/379 [==============================] - 0s 163us/sample - loss: 506.3785 - val_loss: 522.1584\n",
      "Epoch 15/100\n",
      "379/379 [==============================] - 0s 167us/sample - loss: 502.9181 - val_loss: 518.5888\n",
      "Epoch 16/100\n",
      "379/379 [==============================] - 0s 169us/sample - loss: 499.6035 - val_loss: 515.0310\n",
      "Epoch 17/100\n",
      "379/379 [==============================] - 0s 165us/sample - loss: 496.2798 - val_loss: 511.5752\n",
      "Epoch 18/100\n",
      "379/379 [==============================] - 0s 168us/sample - loss: 493.0179 - val_loss: 508.1593\n",
      "Epoch 19/100\n",
      "379/379 [==============================] - 0s 163us/sample - loss: 489.8534 - val_loss: 504.7543\n",
      "Epoch 20/100\n",
      "379/379 [==============================] - 0s 165us/sample - loss: 486.7184 - val_loss: 501.3256\n",
      "Epoch 21/100\n",
      "379/379 [==============================] - 0s 180us/sample - loss: 483.5546 - val_loss: 497.9888\n",
      "Epoch 22/100\n",
      "379/379 [==============================] - 0s 170us/sample - loss: 480.5201 - val_loss: 494.6441\n",
      "Epoch 23/100\n",
      "379/379 [==============================] - 0s 164us/sample - loss: 477.3078 - val_loss: 491.5455\n",
      "Epoch 24/100\n",
      "379/379 [==============================] - 0s 201us/sample - loss: 474.3918 - val_loss: 488.2923\n",
      "Epoch 25/100\n",
      "379/379 [==============================] - 0s 164us/sample - loss: 471.3798 - val_loss: 485.0611\n",
      "Epoch 26/100\n",
      "379/379 [==============================] - 0s 166us/sample - loss: 468.4074 - val_loss: 481.8390\n",
      "Epoch 27/100\n",
      "379/379 [==============================] - 0s 165us/sample - loss: 465.4446 - val_loss: 478.6823\n",
      "Epoch 28/100\n",
      "379/379 [==============================] - 0s 164us/sample - loss: 462.4676 - val_loss: 475.6019\n",
      "Epoch 29/100\n",
      "379/379 [==============================] - 0s 163us/sample - loss: 459.6200 - val_loss: 472.4512\n",
      "Epoch 30/100\n",
      "379/379 [==============================] - 0s 166us/sample - loss: 456.6738 - val_loss: 469.3837\n",
      "Epoch 31/100\n",
      "379/379 [==============================] - 0s 175us/sample - loss: 453.7977 - val_loss: 466.3029\n",
      "Epoch 32/100\n",
      "379/379 [==============================] - 0s 166us/sample - loss: 450.9463 - val_loss: 463.2395\n",
      "Epoch 33/100\n",
      "379/379 [==============================] - 0s 171us/sample - loss: 448.0663 - val_loss: 460.2320\n",
      "Epoch 34/100\n",
      "379/379 [==============================] - 0s 169us/sample - loss: 445.2786 - val_loss: 457.1825\n",
      "Epoch 35/100\n",
      "379/379 [==============================] - 0s 169us/sample - loss: 442.4623 - val_loss: 454.1880\n",
      "Epoch 36/100\n",
      "379/379 [==============================] - 0s 164us/sample - loss: 439.6538 - val_loss: 451.2269\n",
      "Epoch 37/100\n",
      "379/379 [==============================] - 0s 165us/sample - loss: 436.9106 - val_loss: 448.2370\n",
      "Epoch 38/100\n",
      "379/379 [==============================] - 0s 168us/sample - loss: 434.1485 - val_loss: 445.2870\n",
      "Epoch 39/100\n",
      "379/379 [==============================] - 0s 164us/sample - loss: 431.4129 - val_loss: 442.3308\n",
      "Epoch 40/100\n",
      "379/379 [==============================] - 0s 163us/sample - loss: 428.6568 - val_loss: 439.4431\n",
      "Epoch 41/100\n",
      "379/379 [==============================] - 0s 162us/sample - loss: 425.9934 - val_loss: 436.5118\n",
      "Epoch 42/100\n",
      "379/379 [==============================] - 0s 162us/sample - loss: 423.2467 - val_loss: 433.6746\n",
      "Epoch 43/100\n",
      "379/379 [==============================] - 0s 164us/sample - loss: 420.5862 - val_loss: 430.8124\n",
      "Epoch 44/100\n",
      "379/379 [==============================] - 0s 172us/sample - loss: 417.9447 - val_loss: 427.9429\n",
      "Epoch 45/100\n",
      "379/379 [==============================] - 0s 170us/sample - loss: 415.2866 - val_loss: 425.0863\n",
      "Epoch 46/100\n",
      "379/379 [==============================] - 0s 167us/sample - loss: 412.6046 - val_loss: 422.2818\n",
      "Epoch 47/100\n",
      "379/379 [==============================] - 0s 167us/sample - loss: 410.0436 - val_loss: 419.3625\n",
      "Epoch 48/100\n",
      "379/379 [==============================] - 0s 169us/sample - loss: 407.3196 - val_loss: 416.6204\n",
      "Epoch 49/100\n",
      "379/379 [==============================] - 0s 167us/sample - loss: 404.7487 - val_loss: 413.8435\n",
      "Epoch 50/100\n",
      "379/379 [==============================] - 0s 163us/sample - loss: 402.1554 - val_loss: 411.0932\n",
      "Epoch 51/100\n",
      "379/379 [==============================] - 0s 163us/sample - loss: 399.6052 - val_loss: 408.3116\n",
      "Epoch 52/100\n",
      "379/379 [==============================] - 0s 166us/sample - loss: 397.0513 - val_loss: 405.5197\n",
      "Epoch 53/100\n",
      "379/379 [==============================] - 0s 166us/sample - loss: 394.4658 - val_loss: 402.8008\n",
      "Epoch 54/100\n",
      "379/379 [==============================] - 0s 163us/sample - loss: 391.9063 - val_loss: 400.1072\n",
      "Epoch 55/100\n",
      "379/379 [==============================] - 0s 163us/sample - loss: 389.4366 - val_loss: 397.3454\n",
      "Epoch 56/100\n",
      "379/379 [==============================] - 0s 166us/sample - loss: 386.8743 - val_loss: 394.6735\n",
      "Epoch 57/100\n",
      "379/379 [==============================] - 0s 163us/sample - loss: 384.3963 - val_loss: 391.9796\n",
      "Epoch 58/100\n",
      "379/379 [==============================] - 0s 165us/sample - loss: 381.9476 - val_loss: 389.2685\n",
      "Epoch 59/100\n",
      "379/379 [==============================] - 0s 162us/sample - loss: 379.4426 - val_loss: 386.6294\n",
      "Epoch 60/100\n",
      "379/379 [==============================] - 0s 162us/sample - loss: 376.9775 - val_loss: 384.0154\n",
      "Epoch 61/100\n",
      "379/379 [==============================] - 0s 164us/sample - loss: 374.5539 - val_loss: 381.3809\n",
      "Epoch 62/100\n",
      "379/379 [==============================] - 0s 168us/sample - loss: 372.0801 - val_loss: 378.8164\n",
      "Epoch 63/100\n",
      "379/379 [==============================] - 0s 164us/sample - loss: 369.6818 - val_loss: 376.2215\n",
      "Epoch 64/100\n",
      "379/379 [==============================] - 0s 170us/sample - loss: 367.3114 - val_loss: 373.5934\n",
      "Epoch 65/100\n",
      "379/379 [==============================] - 0s 161us/sample - loss: 364.8544 - val_loss: 371.0543\n",
      "Epoch 66/100\n",
      "379/379 [==============================] - 0s 166us/sample - loss: 362.4726 - val_loss: 368.5057\n",
      "Epoch 67/100\n",
      "379/379 [==============================] - 0s 165us/sample - loss: 360.1281 - val_loss: 365.9125\n",
      "Epoch 68/100\n",
      "379/379 [==============================] - 0s 166us/sample - loss: 357.7356 - val_loss: 363.3630\n",
      "Epoch 69/100\n",
      "379/379 [==============================] - 0s 165us/sample - loss: 355.3481 - val_loss: 360.8578\n",
      "Epoch 70/100\n",
      "379/379 [==============================] - 0s 164us/sample - loss: 353.0381 - val_loss: 358.3260\n",
      "Epoch 71/100\n",
      "379/379 [==============================] - 0s 163us/sample - loss: 350.6339 - val_loss: 355.9080\n",
      "Epoch 72/100\n",
      "379/379 [==============================] - 0s 162us/sample - loss: 348.4042 - val_loss: 353.3681\n",
      "Epoch 73/100\n",
      "379/379 [==============================] - 0s 165us/sample - loss: 346.0493 - val_loss: 350.8972\n",
      "Epoch 74/100\n",
      "379/379 [==============================] - 0s 161us/sample - loss: 343.7306 - val_loss: 348.4755\n",
      "Epoch 75/100\n",
      "379/379 [==============================] - 0s 163us/sample - loss: 341.4462 - val_loss: 346.0500\n",
      "Epoch 76/100\n",
      "379/379 [==============================] - 0s 165us/sample - loss: 339.1888 - val_loss: 343.6088\n",
      "Epoch 77/100\n",
      "379/379 [==============================] - 0s 162us/sample - loss: 336.9674 - val_loss: 341.1569\n",
      "Epoch 78/100\n",
      "379/379 [==============================] - 0s 171us/sample - loss: 334.6614 - val_loss: 338.7764\n",
      "Epoch 79/100\n",
      "379/379 [==============================] - 0s 163us/sample - loss: 332.4139 - val_loss: 336.4212\n",
      "Epoch 80/100\n",
      "379/379 [==============================] - 0s 165us/sample - loss: 330.2134 - val_loss: 334.0252\n",
      "Epoch 81/100\n",
      "379/379 [==============================] - 0s 163us/sample - loss: 327.9956 - val_loss: 331.6435\n",
      "Epoch 82/100\n",
      "379/379 [==============================] - 0s 163us/sample - loss: 325.7874 - val_loss: 329.2835\n",
      "Epoch 83/100\n",
      "379/379 [==============================] - 0s 165us/sample - loss: 323.5968 - val_loss: 326.9263\n",
      "Epoch 84/100\n",
      "379/379 [==============================] - 0s 162us/sample - loss: 321.3864 - val_loss: 324.6100\n",
      "Epoch 85/100\n",
      "379/379 [==============================] - 0s 161us/sample - loss: 319.2289 - val_loss: 322.2627\n",
      "Epoch 86/100\n",
      "379/379 [==============================] - 0s 164us/sample - loss: 317.0390 - val_loss: 319.9624\n",
      "Epoch 87/100\n",
      "379/379 [==============================] - 0s 162us/sample - loss: 314.8962 - val_loss: 317.6445\n",
      "Epoch 88/100\n",
      "379/379 [==============================] - 0s 160us/sample - loss: 312.7019 - val_loss: 315.3855\n",
      "Epoch 89/100\n",
      "379/379 [==============================] - 0s 162us/sample - loss: 310.5951 - val_loss: 313.0769\n",
      "Epoch 90/100\n",
      "379/379 [==============================] - 0s 160us/sample - loss: 308.4312 - val_loss: 310.8314\n",
      "Epoch 91/100\n",
      "379/379 [==============================] - 0s 168us/sample - loss: 306.3225 - val_loss: 308.5791\n",
      "Epoch 92/100\n",
      "379/379 [==============================] - 0s 172us/sample - loss: 304.2026 - val_loss: 306.3503\n",
      "Epoch 93/100\n",
      "379/379 [==============================] - 0s 167us/sample - loss: 302.0998 - val_loss: 304.1321\n",
      "Epoch 94/100\n",
      "379/379 [==============================] - 0s 167us/sample - loss: 299.9940 - val_loss: 301.9555\n",
      "Epoch 95/100\n",
      "379/379 [==============================] - 0s 165us/sample - loss: 297.9907 - val_loss: 299.6876\n",
      "Epoch 96/100\n",
      "379/379 [==============================] - 0s 166us/sample - loss: 295.8422 - val_loss: 297.5384\n",
      "Epoch 97/100\n",
      "379/379 [==============================] - 0s 195us/sample - loss: 293.8187 - val_loss: 295.3564\n",
      "Epoch 98/100\n",
      "379/379 [==============================] - 0s 164us/sample - loss: 291.8009 - val_loss: 293.1603\n",
      "Epoch 99/100\n",
      "379/379 [==============================] - 0s 169us/sample - loss: 289.7611 - val_loss: 290.9982\n",
      "Epoch 100/100\n",
      "379/379 [==============================] - 0s 165us/sample - loss: 287.7054 - val_loss: 288.8908\n",
      "\n",
      "Forming the grid-search model #17 using: optimizer=<tensorflow.python.keras.optimizer_v2.adam.Adam object at 0x7f9132bd5290>, kernel=<tensorflow.python.ops.init_ops_v2.GlorotNormal object at 0x7f9132ed1050>, epochs=100, batch_size=64\n",
      "Train on 379 samples, validate on 127 samples\n",
      "Epoch 1/100\n",
      "379/379 [==============================] - 1s 3ms/sample - loss: 569.3639 - val_loss: 588.4388\n",
      "Epoch 2/100\n",
      "379/379 [==============================] - 0s 106us/sample - loss: 565.8603 - val_loss: 584.2869\n",
      "Epoch 3/100\n",
      "379/379 [==============================] - 0s 106us/sample - loss: 561.8050 - val_loss: 580.2061\n",
      "Epoch 4/100\n",
      "379/379 [==============================] - 0s 106us/sample - loss: 557.9324 - val_loss: 576.3140\n",
      "Epoch 5/100\n",
      "379/379 [==============================] - 0s 115us/sample - loss: 554.3709 - val_loss: 572.6223\n",
      "Epoch 6/100\n",
      "379/379 [==============================] - 0s 107us/sample - loss: 550.9330 - val_loss: 569.2174\n",
      "Epoch 7/100\n",
      "379/379 [==============================] - 0s 110us/sample - loss: 547.7172 - val_loss: 565.9980\n",
      "Epoch 8/100\n",
      "379/379 [==============================] - 0s 109us/sample - loss: 544.7564 - val_loss: 562.9388\n",
      "Epoch 9/100\n",
      "379/379 [==============================] - 0s 104us/sample - loss: 541.9151 - val_loss: 560.0421\n",
      "Epoch 10/100\n",
      "379/379 [==============================] - 0s 109us/sample - loss: 539.2450 - val_loss: 557.2485\n",
      "Epoch 11/100\n",
      "379/379 [==============================] - 0s 109us/sample - loss: 536.6428 - val_loss: 554.5855\n",
      "Epoch 12/100\n",
      "379/379 [==============================] - 0s 107us/sample - loss: 534.1709 - val_loss: 551.9821\n",
      "Epoch 13/100\n",
      "379/379 [==============================] - 0s 110us/sample - loss: 531.7043 - val_loss: 549.4891\n",
      "Epoch 14/100\n",
      "379/379 [==============================] - 0s 104us/sample - loss: 529.4026 - val_loss: 547.0267\n",
      "Epoch 15/100\n",
      "379/379 [==============================] - 0s 109us/sample - loss: 527.0732 - val_loss: 544.6331\n",
      "Epoch 16/100\n",
      "379/379 [==============================] - 0s 102us/sample - loss: 524.8682 - val_loss: 542.2717\n",
      "Epoch 17/100\n",
      "379/379 [==============================] - 0s 107us/sample - loss: 522.6599 - val_loss: 539.9822\n",
      "Epoch 18/100\n",
      "379/379 [==============================] - 0s 108us/sample - loss: 520.4977 - val_loss: 537.7334\n",
      "Epoch 19/100\n",
      "379/379 [==============================] - 0s 126us/sample - loss: 518.4307 - val_loss: 535.5007\n",
      "Epoch 20/100\n",
      "379/379 [==============================] - 0s 109us/sample - loss: 516.3728 - val_loss: 533.2925\n",
      "Epoch 21/100\n",
      "379/379 [==============================] - 0s 107us/sample - loss: 514.3233 - val_loss: 531.1293\n",
      "Epoch 22/100\n",
      "379/379 [==============================] - 0s 105us/sample - loss: 512.3562 - val_loss: 528.9883\n",
      "Epoch 23/100\n",
      "379/379 [==============================] - 0s 108us/sample - loss: 510.2543 - val_loss: 526.9583\n",
      "Epoch 24/100\n",
      "379/379 [==============================] - 0s 108us/sample - loss: 508.3727 - val_loss: 524.9026\n",
      "Epoch 25/100\n",
      "379/379 [==============================] - 0s 110us/sample - loss: 506.4526 - val_loss: 522.8500\n",
      "Epoch 26/100\n",
      "379/379 [==============================] - 0s 105us/sample - loss: 504.5590 - val_loss: 520.8104\n",
      "Epoch 27/100\n",
      "379/379 [==============================] - 0s 109us/sample - loss: 502.6801 - val_loss: 518.8036\n",
      "Epoch 28/100\n",
      "379/379 [==============================] - 0s 104us/sample - loss: 500.7808 - val_loss: 516.8489\n",
      "Epoch 29/100\n",
      "379/379 [==============================] - 0s 110us/sample - loss: 498.9975 - val_loss: 514.8737\n",
      "Epoch 30/100\n",
      "379/379 [==============================] - 0s 105us/sample - loss: 497.1480 - val_loss: 512.9175\n",
      "Epoch 31/100\n",
      "379/379 [==============================] - 0s 110us/sample - loss: 495.3208 - val_loss: 510.9724\n",
      "Epoch 32/100\n",
      "379/379 [==============================] - 0s 110us/sample - loss: 493.5290 - val_loss: 509.0453\n",
      "Epoch 33/100\n",
      "379/379 [==============================] - 0s 107us/sample - loss: 491.7187 - val_loss: 507.1381\n",
      "Epoch 34/100\n",
      "379/379 [==============================] - 0s 111us/sample - loss: 489.9673 - val_loss: 505.2213\n",
      "Epoch 35/100\n",
      "379/379 [==============================] - 0s 108us/sample - loss: 488.1961 - val_loss: 503.3249\n",
      "Epoch 36/100\n",
      "379/379 [==============================] - 0s 109us/sample - loss: 486.4251 - val_loss: 501.4486\n",
      "Epoch 37/100\n",
      "379/379 [==============================] - 0s 105us/sample - loss: 484.7097 - val_loss: 499.5627\n",
      "Epoch 38/100\n",
      "379/379 [==============================] - 0s 108us/sample - loss: 482.9725 - val_loss: 497.7056\n",
      "Epoch 39/100\n",
      "379/379 [==============================] - 0s 111us/sample - loss: 481.2508 - val_loss: 495.8507\n",
      "Epoch 40/100\n",
      "379/379 [==============================] - 0s 107us/sample - loss: 479.5275 - val_loss: 494.0173\n",
      "Epoch 41/100\n",
      "379/379 [==============================] - 0s 106us/sample - loss: 477.8391 - val_loss: 492.1826\n",
      "Epoch 42/100\n",
      "379/379 [==============================] - 0s 105us/sample - loss: 476.1058 - val_loss: 490.3819\n",
      "Epoch 43/100\n",
      "379/379 [==============================] - 0s 111us/sample - loss: 474.4255 - val_loss: 488.5795\n",
      "Epoch 44/100\n",
      "379/379 [==============================] - 0s 107us/sample - loss: 472.7629 - val_loss: 486.7771\n",
      "Epoch 45/100\n",
      "379/379 [==============================] - 0s 104us/sample - loss: 471.0890 - val_loss: 484.9842\n",
      "Epoch 46/100\n",
      "379/379 [==============================] - 0s 107us/sample - loss: 469.3990 - val_loss: 483.2087\n",
      "Epoch 47/100\n",
      "379/379 [==============================] - 0s 106us/sample - loss: 467.7997 - val_loss: 481.3974\n",
      "Epoch 48/100\n",
      "379/379 [==============================] - 0s 108us/sample - loss: 466.0776 - val_loss: 479.6483\n",
      "Epoch 49/100\n",
      "379/379 [==============================] - 0s 112us/sample - loss: 464.4513 - val_loss: 477.8976\n",
      "Epoch 50/100\n",
      "379/379 [==============================] - 0s 120us/sample - loss: 462.8192 - val_loss: 476.1541\n",
      "Epoch 51/100\n",
      "379/379 [==============================] - 0s 106us/sample - loss: 461.2197 - val_loss: 474.3956\n",
      "Epoch 52/100\n",
      "379/379 [==============================] - 0s 107us/sample - loss: 459.6102 - val_loss: 472.6413\n",
      "Epoch 53/100\n",
      "379/379 [==============================] - 0s 104us/sample - loss: 457.9667 - val_loss: 470.9279\n",
      "Epoch 54/100\n",
      "379/379 [==============================] - 0s 110us/sample - loss: 456.3582 - val_loss: 469.2151\n",
      "Epoch 55/100\n",
      "379/379 [==============================] - 0s 105us/sample - loss: 454.7950 - val_loss: 467.4829\n",
      "Epoch 56/100\n",
      "379/379 [==============================] - 0s 106us/sample - loss: 453.1733 - val_loss: 465.7786\n",
      "Epoch 57/100\n",
      "379/379 [==============================] - 0s 107us/sample - loss: 451.6056 - val_loss: 464.0689\n",
      "Epoch 58/100\n",
      "379/379 [==============================] - 0s 108us/sample - loss: 450.0517 - val_loss: 462.3562\n",
      "Epoch 59/100\n",
      "379/379 [==============================] - 0s 113us/sample - loss: 448.4517 - val_loss: 460.6776\n",
      "Epoch 60/100\n",
      "379/379 [==============================] - 0s 107us/sample - loss: 446.8889 - val_loss: 459.0028\n",
      "Epoch 61/100\n",
      "379/379 [==============================] - 0s 105us/sample - loss: 445.3397 - val_loss: 457.3244\n",
      "Epoch 62/100\n",
      "379/379 [==============================] - 0s 107us/sample - loss: 443.7461 - val_loss: 455.6866\n",
      "Epoch 63/100\n",
      "379/379 [==============================] - 0s 108us/sample - loss: 442.2270 - val_loss: 454.0310\n",
      "Epoch 64/100\n",
      "379/379 [==============================] - 0s 108us/sample - loss: 440.7238 - val_loss: 452.3591\n",
      "Epoch 65/100\n",
      "379/379 [==============================] - 0s 107us/sample - loss: 439.1457 - val_loss: 450.7295\n",
      "Epoch 66/100\n",
      "379/379 [==============================] - 0s 109us/sample - loss: 437.6297 - val_loss: 449.0978\n",
      "Epoch 67/100\n",
      "379/379 [==============================] - 0s 109us/sample - loss: 436.1393 - val_loss: 447.4515\n",
      "Epoch 68/100\n",
      "379/379 [==============================] - 0s 104us/sample - loss: 434.6159 - val_loss: 445.8228\n",
      "Epoch 69/100\n",
      "379/379 [==============================] - 0s 110us/sample - loss: 433.0865 - val_loss: 444.2153\n",
      "Epoch 70/100\n",
      "379/379 [==============================] - 0s 106us/sample - loss: 431.6126 - val_loss: 442.6014\n",
      "Epoch 71/100\n",
      "379/379 [==============================] - 0s 107us/sample - loss: 430.0683 - val_loss: 441.0288\n",
      "Epoch 72/100\n",
      "379/379 [==============================] - 0s 110us/sample - loss: 428.6559 - val_loss: 439.4087\n",
      "Epoch 73/100\n",
      "379/379 [==============================] - 0s 109us/sample - loss: 427.1491 - val_loss: 437.8127\n",
      "Epoch 74/100\n",
      "379/379 [==============================] - 0s 103us/sample - loss: 425.6483 - val_loss: 436.2449\n",
      "Epoch 75/100\n",
      "379/379 [==============================] - 0s 110us/sample - loss: 424.1724 - val_loss: 434.6820\n",
      "Epoch 76/100\n",
      "379/379 [==============================] - 0s 105us/sample - loss: 422.7261 - val_loss: 433.1074\n",
      "Epoch 77/100\n",
      "379/379 [==============================] - 0s 109us/sample - loss: 421.3015 - val_loss: 431.5225\n",
      "Epoch 78/100\n",
      "379/379 [==============================] - 0s 106us/sample - loss: 419.8069 - val_loss: 429.9640\n",
      "Epoch 79/100\n",
      "379/379 [==============================] - 0s 105us/sample - loss: 418.3347 - val_loss: 428.4284\n",
      "Epoch 80/100\n",
      "379/379 [==============================] - 0s 105us/sample - loss: 416.9235 - val_loss: 426.8721\n",
      "Epoch 81/100\n",
      "379/379 [==============================] - 0s 108us/sample - loss: 415.4811 - val_loss: 425.3218\n",
      "Epoch 82/100\n",
      "379/379 [==============================] - 0s 105us/sample - loss: 414.0491 - val_loss: 423.7795\n",
      "Epoch 83/100\n",
      "379/379 [==============================] - 0s 108us/sample - loss: 412.6285 - val_loss: 422.2383\n",
      "Epoch 84/100\n",
      "379/379 [==============================] - 0s 105us/sample - loss: 411.1914 - val_loss: 420.7131\n",
      "Epoch 85/100\n",
      "379/379 [==============================] - 0s 104us/sample - loss: 409.7826 - val_loss: 419.1802\n",
      "Epoch 86/100\n",
      "379/379 [==============================] - 0s 107us/sample - loss: 408.3663 - val_loss: 417.6586\n",
      "Epoch 87/100\n",
      "379/379 [==============================] - 0s 107us/sample - loss: 406.9680 - val_loss: 416.1390\n",
      "Epoch 88/100\n",
      "379/379 [==============================] - 0s 104us/sample - loss: 405.5374 - val_loss: 414.6409\n",
      "Epoch 89/100\n",
      "379/379 [==============================] - 0s 108us/sample - loss: 404.1618 - val_loss: 413.1286\n",
      "Epoch 90/100\n",
      "379/379 [==============================] - 0s 106us/sample - loss: 402.7473 - val_loss: 411.6353\n",
      "Epoch 91/100\n",
      "379/379 [==============================] - 0s 110us/sample - loss: 401.3602 - val_loss: 410.1386\n",
      "Epoch 92/100\n",
      "379/379 [==============================] - 0s 106us/sample - loss: 399.9682 - val_loss: 408.6488\n",
      "Epoch 93/100\n",
      "379/379 [==============================] - 0s 109us/sample - loss: 398.5783 - val_loss: 407.1636\n",
      "Epoch 94/100\n",
      "379/379 [==============================] - 0s 111us/sample - loss: 397.1744 - val_loss: 405.7069\n",
      "Epoch 95/100\n",
      "379/379 [==============================] - 0s 106us/sample - loss: 395.8650 - val_loss: 404.2078\n",
      "Epoch 96/100\n",
      "379/379 [==============================] - 0s 111us/sample - loss: 394.4397 - val_loss: 402.7462\n",
      "Epoch 97/100\n",
      "379/379 [==============================] - 0s 109us/sample - loss: 393.0824 - val_loss: 401.2785\n",
      "Epoch 98/100\n",
      "379/379 [==============================] - 0s 108us/sample - loss: 391.7574 - val_loss: 399.7911\n",
      "Epoch 99/100\n",
      "379/379 [==============================] - 0s 107us/sample - loss: 390.3854 - val_loss: 398.3222\n",
      "Epoch 100/100\n",
      "379/379 [==============================] - 0s 106us/sample - loss: 389.0034 - val_loss: 396.8749\n",
      "\n",
      "Best score (lowest validation loss) found via grid search: loss=11.535171 RMSE=3.396347 from model iteration #2\n",
      "The best modeling parameters are: optimizer=<tensorflow.python.keras.optimizer_v2.adam.Adam object at 0x7f9132bd52d0>, kernel=<tensorflow.python.ops.init_ops_v2.Orthogonal object at 0x7f9132bbb090>, epochs=100, batch_size=32\n",
      "Total time for performing grid-search of the best parameters: 0:01:57.810244\n"
     ]
    }
   ],
   "source": [
    "startTimeModule = datetime.now()\n",
    "\n",
    "# Set up grid search using different epochs, batch sizes, and optimizers\n",
    "optz_1 = tf.optimizers.Adam(learning_rate=0.0010)\n",
    "optz_2 = tf.optimizers.Adam(learning_rate=0.0005)\n",
    "optz_3 = tf.optimizers.Adam(learning_rate=0.0001)\n",
    "optimizer_grid = [optz_1, optz_2, optz_3]\n",
    "print('Optimizer candidate #1 has the object ID of', optz_1)\n",
    "print('Optimizer candidate #2 has the object ID of', optz_2)\n",
    "print('Optimizer candidate #3 has the object ID of', optz_3)\n",
    "\n",
    "init_1 = tf.initializers.RandomNormal(seed=seedNum)\n",
    "init_2 = tf.initializers.Orthogonal(seed=seedNum)\n",
    "init_3 = tf.initializers.GlorotNormal(seed=seedNum)\n",
    "init_grid = [init_1, init_2, init_3]\n",
    "print('Initializer candidate #1 has the object ID of', init_1)\n",
    "print('Initializer candidate #2 has the object ID of', init_2)\n",
    "print('Initializer candidate #2 has the object ID of', init_3)\n",
    "\n",
    "epoch_grid = [default_epoch]\n",
    "batch_grid = [default_batch, int(default_batch*2)]\n",
    "\n",
    "best_score = float('inf')\n",
    "grid_iteration = 0\n",
    "best_iteration = 0\n",
    "best_optimizer = default_optimizer\n",
    "best_kernel_init = default_kernel_init\n",
    "best_epoch = default_epoch\n",
    "best_batch = default_batch\n",
    "\n",
    "for optimizer in optimizer_grid:\n",
    "    for kernel_init in init_grid:\n",
    "        for epoch_num in epoch_grid:\n",
    "            for batch_num in batch_grid:\n",
    "                print('\\nForming the grid-search model #%d using: optimizer=%s, kernel=%s, epochs=%d, batch_size=%d'\n",
    "                      % (grid_iteration, optimizer, kernel_init, epoch_num, batch_num))\n",
    "                reset_random(seedNum)\n",
    "                grid_model = create_customized_model(optimizer, kernel_init)\n",
    "                grid_hist = grid_model.fit(X_train, y_train, epochs=epoch_num, batch_size=batch_num, \n",
    "                                       validation_data=(X_test, y_test), verbose=1)\n",
    "                if(grid_hist.history['val_loss'][-1] < best_score):\n",
    "                    best_score = grid_hist.history['val_loss'][-1]\n",
    "                    best_iteration = grid_iteration\n",
    "                    best_optimizer = optimizer\n",
    "                    best_kernel_init = kernel_init\n",
    "                    best_epoch = epoch_num\n",
    "                    best_batch = batch_num\n",
    "                grid_iteration = grid_iteration + 1\n",
    "\n",
    "# summarize results\n",
    "print(\"\\nBest score (lowest validation loss) found via grid search: loss=%f RMSE=%f from model iteration #%d\"\n",
    "      % (best_score, math.sqrt(best_score), best_iteration))\n",
    "print('The best modeling parameters are: optimizer=%s, kernel=%s, epochs=%d, batch_size=%d'\n",
    "      % (best_optimizer, best_kernel_init, best_epoch, best_batch))\n",
    "print('Total time for performing grid-search of the best parameters:', (datetime.now() - startTimeModule))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Forming the final model using: optimizer=<tensorflow.python.keras.optimizer_v2.adam.Adam object at 0x7f9132bd52d0>, kernel=<tensorflow.python.ops.init_ops_v2.Orthogonal object at 0x7f9132bbb090>, epochs=100, batch_size=32\n",
      "Train on 379 samples, validate on 127 samples\n",
      "Epoch 1/100\n",
      "379/379 [==============================] - 1s 2ms/sample - loss: 565.7040 - val_loss: 539.4679\n",
      "Epoch 2/100\n",
      "379/379 [==============================] - 0s 165us/sample - loss: 500.6154 - val_loss: 455.7547\n",
      "Epoch 3/100\n",
      "379/379 [==============================] - 0s 164us/sample - loss: 418.7573 - val_loss: 363.8775\n",
      "Epoch 4/100\n",
      "379/379 [==============================] - 0s 165us/sample - loss: 336.7835 - val_loss: 276.4078\n",
      "Epoch 5/100\n",
      "379/379 [==============================] - 0s 165us/sample - loss: 264.2079 - val_loss: 205.9596\n",
      "Epoch 6/100\n",
      "379/379 [==============================] - 0s 165us/sample - loss: 207.3601 - val_loss: 156.6918\n",
      "Epoch 7/100\n",
      "379/379 [==============================] - 0s 169us/sample - loss: 164.2293 - val_loss: 123.7203\n",
      "Epoch 8/100\n",
      "379/379 [==============================] - 0s 170us/sample - loss: 133.0754 - val_loss: 100.1768\n",
      "Epoch 9/100\n",
      "379/379 [==============================] - 0s 165us/sample - loss: 109.3138 - val_loss: 81.9522\n",
      "Epoch 10/100\n",
      "379/379 [==============================] - 0s 162us/sample - loss: 89.8998 - val_loss: 67.8469\n",
      "Epoch 11/100\n",
      "379/379 [==============================] - 0s 167us/sample - loss: 75.3876 - val_loss: 56.4581\n",
      "Epoch 12/100\n",
      "379/379 [==============================] - 0s 165us/sample - loss: 63.4765 - val_loss: 47.8596\n",
      "Epoch 13/100\n",
      "379/379 [==============================] - 0s 166us/sample - loss: 54.3242 - val_loss: 41.6864\n",
      "Epoch 14/100\n",
      "379/379 [==============================] - 0s 167us/sample - loss: 48.2229 - val_loss: 36.7696\n",
      "Epoch 15/100\n",
      "379/379 [==============================] - 0s 167us/sample - loss: 42.9631 - val_loss: 33.3958\n",
      "Epoch 16/100\n",
      "379/379 [==============================] - 0s 158us/sample - loss: 39.2761 - val_loss: 30.8247\n",
      "Epoch 17/100\n",
      "379/379 [==============================] - 0s 161us/sample - loss: 36.7460 - val_loss: 28.7686\n",
      "Epoch 18/100\n",
      "379/379 [==============================] - 0s 163us/sample - loss: 34.4036 - val_loss: 27.4123\n",
      "Epoch 19/100\n",
      "379/379 [==============================] - 0s 166us/sample - loss: 32.7997 - val_loss: 26.3358\n",
      "Epoch 20/100\n",
      "379/379 [==============================] - 0s 165us/sample - loss: 31.4998 - val_loss: 25.4504\n",
      "Epoch 21/100\n",
      "379/379 [==============================] - 0s 167us/sample - loss: 30.4588 - val_loss: 24.7908\n",
      "Epoch 22/100\n",
      "379/379 [==============================] - 0s 168us/sample - loss: 29.5596 - val_loss: 24.2730\n",
      "Epoch 23/100\n",
      "379/379 [==============================] - 0s 166us/sample - loss: 28.8469 - val_loss: 23.8452\n",
      "Epoch 24/100\n",
      "379/379 [==============================] - 0s 167us/sample - loss: 28.2626 - val_loss: 23.3977\n",
      "Epoch 25/100\n",
      "379/379 [==============================] - 0s 164us/sample - loss: 27.5950 - val_loss: 23.0945\n",
      "Epoch 26/100\n",
      "379/379 [==============================] - 0s 167us/sample - loss: 27.1218 - val_loss: 22.7637\n",
      "Epoch 27/100\n",
      "379/379 [==============================] - 0s 164us/sample - loss: 26.6315 - val_loss: 22.4887\n",
      "Epoch 28/100\n",
      "379/379 [==============================] - 0s 172us/sample - loss: 26.1914 - val_loss: 22.1699\n",
      "Epoch 29/100\n",
      "379/379 [==============================] - 0s 166us/sample - loss: 25.7654 - val_loss: 21.9076\n",
      "Epoch 30/100\n",
      "379/379 [==============================] - 0s 167us/sample - loss: 25.3852 - val_loss: 21.6202\n",
      "Epoch 31/100\n",
      "379/379 [==============================] - 0s 166us/sample - loss: 25.0022 - val_loss: 21.4226\n",
      "Epoch 32/100\n",
      "379/379 [==============================] - 0s 166us/sample - loss: 24.6196 - val_loss: 21.1490\n",
      "Epoch 33/100\n",
      "379/379 [==============================] - 0s 160us/sample - loss: 24.2483 - val_loss: 20.9026\n",
      "Epoch 34/100\n",
      "379/379 [==============================] - 0s 167us/sample - loss: 23.9218 - val_loss: 20.6925\n",
      "Epoch 35/100\n",
      "379/379 [==============================] - 0s 167us/sample - loss: 23.5903 - val_loss: 20.4363\n",
      "Epoch 36/100\n",
      "379/379 [==============================] - 0s 164us/sample - loss: 23.2612 - val_loss: 20.2216\n",
      "Epoch 37/100\n",
      "379/379 [==============================] - 0s 173us/sample - loss: 22.9465 - val_loss: 20.0335\n",
      "Epoch 38/100\n",
      "379/379 [==============================] - 0s 165us/sample - loss: 22.6461 - val_loss: 19.8138\n",
      "Epoch 39/100\n",
      "379/379 [==============================] - 0s 164us/sample - loss: 22.3507 - val_loss: 19.5948\n",
      "Epoch 40/100\n",
      "379/379 [==============================] - 0s 165us/sample - loss: 22.0848 - val_loss: 19.4034\n",
      "Epoch 41/100\n",
      "379/379 [==============================] - 0s 166us/sample - loss: 21.8263 - val_loss: 19.1727\n",
      "Epoch 42/100\n",
      "379/379 [==============================] - 0s 167us/sample - loss: 21.5410 - val_loss: 18.9742\n",
      "Epoch 43/100\n",
      "379/379 [==============================] - 0s 164us/sample - loss: 21.3177 - val_loss: 18.8342\n",
      "Epoch 44/100\n",
      "379/379 [==============================] - 0s 166us/sample - loss: 21.0313 - val_loss: 18.6172\n",
      "Epoch 45/100\n",
      "379/379 [==============================] - 0s 163us/sample - loss: 20.7981 - val_loss: 18.4260\n",
      "Epoch 46/100\n",
      "379/379 [==============================] - 0s 165us/sample - loss: 20.5604 - val_loss: 18.2277\n",
      "Epoch 47/100\n",
      "379/379 [==============================] - 0s 162us/sample - loss: 20.3734 - val_loss: 18.0045\n",
      "Epoch 48/100\n",
      "379/379 [==============================] - 0s 163us/sample - loss: 20.1164 - val_loss: 17.8301\n",
      "Epoch 49/100\n",
      "379/379 [==============================] - 0s 161us/sample - loss: 19.8516 - val_loss: 17.6499\n",
      "Epoch 50/100\n",
      "379/379 [==============================] - 0s 165us/sample - loss: 19.6576 - val_loss: 17.4614\n",
      "Epoch 51/100\n",
      "379/379 [==============================] - 0s 164us/sample - loss: 19.4276 - val_loss: 17.2697\n",
      "Epoch 52/100\n",
      "379/379 [==============================] - 0s 162us/sample - loss: 19.2324 - val_loss: 17.0781\n",
      "Epoch 53/100\n",
      "379/379 [==============================] - 0s 175us/sample - loss: 19.0126 - val_loss: 16.8970\n",
      "Epoch 54/100\n",
      "379/379 [==============================] - 0s 166us/sample - loss: 18.7959 - val_loss: 16.7712\n",
      "Epoch 55/100\n",
      "379/379 [==============================] - 0s 166us/sample - loss: 18.6053 - val_loss: 16.5989\n",
      "Epoch 56/100\n",
      "379/379 [==============================] - 0s 171us/sample - loss: 18.4216 - val_loss: 16.4470\n",
      "Epoch 57/100\n",
      "379/379 [==============================] - 0s 160us/sample - loss: 18.2537 - val_loss: 16.3191\n",
      "Epoch 58/100\n",
      "379/379 [==============================] - 0s 166us/sample - loss: 18.0495 - val_loss: 16.1039\n",
      "Epoch 59/100\n",
      "379/379 [==============================] - 0s 166us/sample - loss: 17.8395 - val_loss: 15.9533\n",
      "Epoch 60/100\n",
      "379/379 [==============================] - 0s 163us/sample - loss: 17.6671 - val_loss: 15.8135\n",
      "Epoch 61/100\n",
      "379/379 [==============================] - 0s 165us/sample - loss: 17.4737 - val_loss: 15.6604\n",
      "Epoch 62/100\n",
      "379/379 [==============================] - 0s 168us/sample - loss: 17.2995 - val_loss: 15.5284\n",
      "Epoch 63/100\n",
      "379/379 [==============================] - 0s 164us/sample - loss: 17.1363 - val_loss: 15.3704\n",
      "Epoch 64/100\n",
      "379/379 [==============================] - 0s 163us/sample - loss: 16.9515 - val_loss: 15.2093\n",
      "Epoch 65/100\n",
      "379/379 [==============================] - 0s 159us/sample - loss: 16.7823 - val_loss: 15.0735\n",
      "Epoch 66/100\n",
      "379/379 [==============================] - 0s 169us/sample - loss: 16.6033 - val_loss: 14.9331\n",
      "Epoch 67/100\n",
      "379/379 [==============================] - 0s 165us/sample - loss: 16.4238 - val_loss: 14.8020\n",
      "Epoch 68/100\n",
      "379/379 [==============================] - 0s 165us/sample - loss: 16.3018 - val_loss: 14.6068\n",
      "Epoch 69/100\n",
      "379/379 [==============================] - 0s 166us/sample - loss: 16.1134 - val_loss: 14.4885\n",
      "Epoch 70/100\n",
      "379/379 [==============================] - 0s 162us/sample - loss: 15.9609 - val_loss: 14.3093\n",
      "Epoch 71/100\n",
      "379/379 [==============================] - 0s 163us/sample - loss: 15.7973 - val_loss: 14.1914\n",
      "Epoch 72/100\n",
      "379/379 [==============================] - 0s 166us/sample - loss: 15.6543 - val_loss: 13.9738\n",
      "Epoch 73/100\n",
      "379/379 [==============================] - 0s 163us/sample - loss: 15.4963 - val_loss: 13.8567\n",
      "Epoch 74/100\n",
      "379/379 [==============================] - 0s 167us/sample - loss: 15.4062 - val_loss: 13.7455\n",
      "Epoch 75/100\n",
      "379/379 [==============================] - 0s 163us/sample - loss: 15.2375 - val_loss: 13.5910\n",
      "Epoch 76/100\n",
      "379/379 [==============================] - 0s 168us/sample - loss: 15.0634 - val_loss: 13.4299\n",
      "Epoch 77/100\n",
      "379/379 [==============================] - 0s 164us/sample - loss: 14.9587 - val_loss: 13.3096\n",
      "Epoch 78/100\n",
      "379/379 [==============================] - 0s 163us/sample - loss: 14.8109 - val_loss: 13.1834\n",
      "Epoch 79/100\n",
      "379/379 [==============================] - 0s 162us/sample - loss: 14.6660 - val_loss: 13.0708\n",
      "Epoch 80/100\n",
      "379/379 [==============================] - 0s 155us/sample - loss: 14.5654 - val_loss: 12.9460\n",
      "Epoch 81/100\n",
      "379/379 [==============================] - 0s 163us/sample - loss: 14.4226 - val_loss: 12.8087\n",
      "Epoch 82/100\n",
      "379/379 [==============================] - 0s 162us/sample - loss: 14.3158 - val_loss: 12.7320\n",
      "Epoch 83/100\n",
      "379/379 [==============================] - 0s 164us/sample - loss: 14.2150 - val_loss: 12.6045\n",
      "Epoch 84/100\n",
      "379/379 [==============================] - 0s 165us/sample - loss: 14.1008 - val_loss: 12.4812\n",
      "Epoch 85/100\n",
      "379/379 [==============================] - 0s 165us/sample - loss: 13.9972 - val_loss: 12.4218\n",
      "Epoch 86/100\n",
      "379/379 [==============================] - 0s 166us/sample - loss: 13.8887 - val_loss: 12.2957\n",
      "Epoch 87/100\n",
      "379/379 [==============================] - 0s 165us/sample - loss: 13.7978 - val_loss: 12.1782\n",
      "Epoch 88/100\n",
      "379/379 [==============================] - 0s 163us/sample - loss: 13.6915 - val_loss: 12.1209\n",
      "Epoch 89/100\n",
      "379/379 [==============================] - 0s 167us/sample - loss: 13.5960 - val_loss: 12.0133\n",
      "Epoch 90/100\n",
      "379/379 [==============================] - 0s 166us/sample - loss: 13.4987 - val_loss: 11.9327\n",
      "Epoch 91/100\n",
      "379/379 [==============================] - 0s 168us/sample - loss: 13.4270 - val_loss: 11.8288\n",
      "Epoch 92/100\n",
      "379/379 [==============================] - 0s 165us/sample - loss: 13.3286 - val_loss: 11.7576\n",
      "Epoch 93/100\n",
      "379/379 [==============================] - 0s 164us/sample - loss: 13.2710 - val_loss: 11.7093\n",
      "Epoch 94/100\n",
      "379/379 [==============================] - 0s 165us/sample - loss: 13.1457 - val_loss: 11.5934\n",
      "Epoch 95/100\n",
      "379/379 [==============================] - 0s 165us/sample - loss: 13.0835 - val_loss: 11.4943\n",
      "Epoch 96/100\n",
      "379/379 [==============================] - 0s 166us/sample - loss: 12.9802 - val_loss: 11.4535\n",
      "Epoch 97/100\n",
      "379/379 [==============================] - 0s 168us/sample - loss: 12.9228 - val_loss: 11.3986\n",
      "Epoch 98/100\n",
      "379/379 [==============================] - 0s 166us/sample - loss: 12.8582 - val_loss: 11.2826\n",
      "Epoch 99/100\n",
      "379/379 [==============================] - 0s 169us/sample - loss: 12.7439 - val_loss: 11.2087\n",
      "Epoch 100/100\n",
      "379/379 [==============================] - 0s 166us/sample - loss: 12.7282 - val_loss: 11.1850\n"
     ]
    }
   ],
   "source": [
    "# Create the final model for evaluating the test dataset\n",
    "print('Forming the final model using: optimizer=%s, kernel=%s, epochs=%d, batch_size=%d'\n",
    "      % (best_optimizer, best_kernel_init, best_epoch, best_batch))\n",
    "reset_random(seedNum)\n",
    "final_model = create_customized_model(best_optimizer, best_kernel_init)\n",
    "final_hist = final_model.fit(X_train, y_train, epochs=best_epoch, batch_size=best_batch, validation_data=(X_test, y_test), verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_25\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_50 (Dense)             (None, 25)                350       \n",
      "_________________________________________________________________\n",
      "dense_51 (Dense)             (None, 1)                 26        \n",
      "=================================================================\n",
      "Total params: 376\n",
      "Trainable params: 376\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# Display a summary of the final model\n",
    "print(final_model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'name': 'sequential_25', 'layers': [{'class_name': 'Dense', 'config': {'name': 'dense_50', 'trainable': True, 'batch_input_shape': (None, 13), 'dtype': 'float32', 'units': 25, 'activation': 'relu', 'use_bias': True, 'kernel_initializer': {'class_name': 'Orthogonal', 'config': {'gain': 1.0, 'seed': 888}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': None, 'bias_regularizer': None, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'Dense', 'config': {'name': 'dense_51', 'trainable': True, 'dtype': 'float32', 'units': 1, 'activation': 'linear', 'use_bias': True, 'kernel_initializer': {'class_name': 'Orthogonal', 'config': {'gain': 1.0, 'seed': 888}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': None, 'bias_regularizer': None, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}]}\n"
     ]
    }
   ],
   "source": [
    "# Display the configuration of the final model\n",
    "print(final_model.get_config())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['loss', 'val_loss'])\n"
     ]
    }
   ],
   "source": [
    "# List all data points in model training history\n",
    "print(final_hist.history.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 576x576 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Summarize model training hisotry for accuracy and loss\n",
    "fig, axs = plt.subplots(1, 1, figsize=(8,8))\n",
    "plt.subplot(111)\n",
    "plt.plot(final_hist.history['loss'], color='blue', label='train')\n",
    "plt.plot(final_hist.history['val_loss'], color='red', label='test')\n",
    "plt.title('Final Model Training Performance (Loss)')\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Train', 'Test'], loc='upper right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "127/1 [==================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================] - 0s 76us/sample - loss: 10.6772\n",
      "Final MSE of the model: 11.184967\n",
      "Final RMSE of the model: 3.344393\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the Keras model on previously unseen data\n",
    "scores = final_model.evaluate(X_test, y_test)\n",
    "print(\"Final MSE of the model: %f\" % (scores))\n",
    "print(\"Final RMSE of the model: %f\" % (math.sqrt(scores)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "if (notifyStatus): email_notify(\"Phase 4 Optimize Model completed! \"+datetime.now().strftime('%a %B %d, %Y %I:%M:%S %p'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Section 5. Finalize Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "if (notifyStatus): email_notify(\"Phase 5 Finalize Model has begun! \"+datetime.now().strftime('%a %B %d, %Y %I:%M:%S %p'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data item #0 predicted to be 21.16 (expected 22.40)\n",
      "Data item #1 predicted to be 31.50 (expected 32.40)\n",
      "Data item #2 predicted to be 19.03 (expected 21.70)\n",
      "Data item #3 predicted to be 17.56 (expected 24.50)\n",
      "Data item #4 predicted to be 24.60 (expected 16.80)\n",
      "Data item #5 predicted to be 21.15 (expected 21.10)\n",
      "Data item #6 predicted to be 29.97 (expected 29.40)\n",
      "Data item #7 predicted to be 26.46 (expected 28.70)\n",
      "Data item #8 predicted to be 22.80 (expected 21.50)\n",
      "Data item #9 predicted to be 13.62 (expected 13.60)\n",
      "Data item #10 predicted to be 20.33 (expected 21.40)\n",
      "Data item #11 predicted to be 25.11 (expected 24.80)\n",
      "Data item #12 predicted to be 17.54 (expected 16.80)\n",
      "Data item #13 predicted to be 22.36 (expected 19.40)\n",
      "Data item #14 predicted to be 21.59 (expected 21.70)\n",
      "Data item #15 predicted to be 12.99 (expected 17.20)\n",
      "Data item #16 predicted to be 13.23 (expected 17.10)\n",
      "Data item #17 predicted to be 20.33 (expected 18.70)\n",
      "Data item #18 predicted to be 23.33 (expected 22.30)\n",
      "Data item #19 predicted to be 28.05 (expected 25.00)\n"
     ]
    }
   ],
   "source": [
    "# Make class predictions with the model\n",
    "predictions = final_model.predict(X_test)\n",
    "\n",
    "# Summarize the first 20 cases\n",
    "for i in range(20):\n",
    "\tprint('Data item #%d predicted to be %.2f (expected %.2f)' % (i, predictions[i], y_test[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "if (notifyStatus): email_notify(\"Phase 5 Finalize Model completed! \"+datetime.now().strftime('%a %B %d, %Y %I:%M:%S %p'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total time for the script: 0:04:24.385900\n"
     ]
    }
   ],
   "source": [
    "print ('Total time for the script:',(datetime.now() - startTimeScript))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
